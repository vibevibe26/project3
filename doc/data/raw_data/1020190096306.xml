<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:53:28.5328</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2019.08.07</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2019-0096306</applicationNumber><claimCount>31</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>깊이 맵 산출 방법 및 장치</inventionTitle><inventionTitleEng>METHOD AND APPARATUS FOR CALCULATING DEPTH MAP</inventionTitleEng><openDate>2020.06.26</openDate><openNumber>10-2020-0075727</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2022.08.02</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/593</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>H04N 13/271</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 깊이 맵 산출 방법 및 장치가 개시된다. 일 실시예에 따른 깊이 맵 산출 방법은 현재 프레임을 포함하는 복수의 프레임들을 이용하여, 현재 프레임에 대응하는 글로벌 스파스 깊이 맵(global sparse depth map)을 산출하는 단계, 현재 프레임을 이용하여, 현재 프레임에 대응하는 로컬 덴스 깊이 맵(local dense depth map)을 산출하는 단계, 현재 프레임에서, 정적 객체(static object) 영역을 마스크(mask)하여 동적 객체(none static object) 영역을 추출하는 단계, 글로벌 스파스 깊이 맵에서 동적 객체 영역을 제거하는 단계 및 동적 객체 영역이 제거된 글로벌 스파스 깊이 맵과 로컬 덴스 깊이 맵을 통합하여 현재 프레임에 대응하는 글로벌 덴스 깊이 맵(global dense depth map)을 생성하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 현재 프레임을 포함하는 복수의 프레임들을 이용하여, 상기 현재 프레임에 대응하는 글로벌 스파스 깊이 맵(global sparse depth map)을 산출하는 단계;상기 현재 프레임을 이용하여, 상기 현재 프레임에 대응하는 로컬 덴스 깊이 맵(local dense depth map)을 산출하는 단계;상기 현재 프레임에서, 정적 객체(static object) 영역을 마스크(mask)하여 동적 객체(none static object) 영역을 추출하는 단계;상기 글로벌 스파스 깊이 맵에서 상기 동적 객체 영역을 제거하는 단계; 및상기 동적 객체 영역이 제거된 글로벌 스파스 깊이 맵과 상기 로컬 덴스 깊이 맵을 통합하여 상기 현재 프레임에 대응하는 글로벌 덴스 깊이 맵(global dense depth map)을 생성하는 단계를 포함하는 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 글로벌 스파스 깊이 맵을 산출하는 단계는상기 현재 프레임에 포함된 하나 이상의 픽셀 포인트에 대응하는 깊이 정보를 산출하는 단계;상기 현재 프레임에 대응하는 카메라의 포즈 정보를 추정하는 단계; 및상기 깊이 정보 및 상기 카메라의 포즈 정보에 기초하여, 상기 픽셀 포인트의 3차원 좌표를 산출하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 글로벌 덴스 깊이 맵에 기초하여, 상기 현재 프레임에 대응하는 카메라의 포즈 정보를 업데이트하는 단계를 더 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 업데이트가 완료된 카메라의 포즈 정보에 기초하여, 상기 글로벌 스파스 깊이 맵을 업데이트하는 단계를 더 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 글로벌 스파스 깊이 맵을 산출하는 단계는상기 복수의 프레임들 중 상기 현재 프레임 이전 시점의 키 프레임(key frame)에 대응하는 제1 깊이 정보를 산출하는 단계;상기 현재 프레임에 대응하는 제2 깊이 정보를 산출하는 단계;상기 제1 깊이 정보 및 상기 제2 깊이 정보에 기초하여, 상기 현재 프레임에 대응하는 카메라의 포즈 정보를 추정하는 단계; 및상기 제2 깊이 정보 및 상기 카메라의 포즈 정보에 기초하여, 상기 글로벌 스파스 깊이 맵을 산출하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 제2 깊이 정보를 산출하는 단계는상기 현재 프레임에 포함된 우안 영상과 좌안 영상의 스테레오 매칭(stereo matching)을 수행하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>7. 제5항에 있어서,상기 카메라의 포즈 정보는제1 위치에서 제2 위치로의 상기 카메라의 이동에 따라 변화되는 회전 정보 및 이동 정보 중 적어도 하나를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 로컬 덴스 깊이 맵을 산출하는 단계는복수의 픽셀 포인트들을 포함하는 상기 현재 프레임을 인공 신경망에 입력함으로써, 상기 복수의 픽셀 포인트들의 깊이 정보에 대응하는 상기 인공 신경망의 출력들을 획득하는 단계; 및상기 출력들에 기초하여, 상기 로컬 덴스 깊이 맵을 산출하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 동적 객체 영역을 추출하는 단계는상기 현재 프레임을 인공 신경망에 입력함으로써, 정적 객체 영역과 동적 객체 영역으로 분류된 상기 인공 신경망의 출력들을 획득하는 단계; 및상기 출력들에 기초하여, 상기 동적 객체 영역을 추출하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 글로벌 덴스 깊이 맵을 생성하는 단계는상기 로컬 덴스 깊이 맵을 복수의 그리드 셀들로 분할하는 단계;상기 동적 객체 영역이 제거된 글로벌 스파스 깊이 맵에 기초하여, 상기 그리드 셀들의 꼭지점들(corner points)에 대응하는 픽셀 포인트들의 깊이 정보를 업데이트하는 단계; 및상기 동적 객체 영역이 제거된 글로벌 스파스 깊이 맵 및 상기 업데이트된 꼭지점들에 대응하는 픽셀 포인트들의 깊이 정보에 기초하여, 상기 그리드 셀들의 내부 영역에 포함되는 픽셀 포인트들의 깊이 정보를 업데이트하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>11. 제1항에 있어서, 상기 로컬 덴스 깊이 맵을 산출하는 단계는상기 현재 프레임에 포함된 우안 영상과 좌안 영상을 특징 추출 모듈에 입력하여, 상기 우안 영상에 대응하는 우 특징 맵(feature map)과 상기 좌안 영상에 대응하는 좌 특징 맵을 산출하는 단계;상기 우 특징 맵과 상기 좌 특징 맵에 기초하여, 상기 좌안 영상과 상기 우안 영상 간의 매칭되는 픽셀들의 초기 매칭 비용 데이터(initial matching cost data)를 획득하는 단계;상기 초기 매칭 비용 데이터를 인공 신경망에 입력하여, 매칭 비용 데이터를 예측하는 단계;상기 매칭 비용 데이터에 기초하여, 상기 매칭되는 픽셀들 각각의 깊이 정보를 산출하는 단계; 및상기 각각의 깊이 정보에 기초하여, 상기 로컬 덴스 깊이 맵을 산출하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 특징 추출 모듈은상기 좌안 영상이 입력되는 좌 컨볼루션 인공 신경망과 상기 우안 영상이 입력되는 우 컨볼루션 인공 신경망을 포함하고,상기 좌 컨볼루션 인공 신경망과 상기 우 컨볼루션 인공 신경망은 가중치(weight)를 공유하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 초기 매칭 비용 데이터를 획득하는 단계는상기 우 특징 맵과 상기 좌 특징 맵을 연결하여, 상기 초기 매칭 비용 데이터를 획득하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>14. 제11항에 있어서,상기 매칭 비용 데이터를 예측하는 단계는모래시계(Hourglass) 인공 신경망을 및 초기 매칭 비용 데이터에 기초하여, 상기 매칭 비용 데이터를 예측하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>15. 제11항에 있어서,상기 깊이 정보를 산출하는 단계는컨볼루션 인공 신경망을 이용하여, 상기 매칭 비용 데이터에 대해 공간 컨볼루션 연산을 수행하는 단계;상기 공간 컨볼루션 연산 수행 결과에 기초하여, 상기 좌안 영상과 상기 우안 영상 간의 매칭되는 픽셀들의 시차를 추정하는 단계; 및상기 시차에 기초하여, 상기 깊이 정보를 산출하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서, 상기 공간 컨볼루션 연산을 수행하는 단계는상기 매칭 비용 데이터에 대해 설정된 방향에 따라, 상기 매칭 비용 데이터에 대해 분할을 진행하여 복수의 매칭 비용 레이어를 획득하는 단계; 및상기 복수의 매칭 비용 레이어 각각에 대하여, 상기 방향에 따라 차례대로 컨볼루션 연산을 수행하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 차례대로 컨볼루션 연산을 수행하는 단계는임의의 매칭 비용 레이어에 대해 컨볼루션 연산을 수행할 때, 상기 임의의 매칭 비용 레이어에 상기 임의의 매칭 비용 레이어 이전의 매칭 비용 레이어의 컨볼루션 결과를 누적한 후, 컨볼루션 연산을 수행하는 단계를 포함하는,</claim></claimInfo><claimInfo><claim>18. 제15항에 있어서,상기 시차를 추정하는 단계는상기 공간 컨볼루션 처리 결과 및 소프트맥스(softmax) 함수에 기초하여, 상기 좌안 영상과 상기 우안 영상 간의 매칭되는 픽셀들의 시차 확률 분포를 획득하는 단계; 및상기 시차 확률 분포에 기초하여, 상기 시차를 추정하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>19. 제1항에 있어서, 상기 동적 객체 영역을 추출하는 단계는상기 현재 프레임을 특징 추출 모듈에 입력하여, 상기 현재 프레임에 대응하는 특징 맵을 산출하는 단계;상기 특징 맵에 기초하여, 상기 현재 프레임에 포함된 객체들의 카테고리 속성(category attribute) 정보를 획득하는 단계; 및상기 카테고리 속성 정보에 기초하여, 상기 현재 프레임에 포함된 객체들의 상태 정보를 획득하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>20. 제19항에 있어서,상기 상태 정보를 획득하는 단계는상기 현재 프레임과 상기 현재 프레임의 이전 프레임 사이의 광류 정보(optical flow information)를 결정하는 단계; 및상기 광류 정보 및 상기 카테고리 속성 정보에 기초하여, 상기 상태 정보를 획득하는 단계를 포함하는, 깊이 맵 산출 방법.</claim></claimInfo><claimInfo><claim>21. 하드웨어와 결합되어 제1항 내지 제20항 중 어느 하나의 항의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>22. 현재 프레임을 포함하는 복수의 프레임들을 획득하는 카메라; 및상기 복수의 프레임들을 이용하여, 상기 현재 프레임에 대응하는 글로벌 스파스 깊이 맵(global sparse depth map)을 산출하고, 상기 현재 프레임을 이용하여, 상기 현재 프레임에 대응하는 로컬 덴스 깊이 맵(local dense depth map)을 산출하고, 상기 현재 프레임에서, 정적 객체(static object) 영역을 마스크(mask)하여 동적 객체 영역(none static object)을 추출하고, 상기 글로벌 스파스 깊이 맵에서 상기 동적 객체 영역을 제거하며, 상기 동적 객체 영역이 제거된 글로벌 스파스 깊이 맵과 상기 로컬 덴스 깊이 맵을 통합하여 상기 현재 프레임에 대응하는 글로벌 덴스 깊이 맵(global dense depth map)을 생성하는 프로세서를 포함하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>23. 제22항에 있어서,상기 프로세서는상기 현재 프레임에 포함된 하나 이상의 픽셀 포인트에 대응하는 깊이 정보를 산출하고, 상기 깊이 정보에 기초하여 상기 픽셀 포인트의 3차원 좌표를 산출하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>24. 제23항에 있어서,상기 프로세서는상기 글로벌 덴스 깊이 맵에 기초하여, 상기 현재 프레임에 대응하는 카메라의 포즈 정보를 업데이트하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>25. 제24항에 있어서,상기 프로세서는상기 업데이트가 완료된 카메라의 포즈 정보에 기초하여, 상기 글로벌 스파스 깊이 맵을 업데이트하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>26. 제22항에 있어서,상기 프로세서는상기 복수의 프레임들 중 키 프레임(key frame)에 대응하는 제1 깊이 정보를 산출하고, 상기 현재 프레임에 대응하는 제2 깊이 정보를 산출하고, 상기 제1 깊이 정보 및 상기 제2 깊이 정보에 기초하여, 상기 현재 프레임에 대응하는 카메라의 포즈 정보를 추정하며, 상기 제2 깊이 정보 및 상기 카메라의 포즈 정보에 기초하여, 상기 글로벌 스파스 깊이 맵을 산출하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>27. 제25항에 있어서,상기 프로세서는상기 현재 프레임에 포함된 우안 영상과 좌안 영상의 스테레오 매칭(stereo matching)을 수행하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>28. 제22항에 있어서,상기 프로세서는복수의 픽셀 포인트들을 포함하는 상기 현재 프레임을 인공 신경망에 입력함으로써, 상기 복수의 픽셀 포인트들의 깊이 정보에 대응하는 상기 인공 신경망의 출력들을 획득하고, 상기 출력들에 기초하여 상기 로컬 덴스 깊이 맵을 산출하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>29. 제22항에 있어서,상기 프로세서는상기 현재 프레임을 인공 신경망에 입력함으로써, 정적 객체 영역과 동적 객체 영역으로 분류된 상기 인공 신경망의 출력들을 획득하고, 상기 출력들에 기초하여 상기 동적 객체 영역을 추출하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>30. 제22항에 있어서,상기 프로세서는상기 로컬 덴스 깊이 맵을 복수의 그리드 셀들로 분할하고, 상기 동적 객체 영역이 제거된 글로벌 스파스 깊이 맵에 기초하여, 상기 그리드 셀들의 꼭지점들(corner points)에 대응하는 픽셀 포인트들의 깊이 정보를 업데이트하고, 상기 동적 객체 영역이 제거된 글로벌 스파스 깊이 맵 및 상기 업데이트된 꼭지점들에 대응하는 픽셀 포인트들의 깊이 정보에 기초하여, 상기 그리드 셀들의 내부 영역에 포함되는 픽셀 포인트들의 깊이 정보를 업데이트하는, 깊이 맵 산출 장치.</claim></claimInfo><claimInfo><claim>31. 제22항에 있어서,상기 프로세서는상기 현재 프레임에 포함된 우안 영상과 좌안 영상을 특징 추출 모듈에 입력하여, 상기 우안 영상에 대응하는 우 특징 맵(feature map)과 상기 좌안 영상에 대응하는 좌 특징 맵을 산출하고, 상기 우 특징 맵과 상기 좌 특징 맵에 기초하여, 상기 좌안 영상과 상기 우안 영상 간의 매칭되는 픽셀들의 초기 매칭 비용 데이터(initial matching cost data)를 획득하고, 상기 초기 매칭 비용 데이터를 인공 신경망에 입력하여, 매칭 비용 데이터를 예측하고, 상기 매칭 비용 데이터에 기초하여, 상기 매칭되는 픽셀들 각각의 깊이 정보를 산출하며, 상기 각각의 깊이 정보에 기초하여, 상기 로컬 덴스 깊이 맵을 산출하는, 깊이 맵 산출 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>중국 베이징 차오양 디스트릭스 타...</address><code> </code><country> </country><engName>Zhihua Liu</engName><name>지후아 리우</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code>420030299441</code><country>대한민국</country><engName>Kim Yun Tae</engName><name>김윤태</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code>420180786048</code><country>대한민국</country><engName>LEE, Hyong Euk</engName><name>이형욱</name></inventorInfo><inventorInfo><address>중국 베이징 차오양 디스트릭스 타...</address><code> </code><country> </country><engName>Lin Ma</engName><name>린 마</name></inventorInfo><inventorInfo><address>중국 베이징 차오양 디스트릭스 타...</address><code> </code><country> </country><engName>Qiang Wang</engName><name>치앙 왕</name></inventorInfo><inventorInfo><address>중국 베이징 차오양 디스트릭스 타...</address><code> </code><country> </country><engName>mao yamin</engName><name>마오 야민</name></inventorInfo><inventorInfo><address>중국 베이징 차오양 디스트릭스 타...</address><code> </code><country> </country><engName>Tianhao Gao</engName><name>티안하오 가오</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>중국</priorityApplicationCountry><priorityApplicationDate>2018.12.18</priorityApplicationDate><priorityApplicationNumber>201811550318.1</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2019.08.07</receiptDate><receiptNumber>1-1-2019-0810519-75</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2019.09.17</receiptDate><receiptNumber>9-1-2019-9007622-62</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2019.09.18</receiptDate><receiptNumber>9-1-2019-9007738-86</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2022.08.02</receiptDate><receiptNumber>1-1-2022-0805440-32</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Request for Prior Art Search</documentEngName><documentName>선행기술조사의뢰서</documentName><receiptDate>2024.03.18</receiptDate><receiptNumber>9-1-9999-9999999-89</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Report of Prior Art Search</documentEngName><documentName>선행기술조사보고서</documentName><receiptDate>2024.04.17</receiptDate><receiptNumber>9-6-2025-0077074-21</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for refusal</documentEngName><documentName>의견제출통지서</documentName><receiptDate>2025.04.22</receiptDate><receiptNumber>9-5-2025-0390190-50</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2025.05.26</receiptDate><receiptNumber>1-1-2025-0588085-50</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[거절이유 등 통지에 따른 의견]의견서·답변서·소명서</documentName><receiptDate>2025.05.26</receiptDate><receiptNumber>1-1-2025-0588084-15</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020190096306.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=2ba38663aa11ff0f6ca91af6061a2e00d9b942ad0c785fe720d0753df3090a8786572b7dc95afcd766a5bfb730e6e66110030ba73836030d5af543b6801ac5fcd0258bcfc93ab51c</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf15b229da37d447bcc1c367986794af75fbcfc81a364fe12abacc079da8fe3366861d1a1085c1e707392ad9ab44f55a01fc5e116e33a76c99</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>