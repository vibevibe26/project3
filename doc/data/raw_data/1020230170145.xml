<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:32.4132</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.11.29</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0170145</applicationNumber><claimCount>15</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>3D 오브젝트 인식을 위한 직접적 뉴럴 래디언스 필드 분류</inventionTitle><inventionTitleEng>Direct NeRF Classifi cation for 3D Object Recognition</inventionTitleEng><openDate>2025.06.05</openDate><openNumber>10-2025-0081620</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.11.29</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/84</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/64</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/764</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 실시예에 따르면, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치는, 명령어들을 저장하도록 구성되는 메모리; 및 상기 명령어들을 실행함으로써: 3D 오브젝트에 대해 생성되는 관측 시퀀스들에 각각 대응하는 NeRF 네트워크들을 프리-트레이닝된 프라이어 파라미터들로 초기화하고, 상기 프라이어 파라미터들에 의한 정칙화(regularization)를 규정하는 베이지안 방법론에 따라 상기 관측 시퀀스들을 이용하여 상기 NeRF 네트워크들을 트레이닝하고, 상기 NeRF 네트워크들에 대한 트레이닝을 통해 수렴하는 상기 NeRF 네트워크들의 네트워크 파라미터들을 산출하고, 상기 3D 오브젝트의 구조 및 형상을 암시적으로 표현하는 상기 네트워크 파라미터들을 직접 이용하여 상기 3D 오브젝트의 카테고리 및 인스턴스를 분류하도록 구성되는 프로세서; 를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치에 있어서,명령어들을 저장하도록 구성되는 메모리; 및상기 명령어들을 실행함으로써: 3D 오브젝트에 대해 생성되는 관측 시퀀스들에 각각 대응하는 NeRF 네트워크들을 프리-트레이닝된 프라이어 파라미터들로 초기화하고, 상기 프라이어 파라미터들에 의한 정칙화(regularization)를 규정하는 베이지안 방법론에 따라 상기 관측 시퀀스들을 이용하여 상기 NeRF 네트워크들을 트레이닝하고, 상기 NeRF 네트워크들에 대한 트레이닝을 통해 수렴하는 상기 NeRF 네트워크들의 네트워크 파라미터들을 산출하고, 상기 3D 오브젝트의 구조 및 형상을 암시적으로 표현하는 상기 네트워크 파라미터들을 직접 이용하여 상기 3D 오브젝트의 카테고리 및 인스턴스를 분류하도록 구성되는 프로세서; 를 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 프로세서는, 상기 네트워크 파라미터들(W)이 상기 프리-트레이닝된 프라이어 파라미터들(Wpre)에 기반하는 프라이어 분포(p(W))를 따르도록, 3D 구조물들이 존재하지 않는 빈 3D 공간을 표현하는 프리-트레이닝 데이터셋(Ipre)에 기초하여 상기 프리-트레이닝된 프라이어 파라미터들(Wpre)을 획득하도록 구성되는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 프리-트레이닝 데이터셋(Ipre)에 의해 표현되는 상기 빈 3D 공간은 카메라로부터의 레이가 3D 공간의 임의의 표면을 터치하여 NeRF 강도를 계산할 수 있도록 하는 바운디드 공간을 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서,상기 프로세서는, 상기 프라이어 분포(p(W))에 따라 상기 네트워크 파라미터들(W)을 제한함과 동시에 상기 NeRF 네트워크들이 상기 관측 시퀀스들을 나타낼 확률을 학습시킴으로써 상기 네트워크 파라미터들(W)을 수렴시키도록 구성되는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 프로세서는, 상기 프리-트레이닝된 프라이어 파라미터들(Wpre)을 통한 초기화에도 불구하고 파괴적 망각(catastrophic forgetting)을 회피하기 위해 상기 프리-트레이닝된 프라이어 파라미터들(Wpre) 및 상기 네트워크 파라미터들(W) 모두에 관하여 상기 NeRF 네트워크들을 트레이닝하도록 구성되는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 프로세서는, 로봇의 온-라인 학습을 위한 경량 분류기를 이용하여 상기 네트워크 파라미터들을 기반으로 상기 3D 오브젝트의 카테고리 및 인스턴스를 분류하도록 구성되고,상기 경량 분류기는 서포트 벡터 머신(SVM) 및 랜덤 포레스트를 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 장치.</claim></claimInfo><claimInfo><claim>7. 메모리에 저장되는 명령어들을 실행하는 프로세서에 의해 수행되는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 방법에 있어서,3D 오브젝트에 대해 생성되는 관측 시퀀스들에 각각 대응하는 NeRF 네트워크들을 프리-트레이닝된 프라이어 파라미터들로 초기화하는 단계;상기 프라이어 파라미터들에 의한 정칙화(regularization)를 규정하는 베이지안 방법론에 따라 상기 관측 시퀀스들을 이용하여 상기 NeRF 네트워크들을 트레이닝하는 단계;상기 NeRF 네트워크들에 대한 트레이닝을 통해 수렴하는 상기 NeRF 네트워크들의 네트워크 파라미터들을 산출하는 단계; 및상기 3D 오브젝트의 구조 및 형상을 암시적으로 표현하는 상기 네트워크 파라미터들을 직접 이용하여 상기 3D 오브젝트의 카테고리 및 인스턴스를 분류하는 단계; 를 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 프라이어 파라미터들로 초기화하는 단계는,상기 네트워크 파라미터들(W)이 상기 프리-트레이닝된 프라이어 파라미터들(Wpre)에 기반하는 프라이어 분포(p(W))를 따르도록, 3D 구조물들이 존재하지 않는 빈 3D 공간을 표현하는 프리-트레이닝 데이터셋(Ipre)에 기초하여 상기 프리-트레이닝된 프라이어 파라미터들(Wpre)을 획득하는 단계; 를 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서,상기 프리-트레이닝 데이터셋(Ipre)에 의해 표현되는 상기 빈 3D 공간은 카메라로부터의 레이가 3D 공간의 임의의 표면을 터치하여 NeRF 강도를 계산할 수 있도록 하는 바운디드 공간을 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 방법.</claim></claimInfo><claimInfo><claim>10. 제8항에 있어서,상기 NeRF 네트워크들을 트레이닝하는 단계는,상기 프라이어 분포(p(W))에 따라 상기 네트워크 파라미터들(W)을 제한함과 동시에 상기 NeRF 네트워크들이 상기 관측 시퀀스들을 나타낼 확률을 학습시킴으로써 상기 네트워크 파라미터들(W)을 수렴시키는 단계; 를 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 NeRF 네트워크들을 트레이닝하는 단계는,상기 프리-트레이닝된 프라이어 파라미터들(Wpre)을 통한 초기화에도 불구하고 파괴적 망각(catastrophic forgetting)을 회피하기 위해 상기 프리-트레이닝된 프라이어 파라미터들(Wpre) 및 상기 네트워크 파라미터들(W) 모두에 관하여 상기 NeRF 네트워크들을 트레이닝하는 단계; 를 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 방법.</claim></claimInfo><claimInfo><claim>12. 제7항에 있어서,상기 상기 3D 오브젝트의 카테고리 및 인스턴스를 분류하는 단계는,로봇의 온-라인 학습을 위한 경량 분류기를 이용하여 상기 네트워크 파라미터들을 기반으로 상기 3D 오브젝트의 카테고리 및 인스턴스를 분류하는 단계; 를 포함하고,상기 경량 분류기는 서포트 벡터 머신(SVM) 및 랜덤 포레스트를 포함하는, 3D 오브젝트 인식을 위한 직접적 NeRF 분류 방법.</claim></claimInfo><claimInfo><claim>13. 비일시적 컴퓨터 판독 가능한 기록 매체에 저장되는 컴퓨터 프로그램에 있어서,상기 컴퓨터 프로그램의 명령어들은, 적어도 하나의 프로세서에 의해 실행될 때, 상기 적어도 하나의 프로세서가:3D 오브젝트에 대해 생성되는 관측 시퀀스들에 각각 대응하는 NeRF 네트워크들을 프리-트레이닝된 프라이어 파라미터들로 초기화하는 동작;상기 프라이어 파라미터들에 의한 정칙화(regularization)를 규정하는 베이지안 방법론에 따라 상기 관측 시퀀스들을 이용하여 상기 NeRF 네트워크들을 트레이닝하는 동작;상기 NeRF 네트워크들에 대한 트레이닝을 통해 수렴하는 상기 NeRF 네트워크들의 네트워크 파라미터들을 산출하는 동작; 및상기 3D 오브젝트의 구조 및 형상을 암시적으로 표현하는 상기 네트워크 파라미터들을 직접 이용하여 상기 3D 오브젝트의 카테고리 및 인스턴스를 분류하는 동작; 을 수행하게 하는, 비일시적 컴퓨터 판독 가능한 기록 매체에 저장되는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 프라이어 파라미터들로 초기화하는 동작은,상기 네트워크 파라미터들(W)이 상기 프리-트레이닝된 프라이어 파라미터들(Wpre)에 기반하는 프라이어 분포(p(W))를 따르도록, 3D 구조물들이 존재하지 않는 빈 3D 공간을 표현하는 프리-트레이닝 데이터셋(Ipre)에 기초하여 상기 프리-트레이닝된 프라이어 파라미터들(Wpre)을 획득하는 동작; 을 포함하는, 비일시적 컴퓨터 판독 가능한 기록 매체에 저장되는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 프리-트레이닝 데이터셋(Ipre)에 의해 표현되는 상기 빈 3D 공간은 카메라로부터의 레이가 3D 공간의 임의의 표면을 터치하여 NeRF 강도를 계산할 수 있도록 하는 바운디드 공간을 포함하는, 비일시적 컴퓨터 판독 가능한 기록 매체에 저장되는 컴퓨터 프로그램.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>울산광역시 울주군...</address><code>120150812047</code><country>대한민국</country><engName>UNIST(ULSAN NATIONAL INSTITUTE OF SCIENCE AND TECHNOLOGY)</engName><name>울산과학기술원</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>울산광역시 울주군...</address><code> </code><country> </country><engName>YU Hyeon Woo</engName><name>유현우</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 선릉로***, *층(삼성동, 삼릉빌딩)</address><code>920201001611</code><country>대한민국</country><engName>THE WAVE IP LAW FIRM</engName><name>특허법인더웨이브</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.11.29</receiptDate><receiptNumber>1-1-2023-1339110-96</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>특허고객번호 정보변경(경정)신고서·정정신고서</documentName><receiptDate>2024.01.25</receiptDate><receiptNumber>4-1-2024-5044479-73</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230170145.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9304efce1a7c3df1c670de5c0af40ce0be915b6995defe53021bd80c72230d1229f95808d35aad7fd0f77c53c5cf052e259784d1d7b29e00a9</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf0858ab838100205e1f4605c16f6edfe293d531712e1a11c707ef4d197e5746ec5f55855dfe14354adfbd390dd6394e78c627be3c6ccf51fb</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>