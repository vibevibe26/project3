<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:37:49.3749</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2024.06.11</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-0075466</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>이미지 및 라이다 데이터를 이용한 학습 기반의 깊이 맵 추정 방법 및 장치</inventionTitle><inventionTitleEng>METHOD AND DEVICE WITH DEPTH MAP ESTIMATION BASED ON  LEARNING USING IMAGE AND LIDAR DATA</inventionTitleEng><openDate>2025.03.28</openDate><openNumber>10-2025-0043232</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G01S 17/89</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0895</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/084</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 일 실시예에 따른 전자 장치는, 입력 이미지 및 상기 입력 이미지에 대응하는 포인트 클라우드를 처리하고, 제1 깊이 맵을 생성하기 위해 상기 포인트 클라우드를 프로젝션하고, 상기 입력 이미지에 기초하여 상기 제1 깊이 맵에 새로운 깊이 값들을 추가하며, 입력 이미지들로부터 깊이 맵들을 추론하도록 구성된 깊이 추정 모델(depth estimation model)에 상기 입력 이미지를 입력함으로써 제2 깊이 맵을 획득하고, 상기 제1 깊이 맵 및 상기 제2 깊이 맵 간의 손실 차이에 기초하여 상기 깊이 추정 모델을 트레이닝시킬 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 전자 장치에 있어서,하나 이상의 프로세서; 및명령어들을 저장하는 메모리를 포함하고,상기 명령어들은 상기 하나 이상의 프로세서에 의해 실행될 시 상기 전자 장치로 하여금:입력 이미지 및 상기 입력 이미지에 대응하는 포인트 클라우드를 처리하고;상기 포인트 클라우드를 프로젝션하고 상기 입력 이미지에 기초하여 제1 깊이 맵의 일부 깊이 값들을 결정함으로써 제1 깊이 맵을 생성하며;이미지들로부터 깊이 맵들을 생성하도록 구성된 깊이 추정 모델(depth estimation model)에 상기 입력 이미지를 입력함으로써 제2 깊이 맵을 획득하고;상기 제1 깊이 맵 및 상기 제2 깊이 맵 간의 손실(loss)에 기초하여 상기 깊이 추정 모델을 트레이닝시키며;상기 트레이닝된 깊이 추정 모델을 통해 상기 입력 이미지에 대응하는 최종 깊이 맵을 생성하게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 명령어들은 상기 전자 장치로 하여금:깊이 이미지를 형성하기 위해 상기 포인트 클라우드를 프로젝션(projection)하고 상기 입력 이미지에 기초하여 상기 제1 깊이 맵으로 상기 깊이 이미지를 변환함으로써 상기 제1 깊이 맵을 생성하게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 명령어들은 상기 전자 장치로 하여금:상기 입력 이미지에 기초한 제1 이미지 필터(image filter)를 상기 깊이 이미지에 적용함으로써 상기 깊이 이미지를 상기 제1 깊이 맵으로 변환하게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서,상기 명령어들은 상기 전자 장치로 하여금:상기 입력 이미지에 시맨틱 분할(semantic segmentation)을 수행하여 시맨틱 분할 이미지를 생성하고, 상기 시맨틱 분할 이미지에 기초한 제2 이미지 필터를 상기 깊이 이미지에 적용함으로써 상기 깊이 이미지를 상기 제1 깊이 맵으로 변환하게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 명령어들은 상기 전자 장치로 하여금:상기 제1 깊이 맵에서 픽셀들의 깊이 값들 및 상기 제2 깊이 맵에서 대응하는 픽셀들의 깊이 값들 간의 차이들에 기초하여 상기 손실을 산출하고, 상기 산출된 손실을 상기 깊이 추정 모델의 출력 레이어로부터 입력 레이어로 역전파(back-propagation)시킴으로써 상기 깊이 추정 모델의 파라미터를 업데이트하게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 명령어들은 상기 전자 장치로 하여금:상기 입력 이미지를 상기 깊이 추정 모델에 반복적으로 입력하는 것에 기초하여, 상기 깊이 추정 모델의 파라미터를 반복하여 업데이트하게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 반복적으로 입력하는 것은,상기 입력 이미지를 상기 깊이 추정 모델에 반복적으로 입력하여 획득되는 제2 깊이 맵 및 상기 제1 깊이 맵 간의 해당하는 손실이 임계 손실 미만이라고 결정될 때까지, 수행되는,전자 장치.</claim></claimInfo><claimInfo><claim>8. 제6항에 있어서,상기 반복적으로 입력하는 것은,상기 반복적인 입력이 미리 설정된 반복 한계에 도달하는 것에 기초하여 종료되는,전자 장치.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 명령어들은 상기 전자 장치로 하여금:비디오 세그먼트 내에서 상기 입력 이미지 및 상기 입력 이미지와 인접한 하나 이상의 프레임 이미지를 사용하여 상기 깊이 추정 모델을 트레이닝시키게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 명령어들은 상기 전자 장치로 하여금:상기 입력 이미지 및 상기 입력 이미지에 대응하는 최종 깊이 맵에 기초하여 포인트 클라우드 정보를 생성하고, 상기 생성된 포인트 클라우드 정보를 사용하여 객체 검출(object detection)을 수행하게 하는,전자 장치.</claim></claimInfo><claimInfo><claim>11. 전자 장치의 프로세서에 의해 수행되는 방법에 있어서,입력 이미지 및 상기 입력 이미지에 대응하는 포인트 클라우드를 처리하는 단계;제1 깊이 맵을 생성하기 위해 상기 포인트 클라우드를 프로젝션하고, 상기 입력 이미지에 기초하여 상기 제1 깊이 맵에 새로운 깊이 값들을 추가하는 단계;입력 이미지들로부터 깊이 맵들을 추론하도록 구성된 깊이 추정 모델(depth estimation model)에 상기 입력 이미지를 입력함으로써 제2 깊이 맵을 획득하는 단계; 및상기 제1 깊이 맵 및 상기 제2 깊이 맵 간의 손실 차이에 기초하여 상기 깊이 추정 모델을 트레이닝시키는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 추가된 깊이 값들은 상기 입력 이미지의 색상 값들에 기초하여 산출되는,방법.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서,상기 제1 깊이 맵에 새로운 깊이 값들을 추가하는 단계는,상기 입력 이미지에 기초하여 제1 이미지 필터(image filter)를 상기 제1 깊이 맵에 적용하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>14. 제12항에 있어서,상기 제1 깊이 맵에 새로운 깊이 값들을 추가하는 단계는,상기 입력 이미지에 시맨틱 분할(semantic segmentation)을 수행하여 시맨틱 분할 이미지를 생성하는 단계; 및상기 시맨틱 분할 이미지에 기초하여 제2 이미지 필터를 상기 제1 깊이 맵에 적용함으로써 상기 제1 깊이 맵을 형성하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>15. 제11항에 있어서,상기 깊이 추정 모델을 트레이닝시키는 단계는,상기 제1 깊이 맵에서 픽셀의 깊이 값 및 상기 제2 깊이 맵에서 대응하는 픽셀의 깊이 값 간의 차이에 기초하여 상기 손실 차이를 산출하는 단계; 및상기 차이에 기초하여 상기 깊이 추정 모델의 파라미터를 업데이트하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>16. 제11항에 있어서,상기 깊이 추정 모델을 트레이닝시키는 단계는,상기 입력 이미지를 상기 깊이 추정 모델에 반복적으로 입력하는 것에 기초하여, 상기 깊이 추정 모델의 파라미터를 반복하여 업데이트하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 깊이 추정 모델의 파라미터의 반복적인 업데이트는,상기 입력 이미지를 상기 깊이 추정 모델에 반복적으로 입력하여 획득되는 임시 깊이 맵 및 상기 제1 깊이 맵 간의 손실이 임계 손실 미만이라고 결정하는 것에 기초하여 종료되는,방법.</claim></claimInfo><claimInfo><claim>18. 제16항에 있어서,상기 깊이 추정 모델의 파라미터의 반복적인 업데이트는,상기 입력 이미지를 상기 깊이 추정 모델에 반복적으로 입력하는 것이 미리 설정된 횟수로 수행되는 것에 기초하여, 종료되는,방법.</claim></claimInfo><claimInfo><claim>19. 제11항에 있어서,비디오 세그먼트에서 상기 입력 이미지 및 상기 입력 이미지와 인접한 하나 이상의 프레임 이미지를 사용하여 상기 깊이 추정 모델을 트레이닝시키는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>20. 제11항에 있어서,상기 입력 이미지 및 상기 입력 이미지에 대응하는 최종 깊이 맵에 기초하여 포인트 클라우드 정보를 생성하고, 상기 생성된 포인트 클라우드 정보를 사용하여 객체 검출(object detection)을 수행하는 단계를 더 포함하는 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 권선구...</address><code>420220461751</code><country>대한민국</country><engName>SON, Hyeong Seok</engName><name>손형석</name></inventorInfo><inventorInfo><address>경기도 용인시 수지구...</address><code>420170747129</code><country>대한민국</country><engName>Park, Seung-In</engName><name>박승인</name></inventorInfo><inventorInfo><address>서울특별시 강남구...</address><code>420170743681</code><country>대한민국</country><engName>YOO BYUNG IN</engName><name>유병인</name></inventorInfo><inventorInfo><address>경기도 수원시 권선구...</address><code>420190482041</code><country>대한민국</country><engName>LEE, Dong Wook</engName><name>이동욱</name></inventorInfo><inventorInfo><address>경기도 용인시 수지구...</address><code>420170731829</code><country>대한민국</country><engName>JUNG, Sang Il</engName><name>정상일</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2023.09.21</priorityApplicationDate><priorityApplicationNumber>1020230126512</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2024.04.10</priorityApplicationDate><priorityApplicationNumber>18/631,836</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2024.06.11</receiptDate><receiptNumber>1-1-2024-0626456-32</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Submission of Priority Certificate(USPTO)</documentEngName><documentName>우선권주장증명서류제출서(USPTO)</documentName><receiptDate>2024.06.14</receiptDate><receiptNumber>9-1-2024-9006279-89</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020240075466.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93b6507f49c774daca75ee85db31f62f7abee4b9d90b51db6ecb03c00f4b02b00caf23c767b9e6809fc861943f28ccdf00379a46ba2c64888b</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf00351b7d87ac40fc7a8bed4590c151595a77df22f28698daa2383997f3305c6ddcacef0bce6127db0b7dd484c74052cbd1fcc04f00d650cf</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>