<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:50.4150</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2020.04.16</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2020-0046268</applicationNumber><claimCount>21</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>증강 현실(AR) 디바이스 및 증강 현실 디바이스에서 포즈를 예측하는 방법</inventionTitle><inventionTitleEng>Augmented Reality (AR) device and method for  predicting pose thereof</inventionTitleEng><openDate>2021.10.26</openDate><openNumber>10-2021-0128269</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.04.06</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/223</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/55</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/09</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/044</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 증강 현실(AR) 디바이스 및 증강 현실(AR) 디바이스에서 포즈를 예측하는 방법은, IMU 레이트(IMU rate)로 AR 디바이스의 움직임에 대응하는 IMU 값들을 획득하고, VI-SLAM(Visual-Inertial Simultaneous Localization And Mapping) 모듈에서 IMU 값들 및 AR 디바이스 주변의 이미지들에 기초하여 AR 디바이스의 중간 6D (6 degrees of freedom) 포즈들을 추정하고, 딥 뉴럴 네트워크(DNN)를 이용하여 학습(learning)을 수행함으로써, AR 디바이스의 상대적 6D 포즈들(relative 6D poses)을 예측하는 포즈 예측 모델을 생성한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 증강 현실(AR) 디바이스에서 포즈를 예측하는 방법에 있어서,IMU(Inertial Measurement Unit) 센서를 이용하여, 제 1 주파수의 IMU 레이트(IMU rate)로 상기 AR 디바이스의 움직임에 대응하는 IMU 값들을 획득하는 단계;VI-SLAM(Visual-Inertial Simultaneous Localization And Mapping) 모듈에서, 상기 획득된 IMU 값들 및 상기 AR 디바이스에 구비된 카메라에 의해 제 2 주파수의 프레임 레이트(frame rate)로 획득된 상기 AR 디바이스 주변의 이미지들의 입력들에 기초하여, 상기 AR 디바이스의 중간 6D (6 degrees of freedom) 포즈들을 추정하는 단계; 및프로세서에서, 딥 뉴럴 네트워크(DNN)를 이용하여 상기 획득된 IMU 값들 및 상기 중간 6D 포즈들의 입력들에 기초한 학습(learning)을 수행함으로써, 상기 AR 디바이스의 상대적 6D 포즈들(relative 6D poses)을 예측하는 포즈 예측 모델을 생성하는 단계를 포함하는,방법.</claim></claimInfo><claimInfo><claim>2. 제 1 항에 있어서,상기 중간 6D 포즈들은상기 획득된 IMU 값들 및 상기 제 2 주파수의 프레임 레이트로 획득된 상기 이미지들을 융합하는 SLAM 기법을 수행함으로써 추정된 상기 프레임 레이트의 제 1 타입의 포즈 데이터이거나, 또는상기 SLAM 기법을 수행함으로써 추정된 상기 프레임 레이트의 이미지 기반 포즈 데이터 및 상기 획득된 IMU 값들로부터 프리-인테그레이션(pre-integration) 방식에 의해 계산된 적분 값들에 대응하는 포즈 데이터를 포함하는 상기 IMU 레이트의 제 2 타입의 포즈 데이터인,방법.</claim></claimInfo><claimInfo><claim>3. 제 1 항에 있어서,상기 포즈 예측 모델은슬라이딩 방식으로 이동하는 윈도우 내 k개의(k는 자연수) IMU 값들 및 m개의(m은 자연수) 중간 6D 포즈들의 입력들에 기초한 상기 딥 뉴럴 네트워크의 상기 학습을 수행함으로써, 각 윈도우에 대응하는 상대적 6D 포즈를 출력하는,방법.</claim></claimInfo><claimInfo><claim>4. 제 1 항에 있어서,상기 윈도우는상기 IMU 레이트에 기초하여 슬라이딩되어 상기 딥 뉴럴 네트워크로 입력되고,상기 포즈 예측 모델은상기 IMU 레이트로 상기 각 윈도우에 대응하는 상기 상대적 6D 포즈를 출력하는,방법.</claim></claimInfo><claimInfo><claim>5. 제 1 항에 있어서,상기 VI-SLAM 모듈에 의해 중간 6D 포즈에 대한 상기 추정이 완료될 때마다, 상기 추정된 중간 6D 포즈를 이미지가 촬영된 실제 시점에서의 그라운드 트루스(ground truth, GT) 포즈로 갱신하여 설정하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>6. 제 5 항에 있어서,상기 포즈 예측 모델은상기 GT 포즈의 갱신에 기초하여 자가 지도(self-supervised) 방식으로 상기 딥 뉴럴 네트워크의 상기 학습을 수행하는,방법.</claim></claimInfo><claimInfo><claim>7. 제 1 항에 있어서,상기 포즈 예측 모델은상기 IMU 레이트의 IMU 입력들 및 상기 프레임 레이트의 중간 6D 포즈 입력들을 포함하는 멀티-레이트(multi-rate) 입력들로 상기 딥 뉴럴 네트워크의 상기 학습을 수행하고,상기 제 1 주파수는 상기 제 2 주파수보다 큰 것인,방법.</claim></claimInfo><claimInfo><claim>8. 제 1 항에 있어서,상기 딥 뉴럴 네트워크는LSTM (Long Short Term Memory) 또는 GRU (Gated Recurrent Unit)를 이용한 RNN(Recurrent Neural Network)인,방법.</claim></claimInfo><claimInfo><claim>9. 제 8 항에 있어서,상기 딥 뉴럴 네트워크는갱신된 GT 포즈를 이용하여 손실 함수(Loss function)를 최소화함으로써 BPTT(back propagation through time)를 수행하는,방법.</claim></claimInfo><claimInfo><claim>10. 제 1 항에 있어서,상기 포즈 예측 모델의 상기 학습된 딥 뉴럴 네트워크에 상기 AR 디바이스의 움직임에 따른 새로 획득된 IMU 값들 및 새로 추정된 중간 6D 포즈들이 입력되는 경우, 상기 포즈 예측 모델의 상기 학습된 딥 뉴럴 네트워크에 의한 처리를 통해 실시간으로 상대적 6D 포즈들을 예측한 추론 결과를 출력하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>11. 제 1 항 내지 제 10 항 중에 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 비일시적인(non-transitory) 기록매체.</claim></claimInfo><claimInfo><claim>12. 증강 현실(AR) 디바이스에 있어서,제 1 주파수의 IMU 레이트(IMU rate)로 상기 AR 디바이스의 움직임에 대응하는 IMU 값들을 획득하는 IMU(Inertial Measurement Unit) 센서;상기 획득된 IMU 값들 및 상기 AR 디바이스에 구비된 카메라에 의해 제 2 주파수의 프레임 레이트(frame rate)로 획득된 상기 AR 디바이스 주변의 이미지들의 입력들에 기초하여, 상기 AR 디바이스의 중간 6D (6 degrees of freedom) 포즈들을 추정하는 VI-SLAM(Visual-Inertial Simultaneous Localization And Mapping) 모듈; 및딥 뉴럴 네트워크(DNN)를 이용하여 상기 획득된 IMU 값들 및 상기 중간 6D 포즈들의 입력들에 기초한 학습(learning)을 수행함으로써, 상기 AR 디바이스의 상대적 6D 포즈들(relative 6D poses)을 예측하는 포즈 예측 모델을 생성하는 프로세서를 포함하는,AR 디바이스.</claim></claimInfo><claimInfo><claim>13. 제 12 항에 있어서,상기 포즈 예측 모델은슬라이딩 방식으로 이동하는 윈도우 내 k개의(k는 자연수) IMU 값들 및 m개의(m은 자연수) 중간 6D 포즈들의 입력들에 기초한 상기 딥 뉴럴 네트워크의 상기 학습을 수행함으로써, 각 윈도우에 대응하는 상대적 6D 포즈를 출력하는,AR 디바이스.</claim></claimInfo><claimInfo><claim>14. 제 12 항에 있어서,상기 윈도우는상기 IMU 레이트에 기초하여 슬라이딩되어 상기 딥 뉴럴 네트워크로 입력되고,상기 포즈 예측 모델은상기 IMU 레이트로 상기 각 윈도우에 대응하는 상기 상대적 6D 포즈를 출력하는,AR 디바이스.</claim></claimInfo><claimInfo><claim>15. 제 12 항에 있어서,상기 프로세서는상기 VI-SLAM 모듈에 의해 중간 6D 포즈에 대한 상기 추정이 완료될 때마다, 상기 추정된 중간 6D 포즈를 이미지가 촬영된 실제 시점에서의 그라운드 트루스(ground truth, GT) 포즈로 갱신하여 설정하는,AR 디바이스.</claim></claimInfo><claimInfo><claim>16. 제 15 항에 있어서,상기 포즈 예측 모델은상기 GT 포즈의 갱신에 기초하여 자가 지도(self-supervised) 방식으로 상기 딥 뉴럴 네트워크의 상기 학습을 수행하는,AR 디바이스.</claim></claimInfo><claimInfo><claim>17. 제 12 항에 있어서,상기 포즈 예측 모델은상기 IMU 레이트의 IMU 입력들 및 상기 프레임 레이트의 중간 6D 포즈 입력들을 포함하는 멀티-레이트(multi-rate) 입력들로 상기 딥 뉴럴 네트워크의 상기 학습을 수행하고,상기 제 1 주파수는 상기 제 2 주파수보다 큰 것인,AR 디바이스.</claim></claimInfo><claimInfo><claim>18. 제 12 항에 있어서,상기 딥 뉴럴 네트워크는LSTM (Long Short Term Memory) 또는 GRU (Gated Recurrent Unit)를 이용한 RNN(Recurrent Neural Network)인,AR 디바이스.</claim></claimInfo><claimInfo><claim>19. 제 18 항에 있어서,상기 딥 뉴럴 네트워크는갱신된 GT 포즈를 이용하여 손실 함수(Loss function)를 최소화함으로써 BPTT(back propagation through time)를 수행하는,AR 디바이스.</claim></claimInfo><claimInfo><claim>20. 제 12 항에 있어서,상기 프로세서는상기 포즈 예측 모델의 상기 학습된 딥 뉴럴 네트워크에 상기 AR 디바이스의 움직임에 따른 새로 획득된 IMU 값들 및 새로 추정된 중간 6D 포즈들이 입력되는 경우, 상기 포즈 예측 모델의 상기 학습된 딥 뉴럴 네트워크에 의한 처리를 통해 실시간으로 상대적 6D 포즈들을 예측한 추론 결과를 출력하는,AR 디바이스.</claim></claimInfo><claimInfo><claim>21. 프로세서에서 포즈를 예측하는 방법에 있어서,IMU(Inertial Measurement Unit) 센서로부터, 제 1 주파수의 IMU 레이트(IMU rate)로 AR 디바이스의 움직임에 대응하는 IMU 값들을 획득하는 단계;VI-SLAM(Visual-Inertial Simultaneous Localization And Mapping) 모듈로부터, 상기 획득된 IMU 값들 및 상기 AR 디바이스에 구비된 카메라에 의해 제 2 주파수의 프레임 레이트(frame rate)로 획득된 주변의 이미지들의 입력들에 기초하여 추정된, 상기 AR 디바이스의 중간 6D (6 degrees of freedom) 포즈들을 획득하는 단계; 및딥 뉴럴 네트워크(DNN)를 이용하여 상기 획득된 IMU 값들 및 상기 중간 6D 포즈들의 입력들에 기초한 학습(learning)을 수행함으로써, 상기 AR 디바이스의 상대적 6D 포즈들(relative 6D poses)을 예측하는 포즈 예측 모델을 생성하는 단계를 포함하는,방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo><applicantInfo><address>서울특별시 성북구...</address><code>220040170680</code><country>대한민국</country><engName>Korea University Research and Business Foundation</engName><name>고려대학교 산학협력단</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code>420030299441</code><country>대한민국</country><engName>Kim Yun Tae</engName><name>김윤태</name></inventorInfo><inventorInfo><address>서울특별시 성북구...</address><code> </code><country> </country><engName>SULL, Sang Hoon</engName><name>설상훈</name></inventorInfo><inventorInfo><address>대구광역시 수성구...</address><code>419980555925</code><country>대한민국</country><engName>SUNG, Ki Young</engName><name>성기영</name></inventorInfo><inventorInfo><address>서울특별시 강남구...</address><code>420170655378</code><country>대한민국</country><engName>Lee, Hong-Seok</engName><name>이홍석</name></inventorInfo><inventorInfo><address>경기도 용인시 수지구...</address><code> </code><country> </country><engName>JEON, Myung Jae</engName><name>전명제</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2020.04.16</receiptDate><receiptNumber>1-1-2020-0394040-84</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2023.04.06</receiptDate><receiptNumber>1-1-2023-0388806-68</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020200046268.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93f89a37457391cebe109d116fa55d7ca6542d3528faf07c49d7254d185211439b9734c2b5bf480ed7db33d8d52b3b196581db65538696b9fb</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf0da3e2fcde7a1b79c53508e77ee4603939683296386da3f7a8765768253fc828075bfaffbc3e8dbd5bf548c7a93684715c33ad59ea5d6b5d</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>