<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:40:11.4011</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.08.31</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2025-7010596</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>물체 캡처를 위해 손들을 사용하는 3D 공간 조각</inventionTitle><inventionTitleEng>3D SPACE CARVING USING HANDS FOR OBJECT CAPTURE</inventionTitleEng><openDate>2025.04.30</openDate><openNumber>10-2025-0058761</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.04.01</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.04.01</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/292</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/564</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/64</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G02B 27/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G02B 27/01</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 손 추적을 사용하여 3D 공간을 조각하기 위한 방법이 설명된다. 일 양태에서, 방법은 디스플레이 디바이스의 카메라로부터의 제1 프레임에 액세스하는 단계, 디스플레이 디바이스에서 동작하는 손 추적 알고리즘을 사용하여, 제1 프레임에 묘사된 하나 이상의 사용자 손에 대응하는 손 픽셀들을 추적하는 단계, 디스플레이 디바이스의 센서를 사용하여, 손 픽셀들의 깊이들을 검출하는 단계, 손 픽셀들의 깊이들에 기초하여 3D 영역을 식별하는 단계, 및 3D 재구성 엔진을 3D 영역에 적용하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2024.03.07</internationOpenDate><internationOpenNumber>WO2024050460</internationOpenNumber><internationalApplicationDate>2023.08.31</internationalApplicationDate><internationalApplicationNumber>PCT/US2023/073217</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 방법으로서,디스플레이 디바이스의 카메라로부터의 제1 프레임에 액세스하는 단계;상기 디스플레이 디바이스에서 동작하는 손 추적 알고리즘을 사용하여, 상기 제1 프레임에 묘사된 하나 이상의 사용자 손에 대응하는 손 픽셀들을 추적하는 단계;상기 디스플레이 디바이스의 센서를 사용하여, 상기 손 픽셀들의 깊이들을 검출하는 단계;상기 손 픽셀들의 상기 깊이들에 기초하여 3D 영역을 식별하는 단계; 및상기 3D 영역에 3D 재구성 엔진을 적용하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 3D 영역은 상기 카메라와 상기 하나 이상의 사용자 손 사이에 점유되지 않은 3D 공간을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 센서는 깊이 센서 또는 스테레오 카메라들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 깊이들을 검출하는 단계는 2개의 이미지들에서의 상기 하나 이상의 사용자 손의 윤곽 매칭에 기초하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 3D 영역을 식별하는 단계는:상기 하나 이상의 사용자 손의 모션을 추적하는 단계; 및상기 하나 이상의 사용자 손의 상기 모션에 기초하여 물리적 객체를 포함하는 3D 엔벨로프를 식별하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 3D 영역에 3D 재구성 엔진을 적용하는 단계는:상기 3D 엔벨로프로부터의 포인트 클라우드 데이터에 기초하여 상기 3D 엔벨로프에 포함된 상기 물리적 객체의 3D 모델을 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 물리적 객체의 상기 3D 모델에 기초하여 상기 물리적 객체를 식별하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 물리적 객체 또는 상기 물리적 객체의 상기 3D 모델에 대응하는 가상 콘텐츠를 식별하는 단계; 및상기 디스플레이 디바이스의 디스플레이에서, 상기 가상 콘텐츠를 상기 물리적 객체에 대한 오버레이로서 디스플레이하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 3D 영역을 식별하는 단계는 상기 하나 이상의 사용자 손의 모션에 기초하고,상기 제1 프레임 내의 상기 하나 이상의 사용자 손의 로케이션에 기초하여 제1 관심 영역을 식별하기 위해 상기 제1 프레임의 제1 부분을 필터링하는 단계;제2 프레임 내의 상기 하나 이상의 사용자 손의 로케이션에 기초하여 제2 관심 영역을 식별하기 위해 상기 제2 프레임의 제2 부분을 필터링하는 단계;상기 제1 프레임에서 상기 하나 이상의 사용자 손의 제1 손 픽셀 깊이들을 식별하는 단계;상기 제2 프레임에서 상기 하나 이상의 사용자 손의 제2 손 픽셀 깊이들을 식별하는 단계; 및상기 제1 관심 영역, 상기 제2 관심 영역, 상기 제1 손 픽셀 깊이들, 및 상기 제2 손 픽셀 깊이들에 기초하여 상기 3D 영역을 식별하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 3D 영역에 3D 재구성 엔진을 적용하는 단계는:상기 3D 영역 외부의 3D 공간을 배제하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>11. 컴퓨팅 장치로서,프로세서; 및명령어들을 저장하는 메모리를 포함하고, 상기 명령어들은, 상기 프로세서에 의해 실행될 때, 상기 장치를:디스플레이 디바이스의 카메라로부터의 제1 프레임에 액세스하고;상기 디스플레이 디바이스에서 동작하는 손 추적 알고리즘을 사용하여, 상기 제1 프레임에 묘사된 하나 이상의 사용자 손에 대응하는 손 픽셀들을 추적하고;상기 디스플레이 디바이스의 센서를 사용하여, 상기 손 픽셀들의 깊이들을 검출하고;상기 손 픽셀들의 상기 깊이들에 기초하여 3D 영역을 식별하고;상기 3D 영역에 3D 재구성 엔진을 적용하도록구성하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 3D 영역은 상기 카메라와 상기 하나 이상의 사용자 손 사이에 점유되지 않은 3D 공간을 포함하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 센서는 깊이 센서 또는 스테레오 카메라들을 포함하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>14. 제11항에 있어서,상기 깊이들을 검출하는 것은 2개의 이미지들에서의 상기 하나 이상의 사용자 손의 윤곽 매칭에 기초하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>15. 제11항에 있어서,상기 3D 영역을 식별하는 것은:상기 하나 이상의 사용자 손의 모션을 추적하고;상기 하나 이상의 사용자 손의 상기 모션에 기초하여 물리적 객체를 포함하는 3D 엔벨로프를 식별하는것을 포함하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 3D 영역에 3D 재구성 엔진을 적용하는 것은:상기 3D 엔벨로프로부터의 포인트 클라우드 데이터에 기초하여 상기 3D 엔벨로프에 포함된 상기 물리적 객체의 3D 모델을 생성하는 것을 포함하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 명령어들은, 상기 장치를 상기 물리적 객체의 상기 3D 모델에 기초하여 상기 물리적 객체를 식별하도록 추가로 구성하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>18. 제17항에 있어서,상기 명령어들은, 상기 장치를:상기 물리적 객체 또는 상기 물리적 객체의 상기 3D 모델에 대응하는 가상 콘텐츠를 식별하고;상기 디스플레이 디바이스의 디스플레이에서, 상기 가상 콘텐츠를 상기 물리적 객체에 대한 오버레이로서 디스플레이하도록추가로 구성하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>19. 제11항에 있어서,상기 3D 영역을 식별하는 것은 상기 하나 이상의 사용자 손의 모션에 기초하고,상기 제1 프레임 내의 상기 하나 이상의 사용자 손의 로케이션에 기초하여 제1 관심 영역을 식별하기 위해 상기 제1 프레임의 제1 부분을 필터링하고;제2 프레임 내의 상기 하나 이상의 사용자 손의 로케이션에 기초하여 제2 관심 영역을 식별하기 위해 상기 제2 프레임의 제2 부분을 필터링하고;상기 제1 프레임에서 상기 하나 이상의 사용자 손의 제1 손 픽셀 깊이들을 식별하고;상기 제2 프레임에서 상기 하나 이상의 사용자 손의 제2 손 픽셀 깊이들을 식별하고;상기 제1 관심 영역, 상기 제2 관심 영역, 상기 제1 손 픽셀 깊이들, 및 상기 제2 손 픽셀 깊이들에 기초하여 상기 3D 영역을 식별하는것을 포함하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>20. 비일시적 컴퓨터 판독 가능 저장 매체로서,상기 컴퓨터 판독 가능 저장 매체는 명령어들을 포함하고, 상기 명령어들은, 컴퓨터에 의해 실행될 때, 상기 컴퓨터로 하여금:디스플레이 디바이스의 카메라로부터의 제1 프레임에 액세스하고;상기 디스플레이 디바이스에서 동작하는 손 추적 알고리즘을 사용하여, 상기 제1 프레임에 묘사된 하나 이상의 사용자 손에 대응하는 손 픽셀들을 추적하고;상기 디스플레이 디바이스의 센서를 사용하여, 상기 손 픽셀들의 깊이들을 검출하고;상기 손 픽셀들의 상기 깊이들에 기초하여 3D 영역을 식별하고;상기 3D 영역에 3D 재구성 엔진을 적용하도록하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 캘리포니아주 산타 모니카 써티퍼스트 스트리트 ****</address><code>520140253774</code><country>미국</country><engName>SNAP INC.</engName><name>스냅 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>슬로바키아</country><engName>MICUSIK, Branislav</engName><name>미쿠식, 브라니슬라프</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>그리스</country><engName>EVANGELIDIS, Georgios</engName><name>에반젤리디스, 게오르기오스</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>오스트리아</country><engName>WOLF, Daniel</engName><name>울프, 다니엘</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920010003368</code><country>대한민국</country><engName>Kim Yeon Song</engName><name>김연송</name></agentInfo><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>그리스</priorityApplicationCountry><priorityApplicationDate>2022.09.01</priorityApplicationDate><priorityApplicationNumber>20220100720</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.10.25</priorityApplicationDate><priorityApplicationNumber>17/973,167</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2025.04.01</receiptDate><receiptNumber>1-1-2025-0364244-26</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2025.04.03</receiptDate><receiptNumber>1-5-2025-0055860-65</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257010596.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93b53d90986f2ac3b230c24d3807bd5f9c1b980ab7b129158625d83fc91a48a46b7683322d61dba38350f28934b1fef4aebb6f8ac88f4c36cb</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf0839665a5efeb82426d938be1210ea89d0c0ee0a0a10476fc52a0a916c9f956667ad955aae358d607e63d387ce315a90191d9eeb0e912d2d</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>