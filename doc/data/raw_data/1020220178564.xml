<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:06:16.616</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.12.19</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0178564</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>뷰 증강 기반의 뉴럴 렌더링 방법 및 장치</inventionTitle><inventionTitleEng>METHOD AND APPRATUS OF NEURAL RENDERING BASED ON VIEW  AUGMENTATION</inventionTitleEng><openDate>2024.04.16</openDate><openNumber>10-2024-0049098</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 15/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 15/10</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/194</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 3/18</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/75</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 15/06</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 뷰 증강 기반의 뉴럴 렌더링 방법 및 장치가 제공된다. 그 방법은 타겟 장면에 대한 기본 뷰들(base views)의 원본 트레이닝 영상들을 수신하고, 원본 트레이닝 영상들을 와핑하여 타겟 장면에 대한 새로운 뷰들의 증강 영상들을 생성하고, 원본 트레이닝 영상들 및 증강 영상들에 대한 의미론적 분할을 수행하여 원본 트레이닝 영상들 및 증강 영상들 각각의 전경 영역과 원본 트레이닝 영상들 및 증강 영상들 각각의 배경 영역을 구분하는 분할 마스크들을 결정하고, 타겟 장면에 대한 볼륨 렌더링에 이용되는 뉴럴 장면 표현 모델을 원본 트레이닝 영상들, 증강 영상들, 및 분할 마스크들을 이용하여 트레이닝하는 단계들을 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 타겟 장면에 대한 기본 뷰들(base views)의 원본 트레이닝 영상들을 수신하는 단계;상기 원본 트레이닝 영상들을 와핑하여 상기 타겟 장면에 대한 새로운 뷰들의 증강 영상들을 생성하는 단계;상기 원본 트레이닝 영상들 및 상기 증강 영상들에 대한 의미론적 분할을 수행하여 상기 원본 트레이닝 영상들 및 상기 증강 영상들 각각의 전경 영역과 상기 원본 트레이닝 영상들 및 상기 증강 영상들 각각의 배경 영역을 구분하는 분할 마스크들을 결정하는 단계; 및상기 타겟 장면에 대한 볼륨 렌더링에 이용되는 뉴럴 장면 표현 모델을 상기 원본 트레이닝 영상들, 상기 증강 영상들, 및 상기 분할 마스크들을 이용하여 트레이닝하는 단계를 포함하는 트레이닝 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 증강 영상들을 생성하는 단계는상기 원본 트레이닝 영상들의 제1 원본 트레이닝 영상의 제1 기본 뷰의 카메라 포즈를 상기 증강 영상들의 제1 증강 영상의 제1 새로운 뷰의 카메라 포즈로 변환하는 변환 함수를 결정하는 단계; 및상기 제1 원본 트레이닝 영상의 카메라 내부 파라미터, 상기 제1 원본 트레이닝 영상에 대응하는 원본 깊이 맵, 및 상기 변환 함수를 이용하여 상기 제1 원본 트레이닝 영상을 와핑하여 상기 제1 증강 영상을 생성하는 단계를 포함하는, 트레이닝 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 뉴럴 장면 표현 모델을 트레이닝하는 단계는상기 원본 트레이닝 영상들, 상기 증강 영상들, 상기 분할 마스크들, 및 제1 손실 함수를 이용하여 상기 뉴럴 장면 표현 모델의 1차 트레이닝을 수행하는 단계; 및상기 원본 트레이닝 영상들, 및 제2 손실 함수를 이용하여 상기 뉴럴 장면 표현 모델의 2차 트레이닝을 수행하는 단계를 포함하는, 트레이닝 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 제1 손실 함수는상기 원본 트레이닝 영상들 및 상기 증강 영상들의 실제 픽셀 값과 상기 뉴럴 장면 표현 모델에 의해 추정된 픽셀 값 간의 픽셀 오차를 기반으로 정의되고,상기 제2 손실 함수는상기 원본 트레이닝 영상들과 상기 뉴럴 장면 표현 모델에 의해 추정된 합성 영상 간의 픽셀 오차, 상기 원본 트레이닝 영상들과 상기 합성 영상 간의 의미론적 일관성, 및 광선을 따른 투과율의 불확실성을 기반으로 정의되는,트레이닝 방법.</claim></claimInfo><claimInfo><claim>5. 제3항에 있어서,상기 1차 트레이닝을 수행하는 단계는상기 원본 트레이닝 영상들 및 상기 증강 영상들로부터 제1 샘플 영상을 선택하는 단계;제1 광선을 지시하는 제1 쿼리 입력에 따른 상기 뉴럴 장면 표현 모델의 제1 쿼리 출력을 결정하는 단계;상기 분할 마스크들에 기초하여 상기 제1 샘플 영상의 전경 영역 및 상기 제1 샘플 영상의 배경 영역 중 상기 제1 광선이 속하는 타겟 영역을 결정하는 단계; 및상기 제1 광선에 의해 특정되는 상기 타겟 영역의 제1 픽셀의 실제 픽셀 값과 상기 제1 쿼리 출력에 따른 추정 픽셀 값 간의 픽셀 오차에 기초하여 상기 제1 손실 함수의 손실 값을 결정하는 단계를 포함하는, 트레이닝 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 타겟 영역을 결정하는 단계는상기 분할 마스크들 중 상기 제1 샘플 영상에 대응하는 제1 분할 마스크를 상기 제1 샘플 영상에 적용하여 상기 제1 샘플 영상의 상기 전경 영역 및 상기 제1 샘플 영상의 상기 배경 영역을 구분하는 단계;상기 제1 광선이 상기 제1 샘플 영상의 상기 전경 영역을 지시하는 경우, 상기 제1 샘플 영상의 상기 전경 영역을 상기 타겟 영역으로 결정하는 단계; 및상기 제1 광선이 상기 제1 샘플 영상의 상기 배경 영역을 지시하는 경우, 상기 제1 샘플 영상의 상기 배경 영역이 상기 타겟 영역으로 결정하는 단계를 포함하는, 트레이닝 방법.</claim></claimInfo><claimInfo><claim>7. 제3항에 있어서,상기 2차 트레이닝을 수행하는 단계는상기 뉴럴 장면 표현 모델을 이용하여 상기 원본 트레이닝 영상들의 제1 원본 트레이닝 영상의 제1 광선 세트에 따른 제1 합성 영상을 생성하는 단계; 및상기 제1 원본 트레이닝 영상의 멀티 레벨 패치들의 제1 의미론적 특징들 및 상기 제1 합성 영상의 멀티 레벨 패치들의 제2 의미론적 특징들을 추정하는 단계;상기 제1 의미론적 특징들 및 상기 제2 의미론적 특징들 간의 차이에 따라 상기 제1 원본 트레이닝 영상과 상기 제1 합성 영상 간의 의미론적 일관성을 결정하는 단계; 및상기 결정된 의미론적 일관성에 기초하여 상기 제2 손실 함수의 손실 값을 결정하는 단계를 포함하는, 트레이닝 방법.</claim></claimInfo><claimInfo><claim>8. 제3항에 있어서,상기 2차 트레이닝을 수행하는 단계는상기 원본 트레이닝 영상들의 제1 원본 트레이닝 영상의 광선들의 샘플 지점들의 투과도와 볼륨 밀도의 곱에 따른 상기 샘플 지점들의 가중치들을 결정하는 단계; 및상기 광선들의 상기 가중치들에 기초하여 상기 제2 손실 함수의 손실 값을 결정하는 단계를 포함하는, 트레이닝 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 원본 트레이닝 영상들의 개수는 미리 정해진 수로 제한되는,트레이닝 방법.</claim></claimInfo><claimInfo><claim>10. 하드웨어와 결합되어 제1항 내지 제9항 중 어느 하나의 항의 방법을 실행시키기 위하여 컴퓨터 판독 가능한 기록매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>11. 프로세서; 및상기 프로세서에서 실행가능한 명령어들을 포함하는 메모리를 포함하고,상기 명령어들이 상기 프로세서에서 실행되면, 상기 프로세서는타겟 장면에 대한 기본 뷰들(base views)의 원본 트레이닝 영상들을 수신하고,상기 원본 트레이닝 영상들을 와핑하여 상기 타겟 장면에 대한 새로운 뷰들의 증강 영상들을 생성하고,상기 원본 트레이닝 영상들 및 상기 증강 영상들에 대한 의미론적 분할을 수행하여 상기 원본 트레이닝 영상들 및 상기 증강 영상들 각각의 전경 영역과 상기 원본 트레이닝 영상들 및 상기 증강 영상들 각각의 배경 영역을 구분하는 분할 마스크들을 결정하고,상기 타겟 장면에 대한 볼륨 렌더링에 이용되는 뉴럴 장면 표현 모델을 상기 원본 트레이닝 영상들, 상기 증강 영상들, 및 상기 분할 마스크들을 이용하여 트레이닝하는,트레이닝 장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 프로세서는, 상기 증강 영상들을 생성하기 위해,상기 원본 트레이닝 영상들의 제1 원본 트레이닝 영상의 제1 기본 뷰의 카메라 포즈를 상기 증강 영상들의 제1 증강 영상의 제1 새로운 뷰의 카메라 포즈로 변환하는 변환 함수를 결정하고,상기 제1 원본 트레이닝 영상의 카메라 내부 파라미터, 상기 제1 원본 트레이닝 영상에 대응하는 원본 깊이 맵, 및 상기 변환 함수를 이용하여 상기 제1 원본 트레이닝 영상을 와핑하여 상기 제1 증강 영상을 생성하는,트레이닝 장치.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 프로세서는, 상기 뉴럴 장면 표현 모델을 트레이닝하기 위해,상기 원본 트레이닝 영상들, 상기 증강 영상들, 상기 분할 마스크들, 및 제1 손실 함수를 이용하여 상기 뉴럴 장면 표현 모델의 1차 트레이닝을 수행하고,상기 원본 트레이닝 영상들, 및 제2 손실 함수를 이용하여 상기 뉴럴 장면 표현 모델의 2차 트레이닝을 수행하는,트레이닝 장치.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 제1 손실 함수는상기 원본 트레이닝 영상들 및 상기 증강 영상들의 실제 픽셀 값과 상기 뉴럴 장면 표현 모델에 의해 추정된 픽셀 값 간의 픽셀 오차를 기반으로 정의되고,상기 제2 손실 함수는상기 원본 트레이닝 영상들과 상기 뉴럴 장면 표현 모델에 의해 추정된 합성 영상 간의 픽셀 오차, 및 상기 원본 트레이닝 영상들과 상기 합성 영상 간의 의미론적 일관성, 및 광선을 따른 투과율의 불확실성을 기반으로 정의되는,트레이닝 장치.</claim></claimInfo><claimInfo><claim>15. 제13항에 있어서,상기 프로세서는, 상기 1차 트레이닝을 수행하기 위해,상기 원본 트레이닝 영상들 및 상기 증강 영상들로부터 제1 샘플 영상을 선택하고,제1 광선을 지시하는 제1 쿼리 입력에 따른 상기 뉴럴 장면 표현 모델의 제1 쿼리 출력을 결정하고,상기 분할 마스크들에 기초하여 상기 제1 샘플 영상의 전경 영역 및 상기 제1 샘플 영상의 배경 영역 중 상기 제1 광선이 속하는 타겟 영역을 결정하고,상기 제1 광선에 의해 특정되는 상기 타겟 영역의 제1 픽셀의 실제 픽셀 값과 상기 제1 쿼리 출력에 따른 추정 픽셀 값 간의 픽셀 오차에 기초하여 상기 제1 손실 함수의 손실 값을 결정하는,트레이닝 장치.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 프로세서는, 상기 타겟 영역을 결정하기 위해,상기 분할 마스크들 중 상기 제1 샘플 영상에 대응하는 제1 분할 마스크를 상기 제1 샘플 영상에 적용하여 상기 제1 샘플 영상의 상기 전경 영역 및 상기 제1 샘플 영상의 상기 배경 영역을 구분하고,상기 제1 광선이 상기 제1 샘플 영상의 상기 전경 영역을 지시하는 경우, 상기 제1 샘플 영상의 상기 전경 영역을 상기 타겟 영역으로 결정하고,상기 제1 광선이 상기 제1 샘플 영상의 상기 배경 영역을 지시하는 경우, 상기 제1 샘플 영상의 상기 배경 영역이 상기 타겟 영역으로 결정하는,트레이닝 장치.</claim></claimInfo><claimInfo><claim>17. 제13항에 있어서,상기 프로세서는, 상기 2차 트레이닝을 수행하기 위해,상기 뉴럴 장면 표현 모델을 이용하여 상기 원본 트레이닝 영상들의 제1 원본 트레이닝 영상의 제1 광선 세트에 따른 제1 합성 영상을 생성하고,상기 제1 원본 트레이닝 영상의 멀티 레벨 패치들의 제1 의미론적 특징들 및 상기 제1 합성 영상의 멀티 레벨 패치들의 제2 의미론적 특징들을 추정하고,상기 제1 의미론적 특징들 및 상기 제2 의미론적 특징들 간의 차이에 따라 상기 제1 원본 트레이닝 영상과 상기 제1 합성 영상 간의 의미론적 일관성을 결정하고,상기 결정된 의미론적 일관성에 기초하여 상기 제2 손실 함수의 손실 값을 결정하는,트레이닝 장치.</claim></claimInfo><claimInfo><claim>18. 타겟 장면에 대한 기본 뷰들(base views)의 원본 트레이닝 영상들을 생성하는 카메라; 및상기 원본 트레이닝 영상들을 와핑하여 상기 타겟 장면에 대한 새로운 뷰들의 증강 영상들을 생성하고,상기 원본 트레이닝 영상들 및 상기 증강 영상들에 대한 의미론적 분할을 수행하여 상기 원본 트레이닝 영상들 및 상기 증강 영상들 각각의 전경 영역과 상기 원본 트레이닝 영상들 및 상기 증강 영상들 각각의 배경 영역을 구분하는 분할 마스크들을 결정하고,상기 타겟 장면에 대한 볼륨 렌더링에 이용되는 뉴럴 장면 표현 모델을 상기 원본 트레이닝 영상들, 상기 증강 영상들, 및 상기 분할 마스크들을 이용하여 트레이닝하는, 프로세서를 포함하는, 전자 장치.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서,상기 프로세서는, 상기 증강 영상들을 생성하기 위해,상기 원본 트레이닝 영상들의 제1 원본 트레이닝 영상의 제1 기본 뷰의 카메라 포즈를 상기 증강 영상들의 제1 증강 영상의 제1 새로운 뷰의 카메라 포즈로 변환하는 변환 함수를 결정하고,상기 제1 원본 트레이닝 영상의 카메라 내부 파라미터, 상기 제1 원본 트레이닝 영상에 대응하는 원본 깊이 맵, 및 상기 변환 함수를 이용하여 상기 제1 원본 트레이닝 영상을 와핑하여 상기 제1 증강 영상을 생성하는,전자 장치.</claim></claimInfo><claimInfo><claim>20. 제18항에 있어서,상기 프로세서는, 상기 뉴럴 장면 표현 모델을 트레이닝하기 위해,상기 원본 트레이닝 영상들, 상기 증강 영상들, 상기 분할 마스크들, 및 제1 손실 함수를 이용하여 상기 뉴럴 장면 표현 모델의 1차 트레이닝을 수행하고,상기 원본 트레이닝 영상들, 및 제2 손실 함수를 이용하여 상기 뉴럴 장면 표현 모델의 2차 트레이닝을 수행하고,상기 제1 손실 함수는상기 원본 트레이닝 영상들 및 상기 증강 영상들의 실제 픽셀 값과 상기 뉴럴 장면 표현 모델에 의해 추정된 픽셀 값 간의 픽셀 오차를 기반으로 정의되고,상기 제2 손실 함수는상기 원본 트레이닝 영상들과 상기 뉴럴 장면 표현 모델에 의해 추정된 합성 영상 간의 픽셀 오차, 및 상기 원본 트레이닝 영상들과 상기 합성 영상 간의 의미론적 일관성, 및 광선을 따른 투과율의 불확실성을 기반으로 정의되는,전자 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code>420200811571</code><country>대한민국</country><engName>Young Chun Ahn</engName><name>안영춘</name></inventorInfo><inventorInfo><address>경기도 용인시 수지구...</address><code>420180119290</code><country>대한민국</country><engName>KANG, Nahyup</engName><name>강나협</name></inventorInfo><inventorInfo><address>경기도 안양시 동안구...</address><code>420190614365</code><country>대한민국</country><engName>Jang, Seokhwan</engName><name>장석환</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code>420170747602</code><country>대한민국</country><engName>KIM, Jiyeon</engName><name>김지연</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.10.07</priorityApplicationDate><priorityApplicationNumber>1020220128898</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.12.19</receiptDate><receiptNumber>1-1-2022-1367064-49</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220178564.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9331a95b292e3220ea4201ab3d8ea4380e9d2371c44bb5ec20ea6d4d9c1fa0b2c65fb27051caa6063978e0a1517bb7a71e26d3f33dbb07352f</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfef3d89fddf14f06b8efaf9a6f7824f60b9b292c2598e016e674a8371bebe3a4489d3801bfd3c8074a3b43f783291514879c26c9af9fcb5c1</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>