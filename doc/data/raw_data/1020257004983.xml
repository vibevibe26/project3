<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:10:36.1036</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2018.10.26</applicationDate><applicationFlag>특허</applicationFlag><applicationNumber>10-2025-7004983</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>공동 오디오-비디오 얼굴 애니메이션 시스템</inventionTitle><inventionTitleEng>JOINT AUDIO-VIDEO FACIAL ANIMATION SYSTEM</inventionTitleEng><openDate>2025.02.28</openDate><openNumber>10-2025-0028510</openNumber><originalApplicationDate>2018.10.26</originalApplicationDate><originalApplicationKind>국제출원/분할</originalApplicationKind><originalApplicationNumber>10-2022-7029633</originalApplicationNumber><originalExaminationRequestDate>2025.02.14</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.02.14</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 13/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G10L 21/003</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G10L 21/055</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G10L 15/183</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 13/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/16</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo><familyApplicationNumber>1020227029633</familyApplicationNumber></familyInfo></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 발명은 일부 예시적인 실시예들에서 단어 격자로부터의 획득된 음소 얼라인먼트(phoneme alignment) 및 음성 인식을 위한 강력한 언어 모델을 갖는 풀 스케일 최신식 다 어휘 연속 음성 인식(Large Vocabulary Continuous Speech Recognition, LVCSR)을 포함하는 공동 자동 오디오 시각 주도 얼굴 애니메이션 시스템에 관한 것이다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2019.05.02</internationOpenDate><internationOpenNumber>WO2019084481</internationOpenNumber><internationalApplicationDate>2018.10.26</internationalApplicationDate><internationalApplicationNumber>PCT/US2018/057827</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 방법으로서,클라이언트 디바이스에서 오디오 데이터 및 비디오 데이터에 액세스하는 단계 - 상기 오디오 데이터는 음성 신호를 포함함 -;상기 비디오 데이터에 기초하여 얼굴 랜드마크들의 세트의 위치들을 결정하는 단계;상기 얼굴 랜드마크들의 세트의 상기 위치들에 기초하여 사용자 프로파일을 식별하는 단계 - 상기 사용자 프로파일은 사용자 아바타의 선택을 포함함 -;적어도 상기 오디오 데이터의 상기 음성 신호에 기초하여 가중 유한 상태 변환기(weighted finite state transducer, WFST)를 생성하는 단계;상기 WFST의 출력에 너비 우선 탐색(Breadth-First Search, BFS)을 수행하는 단계;상기 BFS에 기초하여 음 시퀀스(phone sequence)를 결정하는 단계;상기 얼굴 랜드마크들의 세트의 상기 위치들에 기초하여 제1 얼굴 모델을 생성하는 단계;상기 음 시퀀스에 기초하여 제2 얼굴 모델을 생성하는 단계;상기 제1 얼굴 모델, 상기 제2 얼굴 모델 및 상기 사용자 아바타의 선택에 기초하여 합성 얼굴 모델을 구성하는 단계; 및상기 클라이언트 디바이스에서 상기 합성 얼굴 모델의 디스플레이를 야기하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 클라이언트 디바이스에서 상기 오디오 데이터 및 상기 비디오 데이터를 수신하는 단계는:상기 클라이언트 디바이스에서 비디오를 수신하는 단계 - 상기 비디오는 상기 오디오 데이터 및 상기 비디오 데이터를 포함함 -; 및상기 비디오로부터 상기 오디오 데이터 및 상기 비디오 데이터를 추출하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서, 상기 클라이언트 디바이스는 제1 클라이언트 디바이스이고, 상기 합성 얼굴 모델의 디스플레이를 야기하는 단계는:상기 합성 얼굴 모델을 포함하는 메시지를 생성하는 단계; 및제2 클라이언트 디바이스에서 상기 메시지의 프레젠테이션의 디스플레이를 야기하는 단계를 포함하고, 상기 메시지의 프레젠테이션은 상기 합성 얼굴 모델을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 방법은:상기 사용자의 사용자 프로파일로부터 디스플레이 명세들을 검색하는 단계를 추가로 포함하고, 상기 합성 얼굴 모델을 구성하는 단계는 상기 제1 얼굴 모델, 상기 제2 얼굴 모델, 및 상기 디스플레이 명세들에 기초하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서, 상기 WFST를 생성하는 단계는:상기 오디오 데이터에 기초하여 트레이닝된 음향 모델, 상태들을 음(phone)들에 매핑하기 위한 결정 트리, 어휘(lexicon) 및 사전 트레이닝된 N-gram 언어 모델을 이용하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 제1 얼굴 모델 및 상기 제2 얼굴 모델에 기초하여 상기 합성 얼굴 모델을 구성하는 단계는 실시간으로 발생하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 위치들은 제1 세트의 위치들이고, 상기 비디오 데이터는 비디오 프레임들의 세트를 포함하고, 상기 방법은:실시간 데이터의 손실을 검출하는 단계;상기 실시간 데이터의 손실을 검출하는 것에 응답하여 상기 비디오 데이터를 파싱하여 상기 비디오 프레임들의 세트 중에서 제1 프레임을 식별하는 단계;상기 비디오 데이터의 상기 제1 프레임 내의 상기 얼굴 랜드마크들의 세트의 제2 세트의 위치들을 결정하는 단계; 및상기 얼굴 랜드마크들의 세트의 상기 제2 세트의 위치들에 기초하여 상기 합성 얼굴 모델을 변경하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 시스템으로서,메모리; 및상기 메모리에 결합되고 명령어들을 포함하는 적어도 하나의 하드웨어 프로세서를 포함하고, 상기 명령어들은 상기 시스템으로 하여금:클라이언트 디바이스에서 오디오 데이터 및 비디오 데이터에 액세스하는 단계 - 상기 오디오 데이터는 음성 신호를 포함함 -;상기 비디오 데이터에 기초하여 얼굴 랜드마크들의 세트의 위치들을 결정하는 단계;상기 얼굴 랜드마크들의 세트의 상기 위치들에 기초하여 사용자 프로파일을 식별하는 단계 - 상기 사용자 프로파일은 사용자 아바타의 선택을 포함함 -;적어도 상기 오디오 데이터의 상기 음성 신호에 기초하여 가중 유한 상태 변환기(weighted finite state transducer, WFST)를 생성하는 단계;상기 WFST의 출력에 너비 우선 탐색(Breadth-First Search, BFS)을 수행하는 단계;상기 BFS에 기초하여 음 시퀀스(phone sequence)를 결정하는 단계;상기 얼굴 랜드마크들의 세트의 상기 위치들에 기초하여 제1 얼굴 모델을 생성하는 단계;상기 음 시퀀스에 기초하여 제2 얼굴 모델을 생성하는 단계;상기 제1 얼굴 모델, 상기 제2 얼굴 모델 및 상기 사용자 아바타의 선택에 기초하여 합성 얼굴 모델을 구성하는 단계; 및상기 클라이언트 디바이스에서 상기 합성 얼굴 모델의 디스플레이를 야기하는 단계를 포함하는 동작들을 수행하게 하는, 시스템.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서, 상기 클라이언트 디바이스에서 상기 오디오 데이터 및 상기 비디오 데이터를 수신하는 단계는: 상기 클라이언트 디바이스에서 비디오를 수신하는 단계 - 상기 비디오는 상기 오디오 데이터 및 상기 비디오 데이터를 포함함 -; 및 상기 비디오로부터 상기 오디오 데이터 및 상기 비디오 데이터를 추출하는 단계를 추가로 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>10. 제8항에 있어서,상기 클라이언트 디바이스는 제1 클라이언트 디바이스이고, 상기 합성 얼굴 모델의 디스플레이를 야기하는 단계는:상기 합성 얼굴 모델을 포함하는 메시지를 생성하는 단계; 및제2 클라이언트 디바이스에서 상기 메시지의 프레젠테이션의 디스플레이를 야기하는 단계를 포함하고, 상기 메시지의 프레젠테이션은 상기 합성 얼굴 모델을포함하는, 시스템.</claim></claimInfo><claimInfo><claim>11. 제8항에 있어서, 상기 명령어들은 상기 시스템으로 하여금:상기 사용자의 사용자 프로파일로부터 디스플레이 명세들을 검색하는 단계를 더 포함하는 동작들을 수행하게 하고, 상기 합성 얼굴 모델을 구성하는 단계는 상기 제1 얼굴 모델, 상기 제2 얼굴 모델 및 상기 디스플레이 명세들에 기초하는, 시스템.</claim></claimInfo><claimInfo><claim>12. 제8항에 있어서, 상기 WFST를 생성하는 단계는:상기 오디오 데이터에 기초하여 트레이닝된 음향 모델, 상태들을 음(phone)들에 매핑하기 위한 결정 트리, 어휘(lexicon) 및 사전 트레이닝된 N-gram 언어 모델을 이용하는 단계를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>13. 제8항에 있어서,상기 제1 얼굴 모델 및 상기 제2 얼굴 모델에 기초하여 상기 합성 얼굴 모델을 구성하는 단계는 실시간으로 발생하는, 시스템.</claim></claimInfo><claimInfo><claim>14. 제8항에 있어서,상기 위치들은 제1 세트의 위치들이고, 상기 비디오 데이터는 비디오 프레임들의 세트를 포함하고, 상기 명령어들은 상기 시스템으로 하여금:실시간 데이터의 손실을 검출하는 단계;상기 실시간 데이터의 손실을 검출하는 것에 응답하여 상기 비디오 데이터를 파싱하여 상기 비디오 프레임들의 세트 중에서 제1 프레임을 식별하는 단계;상기 비디오 데이터의 상기 제1 프레임 내의 상기 얼굴 랜드마크들의 세트의 제2 세트의 위치들을 결정하는 단계; 및상기 얼굴 랜드마크들의 세트의 상기 제2 세트의 위치들에 기초하여 상기 합성 얼굴 모델을 변경하는 단계를 더 포함하는 동작들을 수행하게 하는, 시스템.</claim></claimInfo><claimInfo><claim>15. 명령어들을 포함하는 비일시적 머신 판독가능 저장 매체로서, 상기 명령어들은, 머신의 하나 이상의 프로세서에 의해 실행될 때, 상기 머신으로 하여금:클라이언트 디바이스에서 오디오 데이터 및 비디오 데이터에 액세스하는 단계 - 상기 오디오 데이터는 음성 신호를 포함함 -;상기 비디오 데이터에 기초하여 얼굴 랜드마크들의 세트의 위치들을 결정하는 단계;상기 얼굴 랜드마크들의 세트의 상기 위치들에 기초하여 사용자 프로파일을 식별하는 단계 - 상기 사용자 프로파일은 사용자 아바타의 선택을 포함함 -;적어도 상기 오디오 데이터의 상기 음성 신호에 기초하여 가중 유한 상태 변환기(weighted finite state transducer, WFST)를 생성하는 단계;상기 WFST의 출력에 너비 우선 탐색(Breadth-First Search, BFS)을 수행하는 단계;상기 BFS에 기초하여 음 시퀀스(phone sequence)를 결정하는 단계;상기 얼굴 랜드마크들의 세트의 상기 위치들에 기초하여 제1 얼굴 모델을 생성하는 단계;상기 음 시퀀스에 기초하여 제2 얼굴 모델을 생성하는 단계;상기 제1 얼굴 모델, 상기 제2 얼굴 모델 및 상기 사용자 아바타의 선택에 기초하여 합성 얼굴 모델을 구성하는 단계; 및상기 클라이언트 디바이스에서 상기 합성 얼굴 모델의 디스플레이를 야기하는 단계를 포함하는 동작들을 수행하게 하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서, 상기 클라이언트 디바이스에서 상기 오디오 데이터 및 상기 비디오 데이터를 수신하는 단계는:상기 클라이언트 디바이스에서 비디오를 수신하는 단계 - 상기 비디오는 상기 오디오 데이터 및 상기 비디오 데이터를 포함함 -; 및상기 비디오로부터 상기 오디오 데이터 및 상기 비디오 데이터를 추출하는 단계를 추가로 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>17. 제15항에 있어서,상기 클라이언트 디바이스는 제1 클라이언트 디바이스이고, 상기 합성 얼굴 모델의 디스플레이를 야기하는 단계는:상기 합성 얼굴 모델을 포함하는 메시지를 생성하는 단계; 및제2 클라이언트 디바이스에서 상기 메시지의 프레젠테이션의 디스플레이를 야기하는 단계를 포함하고, 상기 메시지의 프레젠테이션은 상기 합성 얼굴 모델을 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>18. 제15항에 있어서, 상기 명령어들은 상기 머신으로 하여금:상기 사용자의 사용자 프로파일로부터 디스플레이 명세들을 검색하는 단계를 더 포함하는 동작들을 수행하게 하고, 상기 합성 얼굴 모델을 구성하는 단계는 상기 제1 얼굴 모델, 상기 제2 얼굴 모델, 및 상기 디스플레이 명세들에 기초하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>19. 제15항에 있어서, 상기 WFST를 생성하는 단계는:상기 오디오 데이터에 기초하여 트레이닝된 음향 모델, 상태들을 음(phone)들에 매핑하기 위한 결정 트리, 어휘(lexicon) 및 사전 트레이닝된 N-gram 언어 모델을 이용하는 단계를 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>20. 제15항에 있어서,상기 위치들은 제1 세트의 위치들이고, 상기 비디오 데이터는 비디오 프레임들의 세트를 포함하고, 상기 명령어들은 상기 머신으로 하여금:실시간 데이터의 손실을 검출하는 단계;상기 실시간 데이터의 손실을 검출하는 것에 응답하여 상기 비디오 데이터를 파싱하여 상기 비디오 프레임들의 세트 중에서 제1 프레임을 식별하는 단계;상기 비디오 데이터의 상기 제1 프레임 내의 상기 얼굴 랜드마크들의 세트의 제2 세트의 위치들을 결정하는 단계; 및상기 얼굴 랜드마크들의 세트의 상기 제2 세트의 위치들에 기초하여 상기 합성 얼굴 모델을 변경하는 단계를 더 포함하는 동작들을 수행하게 하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 캘리포니아주 산타 모니카 써티퍼스트 스트리트 ****</address><code>520140253774</code><country>미국</country><engName>SNAP INC.</engName><name>스냅 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아주 산...</address><code> </code><country>중국</country><engName>CAO, Chen</engName><name>차오, 천</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아주 산...</address><code> </code><country>중국</country><engName>CHEN, Xin</engName><name>천, 신</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아주 산...</address><code> </code><country>중국</country><engName>CHU, Wei</engName><name>추, 웨이</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아주 산...</address><code> </code><country>중국</country><engName>XUE, Zehao</engName><name>쉐, 쩌하오</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920010003368</code><country>대한민국</country><engName>Kim Yeon Song</engName><name>김연송</name></agentInfo><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2017.10.26</priorityApplicationDate><priorityApplicationNumber>62/577,548</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2017.12.29</priorityApplicationDate><priorityApplicationNumber>15/858,992</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Divisional Application(International Application)] Patent Application</documentEngName><documentName>[분할출원(국제출원)]특허출원서</documentName><receiptDate>2025.02.14</receiptDate><receiptNumber>1-1-2025-0175580-74</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257004983.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9311487273ec768278367f2b3b9b97f09f8e3d4d14ead7fbf9e8589be801a9d609f0d7a77ef2c998a6b041520c3b737cd9bd6dab0776f18298</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf3ad4c6fb2b259ee804e8c0db3483dbb7ac93ac12c514a49f0e3257f56310d52b975d4b94e0faa45d0ad0eb478d305a60ed8073924613e419</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>