<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:54:25.5425</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2024.01.08</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-0003120</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>전자 장치와 그 동작 방법</inventionTitle><inventionTitleEng>ELECTRONIC DEVICE AND OPERATING METHOD THEREOF</inventionTitleEng><openDate>2025.07.15</openDate><openNumber>10-2025-0108410</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/18</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 전자 장치의 동작 방법은 기준 시점에 대응되는 타겟 부위의 위치 변화 정보를 획득하는 단계, 타겟 부위의 미래 속도를 예측하는 단계, 기준 시점에 대응되는 타겟 부위의 속도 변화 정보를 획득하는 단계, 타겟 부위의 미래 가속도를 예측하는 단계, 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계 및 타겟 시점에 대응되는 양안의 미래 위치를 기초로, 영상을 출력하는 단계를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 전자 장치의 동작 방법에 있어서,카메라를 통해 입력되는 시청자 얼굴 영역을 포함하는 영상으로부터, 과거 시점에 대응되는 타겟 부위의 위치 정보와 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 획득하는 단계;상기 과거 시점에 대응되는 상기 타겟 부위의 위치 정보와 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 획득하는 단계;상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보와 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 기초로, 상기 타겟 부위의 미래 속도를 예측하는 단계;상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보와 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 속도 변화 정보를 획득하는 단계;상기 기준 시점에 대응되는 상기 타겟 부위의 속도 변화 정보와 상기 과거 시점에 대응되는 상기 타겟 부위의 속도 변화 정보를 기초로, 상기 타겟 부위의 미래 가속도를 예측하는 단계;상기 미래 속도 및 상기 미래 가속도를 기초로, 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계; 및상기 타겟 시점에 대응되는 양안의 미래 위치를 기초로, 영상을 출력하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 미래 속도 및 상기 미래 가속도를 기초로, 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계는,상기 미래 속도 및 상기 미래 가속도를 기초로, 상기 타겟 시점에 대응되는 상기 타겟 부위의 제1 미래 위치를 예측하는 단계;상기 타겟 시점에 대응되는 상기 타겟 부위의 제1 미래 위치와 상기 타겟 시점의 이전 제1 시점에 대응되는 상기 타겟 부위의 미래 위치를 기초로, 상기 타겟 시점에 대응되는 상기 타겟 부위의 제2 미래 위치를 획득하는 단계; 및상기 타겟 부위의 제2 미래 위치를 기초로, 상기 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항 또는 제2 항에 있어서,상기 미래 속도 및 상기 미래 가속도를 기초로, 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계는,상기 미래 속도 및 상기 미래 가속도를 기초로, 상기 타겟 시점에 대응되는 양안 중심의 제1 미래 위치를 예측하는 단계; 및상기 타겟 시점에 대응되는 양안 중심의 제1 미래 위치 및 IPD(inter-pupillary distance) 정보를 기초로, 상기 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항 내지 제3항 중 어느 한 항에 있어서,상기 카메라를 통해 입력되는 시청자 얼굴 영역을 포함하는 영상으로부터, 과거 시점에 대응되는 타겟 부위의 위치 정보와 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 획득하는 단계는,상기 시청자 얼굴 영역을 포함하는 영상으로부터, 상기 기준 시점에 대응되는 상기 타겟 부위의 원시(raw) 위치 정보를 획득하는 단계; 및 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 정보와 상기 타겟 부위의 원시 위치 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 획득하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항 내지 제4항 중 어느 한 항에 있어서,상기 과거 시점에 대응되는 상기 타겟 부위의 위치 정보와 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 획득하는 단계는,상기 과거 시점에 대응되는 상기 타겟 부위의 위치 정보와 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화에 대한 원시 정보를 획득하는 단계; 및상기 타겟 부위의 움직임 정도를 기초로 결정된 필터링 강도, 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 변화 정보 및 상기 타겟 부위의 위치 변화에 대한 원시 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 획득하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5항 중 어느 한 항에 있어서,상기 기준 시점에 대응되는 상기 타겟 부위의 속도 변화 정보와 상기 과거 시점에 대응되는 상기 타겟 부위의 속도 변화 정보를 기초로, 상기 타겟 부위의 미래 가속도를 예측하는 단계는,상기 기준 시점에 대응되는 상기 타겟 부위의 속도 변화 정보, 상기 과거 시점에 대응되는 상기 타겟 부위의 속도 변화 정보 및 시청자 움직임 상태를 기초로 결정된 파라미터 값을 기초로, 상기 타겟 시점에 대응되는 상기 타겟 부위의 비선형 예측 가속도 정보를 획득하는 단계; 및상기 비선형 예측 가속도 정보 및 상기 타겟 시점의 이전 제1 시점에 대하여 예측된 상기 타겟 부위의 가속도를 기초로, 상기 타겟 부위의 미래 가속도를 예측하는 단계를 포함하고,상기 타겟 부위의 미래 가속도는, 상기 타겟 시점에 대응되는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항 내지 제6항 중 어느 한 항에 있어서,상기 미래 속도의 방향을 기초로, 상기 미래 속도에 대한 스케일링 값을 획득하는 단계를 포함하고,상기 미래 속도 및 상기 미래 가속도를 기초로, 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계는,상기 미래 속도, 상기 미래 가속도 및 상기 미래 속도에 대한 스케일링 값을 기초로, 상기 타겟 시점에 대응되는 양안의 미래 위치를 예측하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 제1항 내지 제7항 중 어느 한 항에 있어서,상기 타겟 시점의 이후 제2 시점에 대응되는 양안의 미래 위치를 획득하는 단계를 포함하고,상기 타겟 시점에 대응되는 양안의 미래 위치를 기초로, 영상을 출력하는 단계는,상기 타겟 시점에 대응되는 양안의 미래 위치 및 상기 제2 시점에 대응되는 양안의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안의 미래 위치를 예측하는 단계; 및상기 타겟 시점에 대한 양안의 미래 위치를 기초로, 상기 영상을 출력하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서,상기 타겟 시점과 상기 제2 시점 사이의 제3 시점에 대응되는 양안의 미래 위치를 획득하는 단계를 포함하고상기 타겟 시점에 대응되는 양안의 미래 위치 및 상기 제2 시점에 대응되는 양안의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안의 미래 위치를 예측하는 단계는,상기 타겟 시점에 대응되는 양안의 미래 위치, 상기 제2 시점에 대응되는 양안의 미래 위치 및 상기 제3 시점에 대응되는 양안의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안의 미래 위치를 예측하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항 내지 9항 중 어느 한 항에 있어서,양안 중심의 미래 가속도 및 상기 양안 중심의 미래 속도를 기초로, 상기 타겟 시점에 대응되는 상기 양안 중심의 미래 위치를 예측하는 단계를 포함하고,상기 타겟 부위는 좌안 또는 우안 중 적어도 하나를 포함하고,상기 타겟 시점에 대응되는 양안의 미래 위치를 기초로, 영상을 출력하는 단계는,상기 타겟 시점에 대응되는 양안의 미래 위치 및 상기 타겟 시점에 대응되는 상기 양안 중심의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안의 미래 위치를 예측하는 단계; 및상기 타겟 시점에 대한 양안의 미래 위치를 기초로, 상기 영상을 출력하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 타겟 시점에 대응되는 양안의 미래 위치 및 상기 타겟 시점에 대응되는 상기 양안 중심의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안의 미래 위치를 예측하는 단계는,상기 타겟 시점에 대응되는 양안의 미래 위치 및 상기 타겟 시점에 대응되는 상기 양안 중심의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안 중심의 미래 위치를 예측하는 단계;상기 타겟 시점에 대한 양안 중심의 미래 위치에서 노이즈를 필터링하는 단계; 및상기 노이즈가 필터링된 양안 중심의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안의 미래 위치를 예측하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 제1항 내지 제11항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체.</claim></claimInfo><claimInfo><claim>13. 전자 장치에 있어서,하나 이상의 명령어(instruction)를 저장하는 메모리(1520); 및상기 메모리(1520)에 저장된 상기 하나 이상의 명령어를 실행하는 적어도 하나의 프로세서(1510)를 포함하고,상기 적어도 하나의 프로세서(1510)는 상기 하나 이상의 명령어를 실행함으로써,카메라를 통해 입력되는 시청자 얼굴 영역을 포함하는 영상으로부터, 과거 시점에 대응되는 타겟 부위의 위치 정보와 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 획득하고,상기 과거 시점에 대응되는 상기 타겟 부위의 위치 정보와 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 획득하고,상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보와 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 기초로, 상기 타겟 부위의 미래 속도를 예측하고,상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보와 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 속도 변화 정보를 획득하고,상기 기준 시점에 대응되는 상기 타겟 부위의 속도 변화 정보와 상기 과거 시점에 대응되는 상기 타겟 부위의 속도 변화 정보를 기초로, 상기 타겟 부위의 미래 가속도를 예측하고,상기 미래 속도 및 상기 미래 가속도를 기초로, 타겟 시점에 대응되는 양안의 미래 위치를 예측하고,상기 타겟 시점에 대응되는 양안의 미래 위치를 기초로, 영상을 출력하는, 전자 장치.|</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 적어도 하나의 프로세서(1510)는,상기 미래 속도 및 상기 미래 가속도를 기초로, 상기 타겟 시점에 대응되는 상기 타겟 부위의 제1 미래 위치를 예측하고,상기 타겟 시점에 대응되는 상기 타겟 부위의 제1 미래 위치와 상기 타겟 시점의 이전 제1 시점에 대한 상기 타겟 부위의 미래 위치를 기초로, 상기 타겟 시점에 대응되는 상기 타겟 부위의 제2 미래 위치를 획득하고,상기 타겟 부위의 제2 미래 위치를 기초로, 상기 타겟 시점에 대응되는 양안의 미래 위치를 예측하는, 전자 장치.</claim></claimInfo><claimInfo><claim>15. 제13항 또는 제14항에 있어서,상기 적어도 하나의 프로세서(1510)는,상기 미래 속도 및 상기 미래 가속도를 기초로, 상기 타겟 시점에 대응되는 양안 중심의 제1 미래 위치를 예측하고,상기 타겟 시점에 대응되는 양안 중심의 제1 미래 위치 및 IPD(inter-pupillary distance) 정보를 기초로, 상기 타겟 시점에 대응되는 양안의 미래 위치를 예측하는, 전자 장치.</claim></claimInfo><claimInfo><claim>16. 제13항 내지 제15항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(1510)는,상기 시청자 얼굴 영역을 포함하는 영상으로부터, 상기 기준 시점에 대응되는 상기 타겟 부위의 원시(raw) 위치 정보를 획득하고, 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 정보와 상기 타겟 부위의 원시 위치 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>17. 제13항 내지 제16항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(1510)는,상기 과거 시점에 대응되는 상기 타겟 부위의 위치 정보와 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화에 대한 원시 정보를 획득하고,상기 타겟 부위의 움직임 정도를 기초로 결정된 필터링 강도, 상기 과거 시점에 대응되는 상기 타겟 부위의 위치 변화 정보 및 상기 타겟 부위의 위치 변화에 대한 원시 정보를 기초로, 상기 기준 시점에 대응되는 상기 타겟 부위의 위치 변화 정보를 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>18. 제13항 내지 제17항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(1510)는,상기 기준 시점에 대응되는 상기 타겟 부위의 속도 변화 정보, 상기 과거 시점에 대응되는 상기 타겟 부위의 속도 변화 정보 및 시청자 움직임 상태를 기초로 결정된 파라미터 값을 기초로, 상기 타겟 시점에 대응되는 상기 타겟 부위의 비선형 예측 가속도 정보를 획득하고,상기 비선형 예측 가속도 정보 및 상기 타겟 시점의 이전 제1 시점에 대하여 예측된 상기 타겟 부위의 가속도를 기초로, 상기 타겟 부위의 미래 가속도를 예측하고,상기 타겟 부위의 미래 가속도는, 상기 타겟 시점에 대응되는, 전자 장치.</claim></claimInfo><claimInfo><claim>19. 제13항 내지 제18항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(1510)는,상기 미래 속도의 방향을 기초로, 상기 미래 속도에 대한 스케일링 값을 획득하고,상기 미래 속도, 상기 미래 가속도 및 상기 미래 속도에 대한 스케일링 값을 기초로, 상기 타겟 시점에 대응되는 양안의 미래 위치를 예측하는, 전자 장치.</claim></claimInfo><claimInfo><claim>20. 제13항 내지 제19항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(1510)는,상기 타겟 시점의 이후 제2 시점에 대응되는 양안의 미래 위치를 획득하고,상기 타겟 시점에 대응되는 양안의 미래 위치 및 상기 제2 시점에 대응되는 양안의 미래 위치를 기초로, 상기 타겟 시점에 대한 양안의 미래 위치를 예측하고,상기 타겟 시점에 대한 양안의 미래 위치를 기초로, 상기 영상을 출력하는, 전자 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>WON, Kwang Hyun</engName><name>원광현</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>PARK, Cheol Seong</engName><name>박철성</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>OH, Young Ho</engName><name>오영호</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>CHOI, Woong Il</engName><name>최웅일</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2024.01.08</receiptDate><receiptNumber>1-1-2024-0027234-75</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020240003120.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9366f162581ea5e2a35dea60ca3df1326ea47040a9b231ee7fdc36fc641edaf7b6b1ae3e572f474729f746a647205e703c7844b0794e56f133</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf10656c342785203de5b98f896ab8c27ade33e82cdb54559cc285cbdffc8a039b0f2a3212999a6d8413ceb7317fb0413e242c5749f1eb1a51</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>