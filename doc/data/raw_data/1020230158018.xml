<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:06:11.611</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.11.15</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0158018</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>이미지 처리 방법 및 장치</inventionTitle><inventionTitleEng>METHOD AND APPARATUS FOR IMAGE PROCESSING</inventionTitleEng><openDate>2024.07.25</openDate><openNumber>10-2024-0115158</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/74</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 개시는 이미지 처리 방법 및 장치에 관한 것으로, 획득한 이미지에 대해 특징을 추출하여 상기 이미지의 이미지 특징을 획득하고, 상기 이미지 특징을 기반으로 상기 이미지의 깊이 인식 특징을 생성하고, 상기 깊이 인식 특징 및 깊이 인식 표현에 기초하여 상기 이미지의 분할 결과를 획득할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 이미지 처리 방법에 있어서,획득한 이미지에 대해 특징을 추출하여 상기 이미지의 이미지 특징을 획득하는 단계;상기 이미지 특징을 기반으로 상기 이미지의 깊이 인식 특징을 생성하는 단계; 및상기 깊이 인식 특징 및 깊이 인식 표현에 기초하여 상기 이미지의 분할 결과를 획득하는 단계를 포함하고,상기 깊이 인식 표현은,상기 이미지의 분할 결과를 예측하기 위한 깊이 정보 및 비주얼 정보를 포함하는이미지 처리 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 이미지 특징을 기반으로 상기 이미지의 상기 깊이 인식 특징을 생성하는 단계는,상기 이미지 특징을 기반으로 상기 이미지의 깊이 특징과 비주얼 특징을 각각 획득하는 단계; 및상기 깊이 특징과 상기 비주얼 특징에 대해 특징을 융합하여 상기 깊이 인식 특징을 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 깊이 특징과 상기 비주얼 특징에 대해 특징을 융합하여 상기 깊이 인식 특징을 획득하는 단계는,상기 비주얼 특징 및 상기 깊이 특징에 대해 각각 컨볼루션 연산을 수행하여 제1 비주얼 특징 및 제1 깊이 특징을 획득하는 단계;상기 제1 비주얼 특징 및 상기 제1 깊이 특징을 기반으로 특징을 융합하여 제1 특징을 획득하는 단계;상기 제1 특징을 기반으로 상기 제1 깊이 특징과 특징을 융합하여 제2 특징을 획득하는 단계; 및상기 제2 특징과 상기 비주얼 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하는 것을 기반으로 상기 깊이 인식 특징을 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 제2 특징과 상기 비주얼 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하는 것을 기반으로 상기 깊이 인식 특징을 획득하는 단계는,상기 제2 특징과 상기 비주얼 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하여 제3 특징을 획득하는 단계;상기 깊이 특징의 차원을 재형성(reshape)하여 제4 특징을 획득하는 단계;상기 제4 특징과 깊이와 관련된 위치 임베딩(position embedding)에 대해 특징을 융합하여 제1 깊이 위치 특징을 획득하는 단계; 및상기 제3 특징과 상기 제1 깊이 위치 특징에 대해 특징을 융합하여 상기 깊이 인식 특징을 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 깊이 인식 특징 및 상기 깊이 인식 표현에 기초하여 상기 이미지의 상기 분할 결과를 획득하는 단계는,상기 깊이 인식 표현에 대해 리파이닝(refining) 처리하여 리파이닝된 깊이 인식 표현을 획득하는 단계;상기 리파이닝된 깊이 인식 표현 및 상기 깊이 인식 특징에 기초하여, 상기 분할 결과의 깊이 예측 결과를 획득하는 단계;상기 깊이 인식 특징에 대해 특징을 향상하여 향상된 깊이 인식 특징을 획득하는 단계; 및상기 리파이닝된 깊이 인식 표현과 상기 향상된 깊이 인식 특징을 기반으로, 상기 분할 결과 중 마스크 예측 결과와 카테고리 예측 결과를 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 깊이 인식 표현에 대해 리파이닝 처리하여 상기 리파이닝된 깊이 인식 표현을 획득하는 단계는,제1 어텐션 네트워크를 통해 상기 깊이 인식 표현을 처리하여 제1 깊이 인식 표현을 획득하는 단계;상기 깊이 인식 표현과 상기 제1 깊이 인식 표현에 대해 특징을 융합하고, 특징 융합된 표현에 대해 정규화 처리하여 제2 깊이 인식 표현을 획득하는 단계;제2 어텐션 네트워크를 통해 상기 깊이 인식 특징과 상기 제2 깊이 인식 표현을 처리하여 제3 깊이 인식 표현을 획득하는 단계;상기 제2 깊이 인식 표현과 상기 제3 깊이 인식 표현에 대해 특징을 융합하고, 특징 융합된 표현에 대해 정규화 처리하여 제4 깊이 인식 표현을 획득하는 단계; 및상기 제4 깊이 인식 표현을 기반으로 피드 포워드 네트워크를 사용하여 상기 리파이닝된 깊이 인식 표현을 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>7. 제5항에 있어서, 상기 리파이닝된 깊이 인식 표현과 상기 향상된 깊이 인식 특징을 기반으로, 상기 분할 결과 중 깊이 예측 결과를 획득하는 단계는,상기 리파이닝된 깊이 인식 표현에 대해 선형 연산을 수행하여 제5 특징을 얻고, 상기 깊이 인식 특징에 대해 컨볼루션 연산을 수행하여 제6 특징을 획득하는 단계;상기 제5 특징과 상기 제6 특징에 대해 특징을 융합하여 제7 특징을 획득하는 단계;상기 제7 특징과 상기 제5 특징에 대해 특징을 융합하여 제8 특징을 획득하는 단계;상기 제8 특징과 상기 제6 특징에 대해 특징을 융합하여 제9 특징을 획득하는 단계; 및상기 제9 특징에 기반하여, 깊이 추정 네트워크를 사용하여 상기 깊이 예측 결과를 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서, 상기 제9 특징에 기반하여, 상기 깊이 추정 네트워크를 사용하여 상기 깊이 예측 결과를 획득하는 단계는,상기 제9 특징에 대해 풀링 처리하고, 풀링 처리된 제9 특징을 선형 처리하여 상기 제9 특징에 대응하는 특징 가중치를 획득하는 단계; 및상기 제9 특징에 대해 상기 특징 가중치와 선형 연산을 수행하여 상기 깊이 예측 결과를 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>9. 제5항에 있어서, 상기 리파이닝된 깊이 인식 표현 및 상기 깊이 인식 특징에 기초하여, 상기 분할 결과의 깊이 예측 결과를 획득하는 단계는,상기 리파이닝된 깊이 인식 표현 및 상기 깊이 인식 특징을 기반으로, 상기 분할 결과 중 상기 깊이 예측 결과 및 향상된 깊이 정보를 획득하는 단계를 포함하고,상기 깊이 인식 특징에 대해 특징을 향상하여 향상된 깊이 인식 특징을 획득하는 단계는,상기 깊이 인식 특징에 대해 컨볼루션 연산을 수행하여 제10 특징을 얻고, 상기 향상된 깊이 정보에 대해 컨볼루션 연산을 수행하여 제11 특징을 획득하는 단계;상기 제10 특징과 상기 제11 특징에 대해 특징을 융합하여 제12 특징을 획득하는 단계;상기 제11 특징과 상기 제12 특징에 대해 특징을 융합하여 제13 특징을 획득하는 단계; 및상기 제13 특징과 상기 깊이 인식 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하는 것을 기반으로 상기 향상된 깊이 인식 특징을 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>10. 제9항에 있어서,상기 제13 특징과 상기 깊이 인식 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하는 것을 기반으로 상기 향상된 깊이 인식 특징을 획득하는 단계는,상기 제13 특징과 상기 깊이 인식 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하여 제14 특징을 획득하는 단계;상기 향상된 깊이 정보의 차원을 재형성하여 제15 특징을 획득하는 단계;상기 제15 특징과 깊이와 관련된 위치 임베딩에 대해 특징을 융합하여 제2 깊이 위치 특징을 획득하는 단계; 및상기 제14 특징과 상기 제2 깊이 위치 특징에 대해 특징을 융합하여 상기 향상된 깊이 인식 특징을 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>11. 제5항에 있어서,상기 리파이닝된 깊이 인식 표현과 상기 향상된 깊이 인식 특징을 기반으로, 상기 분할 결과 중 마스크 예측 결과와 카테고리 예측 결과를 획득하는 단계는,상기 리파이닝된 깊이 인식 표현을 기반으로, 제1 선형 레이어를 사용하여 상기 카테고리 예측 결과를 획득하는 단계; 및상기 리파이닝된 깊이 인식 표현을 기반으로, 제2 선형 레이어를 사용하여, 마스크와 관련된 제16 특징을 획득하고, 상기 제16 특징과 상기 향상된 깊이 인식 특징에 대해 특징을 융합하여 상기 마스크 예측 결과를 획득하는 단계를 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>12. 제5항에 있어서,상기 이미지는,처리될 비디오의 현재 프레임 이미지이고,상기 이미지 처리 방법은,상기 현재 프레임 이미지의 이전 프레임 이미지의 리파이닝된 깊이 인식 표현을 획득하는 단계; 및상기 현재 프레임 이미지의 리파이닝된 깊이 인식 표현과 상기 이전 프레임 이미지의 리파이닝된 깊이 인식 표현의 유사성 매칭을 수행하여, 상기 현재 프레임 이미지와 상기 이전 프레임 이미지의 동일한 인스턴스가 통일된 표식을 갖도록 하는 단계를 더 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>13. 제5항에 있어서,상기 이미지는,처리될 비디오의 현재 프레임 이미지이고,상기 이미지 처리 방법은,상기 현재 프레임 이미지의 이전 프레임 이미지의 리파이닝된 깊이 인식 표현을 획득하는 단계; 및제3 어텐션 네트워크를 통해, 상기 현재 프레임 이미지의 리파이닝된 깊이 인식 표현 및 상기 이전 프레임 이미지의 리파이닝된 깊이 인식 표현을 처리하고, 시간 도메인 컨텍스트(Time domain context)의 리파이닝된 깊이 인식 표현을 획득하고, 상기 현재 프레임 이미지의 리파이닝된 깊이 인식 표현으로 결정하는 단계를 더 포함하는 이미지 처리 방법.</claim></claimInfo><claimInfo><claim>14. 제1항에 있어서,상기 깊이 인식 표현의 하나의 벡터는,상기 이미지의 하나의 객체를 나타내는 이미지 처리 방법. </claim></claimInfo><claimInfo><claim>15. 제1항 내지 제14항 중 어느 한 항의 방법을 실행하기 위한 프로그램이 기록되어 있는 것을 특징으로 하는 컴퓨터에서 판독 가능한 기록 매체.</claim></claimInfo><claimInfo><claim>16. 전자 장치에 있어서, 적어도 하나의 프로세서; 및컴퓨터 실행 가능 명령을 저장한 적어도 하나의 메모리를 포함하고,상기 적어도 하나의 프로세서는,상기 컴퓨터 실행 가능 명령을 실행하여 획득한 이미지에 대해 특징을 추출하여 상기 이미지의 이미지 특징을 획득하고, 상기 이미지 특징을 기반으로 상기 이미지의 깊이 인식 특징을 생성하고, 상기 깊이 인식 특징 및 깊이 인식 표현에 기초하여 상기 이미지의 분할 결과를 획득하고,상기 깊이 인식 표현은,상기 이미지의 분할 결과를 예측하기 위한 깊이 정보 및 비주얼 정보를 포함하는전자 장치.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서, 상기 적어도 하나의 프로세서는,상기 이미지 특징을 기반으로 상기 이미지의 깊이 인식 특징을 생성할 때, 상기 이미지 특징을 기반으로 상기 이미지의 깊이 특징과 비주얼 특징을 각각 획득하고,상기 깊이 특징과 상기 비주얼 특징에 대해 특징을 융합하여 상기 깊이 인식 특징을 획득하는전자 장치.</claim></claimInfo><claimInfo><claim>18. 제17항에 있어서, 상기 적어도 하나의 프로세서는,상기 깊이 특징과 상기 비주얼 특징에 대해 특징을 융합하여 상기 깊이 인식 특징을 획득할 때,상기 비주얼 특징 및 상기 깊이 특징에 대해 각각 컨볼루션 연산을 수행하여 제1 비주얼 특징 및 제1 깊이 특징을 획득하고,상기 제1 비주얼 특징 및 상기 제1 깊이 특징을 기반으로 특징을 융합하여 제1 특징을 획득하고,상기 제1 특징을 기반으로 상기 제1 깊이 특징과 특징을 융합하여 제2 특징을 획득하고,상기 제2 특징과 상기 비주얼 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하는 것을 기반으로 상기 깊이 인식 특징을 획득하는전자 장치.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서,상기 적어도 하나의 프로세서는,상기 제2 특징과 상기 비주얼 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하는 것을 기반으로 상기 깊이 인식 특징을 획득할 때,상기 제2 특징과 상기 비주얼 특징에 대해 순차적으로 특징 연결 및 특징 변환을 수행하여 제3 특징을 획득하고,상기 깊이 특징의 차원을 재형성(reshape)하여 제4 특징을 획득하고,상기 제4 특징과 깊이와 관련된 위치 임베딩(position embedding)에 대해 특징을 융합하여 제1 깊이 위치 특징을 획득하고,상기 제3 특징과 상기 제1 깊이 위치 특징에 대해 특징을 융합하여 상기 깊이 인식 특징을 획득하는전자 장치</claim></claimInfo><claimInfo><claim>20. 제16항에 있어서,상기 적어도 하나의 프로세서는,상기 깊이 인식 특징 및 깊이 인식 표현에 기초하여 상기 이미지의 상기 분할 결과를 획득할 때, 상기 깊이 인식 표현에 대해 리파이닝(refining) 처리하여 리파이닝된 깊이 인식 표현을 획득하고,상기 리파이닝된 깊이 인식 표현 및 상기 깊이 인식 특징에 기초하여, 상기 분할 결과의 깊이 예측 결과를 획득하고,상기 깊이 인식 특징에 대해 특징을 향상하여 향상된 깊이 인식 특징을 획득하고,상기 리파이닝된 깊이 인식 표현과 상기 향상된 깊이 인식 특징을 기반으로, 상기 분할 결과 중 마스크 예측 결과와 카테고리 예측 결과를 획득하는전자 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>중화인민공화국 베이징 차오양 구...</address><code> </code><country> </country><engName>Yi ZHOU</engName><name>이 저우</name></inventorInfo><inventorInfo><address>경기도 용인시 수지구...</address><code>420170747129</code><country>대한민국</country><engName>Park, Seung-In</engName><name>박승인</name></inventorInfo><inventorInfo><address>서울특별시 강남구...</address><code>420170743681</code><country>대한민국</country><engName>YOO BYUNG IN</engName><name>유병인</name></inventorInfo><inventorInfo><address>경기도 용인시 수지구...</address><code>420170731829</code><country>대한민국</country><engName>JUNG, Sang Il</engName><name>정상일</name></inventorInfo><inventorInfo><address>중화인민공화국 베이징 차오양 구...</address><code> </code><country> </country><engName>Hui ZHANG</engName><name>후이 장</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>중국</priorityApplicationCountry><priorityApplicationDate>2023.01.18</priorityApplicationDate><priorityApplicationNumber>202310084221.0</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.11.15</receiptDate><receiptNumber>1-1-2023-1263187-85</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2023.11.22</receiptDate><receiptNumber>9-1-2023-9012626-81</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230158018.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93e19e4e999db58526fdb9973e821656b394feb5b9eef4b63e307ad7c5313b5b283242123075191db0e0c8584efb95bb3f5a566917ce23159a</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf0f7a0e1625b9ea43a37664efb2460dba5abb254287921ca919e2b84b92ecac292a0d2a7afaea520fa9dcf33287737e398678271a608a698d</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>