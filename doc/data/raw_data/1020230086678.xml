<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:42.4142</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.07.04</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0086678</applicationNumber><claimCount>46</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>정보 검색을 위한 희소 표현을 생성하는 뉴럴 랭킹 모델</inventionTitle><inventionTitleEng>NEURAL RANKING MODEL FOR GENERATING SPARSE REPRESENTATIONS  FOR INFORMATION RETRIEVAL</inventionTitleEng><openDate>2024.01.16</openDate><openNumber>10-2024-0007078</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.07.04</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 16/2457</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2025.01.01)</ipcDate><ipcNumber>G06F 16/332</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2025.01.01)</ipcDate><ipcNumber>G06F 16/33</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/284</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0455</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/092</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/096</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 뉴럴 정보 검색 모델을 위한 랭커는, 사전 훈련된 언어 모델 레이어를 갖고, 하나 이상의 문서들을 수신하고 어휘를 통해 문서의 용어 중요도를 예측하는 문서들의 각각에 대한 희소 표현을 생성하도록 구성되는 문서 인코더를 포함한다. 별도의 쿼리 인코더는 쿼리를 수신하고 어휘를 통해 쿼리의 표현을 생성하도록 구성된다. 생성된 표현들은 각각의 문서 스코어들의 세트를 생성하고 하나 이상의 문서들을 순위화하기 위해 비교된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 뉴럴 정보 검색 모델을 위한 컴퓨터 구현 랭커에 있어서, 상기 랭커는,사전 훈련된 언어 모델 레이어를 포함하고, 하나 이상의 문서들을 수신하고 어휘(vocabulary)를 통해 문서의 용어 중요도(term importance)를 예측하는 상기 하나 이상의 문서들의 각각에 대한 희소 표현(sparse representation)을 생성하도록 구성되는 문서 인코더; 쿼리(query)를 수신하고 상기 어휘를 통해 상기 쿼리의 표현을 생성하도록 구성되는 쿼리 인코더; 및상기 쿼리의 상기 생성된 표현을 상기 하나 이상의 문서들의 상기 생성된 표현들과 비교하여 각각의 문서 스코어들의 세트를 생성하고, 상기 생성된 문서 스코어들의 세트에 기반하여 상기 하나 이상의 문서들을 순위화하도록 구성되는 비교기 블록을 포함하고, 상기 문서 인코더 및 상기 쿼리 인코더는 각각 별도의 인코더들인,랭커.</claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 문서 인코더 및 상기 쿼리 인코더는 모델 아키텍처, 모델 크기, 모델 가중치들, 모델 훈련, 모델 정규화, 모델 하이퍼파라미터들, 또는 상기 랭커 내의 모델 위치 중 하나 이상에 의해 서로로부터 구별되는,랭커.</claim></claimInfo><claimInfo><claim>3. 제1 항에 있어서, 상기 문서 인코더 및 상기 쿼리 인코더는 상이한 정규화기들을 사용하여 훈련되고, 상기 쿼리 인코더는 L1 정규화를 사용하여 정규화되고, 상기 문서 인코더는 FLOPS 정규화를 사용하여 정규화되는,랭커.</claim></claimInfo><claimInfo><claim>4. 제1 항에 있어서, 상기 쿼리 인코더의 아키텍처는 상기 문서 인코더의 아키텍처보다 더 작고,상기 문서 인코더는 상기 어휘 내에서의 문서 확장을 위해 구성되고, 상기 쿼리 인코더는 상기 어휘 내에서의 쿼리 확장을 위해 구성되지 않는,랭커.</claim></claimInfo><claimInfo><claim>5. 제1 항에 있어서,상기 쿼리 인코더는 상기 문서 인코더보다 더 효율적이고, 효율성은 (i) 상기 쿼리 인코더의 일부를 형성하는 레이어들의 수를 감소시키는 것, (ii) 상기 쿼리 인코더를 토큰화기(tokenizer)로 줄이는 것, 및 (iii) 쿼리 표현을 정규화하는 것 중 하나에 의해 얻어지는,랭커.</claim></claimInfo><claimInfo><claim>6. 제1 항에 있어서, 상기 쿼리 인코더는 상기 문서 인코더의 상기 사전 훈련된 언어 모델보다 더 효율적인 사전 훈련된 언어 모델을 포함하고,상기 효율성은 사전 훈련 또는 중간 훈련 중에 FLOPS 정규화를 사용하여 얻어지는,랭커.</claim></claimInfo><claimInfo><claim>7. 제1 항에 있어서, 상기 문서 인코더는 토큰화된 입력 시퀀스로서 문서를 수신하고, 상기 토큰화된 입력 시퀀스는 상기 어휘를 사용하여 토큰화되고, 상기 사전 훈련된 언어 모델 레이어는 컨텍스트 특징들(contextual features)로 상기 토큰화된 입력 시퀀스에 각 토큰을 임베드하고, 하나 이상의 선형 레이어들을 사용하여 컨텍스트 임베드된 토큰들을 변환함으로써 상기 어휘를 통해 상기 임베드된 입력 시퀀스의 각 토큰에 관하여 중요도를 예측하도록 구성되고, 상기 문서 인코더는 상기 어휘를 통해 각 토큰에 관하여 상기 예측된 중요도를 수신하고 상기 어휘를 통해 상기 입력 시퀀스의 상기 예측된 용어 중요도를 획득하도록 구성되는 표현 레이어를 더 포함하고, 상기 표현 레이어는 상기 임베드된 입력 시퀀스에 대한 상기 예측된 중요도의 오목 활성화(concave activation)를 수행하도록 구성되는 오목 활성화 레이어를 포함하고, 상기 표현 레이어는 상기 어휘를 통해 상기 입력 시퀀스의 표현으로서 상기 입력 시퀀스의 상기 예측된 용어 중요도를 출력하는,랭커.</claim></claimInfo><claimInfo><claim>8. 제1 항에 있어서,상기 문서 인코더의 상기 사전 훈련된 언어 모델은 상기 언어 모델이 정보 검색을 위해 미세 조정되기 전에 중간 훈련에 의해 훈련되고, 상기 중간 훈련은 예측을 위해 상기 사전 훈련된 언어 모델을 사전 훈련한 후에 발생하거나, 상기 중간 훈련은 강화된 사전 훈련을 제공하기 위해 예측을 위해 상기 사전 훈련된 언어 모델을 사전 훈련하는 것과 동시에 발생하고, 상기 중간 훈련 또는 강화된 사전 훈련은 FLOPS 정규화와 결합되는 마스크드 언어 모델(masked language model; MLM) 훈련을 사용하여 상기 언어 모델을 훈련하는 것을 포함하는,랭커.</claim></claimInfo><claimInfo><claim>9. 제1 항에 있어서, 상기 랭커는 하나 이상의 하이퍼파라미터들을 포함하는 최적화를 사용하여 훈련되고, 상기 하이퍼파라미터들은 미리 결정된 쿼리 및 문서 크기들에 기반하여 선택되고, 상기 랭커는 증류(distillation)를 사용하여 훈련되는,랭커.</claim></claimInfo><claimInfo><claim>10. 제1 항에 있어서, 상기 랭커는,상기 문서 스코어들의 세트를 생성하는 데 사용되는 방법보다 더 짧은 대기 시간(lower-latency)을 갖는 추가 검색 방법을 사용하여 상기 쿼리를 처리함으로써 상기 하나 이상의 문서들에 대한 각각의 문서 스코어들의 추가 세트를 생성하고,상기 문서 스코어들의 세트 및 상기 각각의 문서 스코어들의 추가 세트를 병합하고, 상기 병합된 문서 스코어들의 세트들에 기반하여 상기 하나 이상의 문서들을 순위화하도록 더 구성되는,랭커. </claim></claimInfo><claimInfo><claim>11. 제1 항에 있어서,상기 어휘를 통해 문서의 용어 중요도를 예측하는 상기 하나 이상의 문서들의 각각에 대한 상기 희소 표현은 그 요소들의 절반 이상이 0 값을 갖는 고차원 벡터인,랭커.</claim></claimInfo><claimInfo><claim>12. 정보 검색을 위한 컴퓨터 구현 방법에 있어서, 상기 방법은,사전 훈련된 언어 모델 레이어를 포함하는 문서 인코더에 의해, 어휘를 통해 문서의 용어 중요도를 예측하는 수신된 하나 이상의 문서들의 각각에 대한 희소 표현을 생성하는 단계; 쿼리 인코더에 의해, 상기 어휘를 통해 수신된 쿼리의 표현을 생성하는 단계;상기 쿼리의 상기 생성된 표현을 상기 하나 이상의 문서들의 상기 생성된 표현들과 비교하여, 각각의 문서 스코어들의 세트를 생성하는 단계; 및상기 생성된 문서 스코어들의 세트에 기반하여 상기 하나 이상의 문서들을 순위화하는 단계를 포함하고, 상기 문서 인코더 및 상기 쿼리 인코더는 각각 별도의 인코더들인,방법.</claim></claimInfo><claimInfo><claim>13. 제12 항에 있어서,상기 문서 인코더 및 상기 쿼리 인코더는 모델 아키텍처, 모델 크기, 모델 가중치들, 모델 훈련, 모델 정규화, 모델 하이퍼파라미터들, 또는 상기 랭커 내의 모델 위치 중 하나 이상에 의해 서로로부터 구별되는,방법.</claim></claimInfo><claimInfo><claim>14. 제12 항에 있어서, 상기 문서 인코더 및 상기 쿼리 인코더는 상이한 정규화기들을 사용하여 훈련되고,상기 쿼리 인코더는 L1 정규화를 사용하여 정규화되고, 상기 문서 인코더는 FLOPS 정규화를 사용하여 정규화되는,방법.</claim></claimInfo><claimInfo><claim>15. 제12 항에 있어서,상기 쿼리 인코더의 아키텍처는 상기 문서 인코더의 아키텍처보다 더 작은,방법.</claim></claimInfo><claimInfo><claim>16. 제12 항에 있어서,상기 문서 인코더는 상기 어휘 내에서 상기 수신된 하나 이상의 문서들을 확장시키고, 상기 쿼리 인코더는 상기 어휘 내에서 상기 수신된 쿼리를 확장시키지 않는, 방법.</claim></claimInfo><claimInfo><claim>17. 제12 항에 있어서,상기 희소 표현을 생성하는 단계는 정규화와 결합되는 오목 활성화 함수들을 사용하여 상기 희소 표현을 생성하는,방법.</claim></claimInfo><claimInfo><claim>18. 제12 항에 있어서,상기 문서 인코더는 토큰화된 입력 시퀀스로서 각 문서를 수신하고, 상기 토큰화된 입력 시퀀스는 상기 어휘를 사용하여 토큰화되고, 상기 사전 훈련된 언어 모델 레이어는 컨텍스트 특징들로 상기 토큰화된 입력 시퀀스에 각 토큰을 임베드하고, 하나 이상의 선형 레이어들을 사용하여 컨텍스트 임베드된 토큰들을 변환함으로써 상기 어휘를 통해 상기 임베드된 입력 시퀀스의 각 토큰에 관하여 중요도를 예측하는,방법.</claim></claimInfo><claimInfo><claim>19. 제18 항에 있어서,상기 문서 인코더는 상기 어휘를 통해 각 토큰에 관하여 상기 예측된 중요도를 수신하고, 상기 임베드된 입력 시퀀스에 대한 상기 예측된 중요도의 오목 활성화를 수행함으로써 상기 어휘를 통해 상기 입력 시퀀스의 상기 예측된 용어 중요도를 획득하고, 상기 어휘를 통해 상기 입력 시퀀스의 표현으로서 상기 입력 시퀀스의 상기 예측된 용어 중요도를 출력하는,방법.</claim></claimInfo><claimInfo><claim>20. 제12 항에 있어서,상기 문서 인코더의 상기 사전 훈련된 언어 모델은 상기 언어 모델이 정보 검색을 위해 미세 조정되기 전에 중간 훈련에 의해 훈련되고, 상기 중간 훈련은 예측을 위해 상기 사전 훈련된 언어 모델을 사전 훈련한 후에 발생하거나, 상기 중간 훈련은 강화된 사전 훈련을 제공하기 위해 예측을 위해 상기 사전 훈련된 언어 모델을 사전 훈련하는 것과 동시에 발생하는,방법. </claim></claimInfo><claimInfo><claim>21. 제20 항에 있어서,상기 사전 훈련된 언어 모델은 예측을 위해 사전 훈련되고,상기 중간 훈련 또는 강화된 사전 훈련은 FLOPS 정규화와 결합되는 마스크드 언어 모델(MLM)을 사용하여 상기 사전 훈련된 언어 모델을 훈련하는 것을 포함하는,방법.</claim></claimInfo><claimInfo><claim>22. 제21 항에 있어서,상기 중간 훈련 또는 강화된 사전 훈련은,표준 MLM 손실;로짓들(logits)의 희소 세트에 대한 MLM 손실; 및FLOPS 정규화 손실 을 포함하는 손실을 기반으로 하는, 방법.</claim></claimInfo><claimInfo><claim>23. 제12 항에 있어서, 상기 랭커는 하나 이상의 하이퍼파라미터들을 포함하는 최적화를 사용하여 훈련되고, 상기 하이퍼파라미터들은 미리 결정된 쿼리 및 문서 크기들에 기반하여 선택되는,방법.</claim></claimInfo><claimInfo><claim>24. 제12 항에 있어서,상기 랭커는 증류를 사용하여 훈련되는,방법.</claim></claimInfo><claimInfo><claim>25. 제12 항에 있어서, 상기 문서 스코어들의 세트를 생성하는 데 사용되는 방법보다 더 짧은 대기 시간을 갖는 추가 검색 방법을 사용하여 상기 쿼리를 처리함으로써 상기 하나 이상의 문서들에 대한 각각의 문서 스코어들의 추가 세트를 생성하는 단계; 상기 문서 스코어들의 세트 및 상기 각각의 문서 스코어들의 추가 세트를 병합하는 단계; 및상기 병합된 문서 스코어들의 세트들에 기반하여 상기 하나 이상의 문서들을 순위화하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>26. 제12 항에 있어서,상기 문서 인코더는 오프라인 동안 상기 수신된 하나 이상의 문서들의 적어도 서브세트에 대해 상기 희소 표현을 생성하고, 상기 쿼리 인코더는 오프라인 동안 상기 수신된 쿼리의 상기 표현을 생성하는,방법.</claim></claimInfo><claimInfo><claim>27. 정보 검색을 위한 컴퓨터 구현 방법에 있어서, 상기 방법은,사전 훈련된 언어 모델 레이어를 포함하는 문서 인코더에 의해, 어휘를 통해 문서의 용어 중요도를 예측하는 수신된 하나 이상의 문서들의 각각에 대한 희소 표현을 생성하는 단계; 쿼리 인코더에 의해, 상기 어휘를 통해 수신된 쿼리의 표현을 생성하는 단계;상기 쿼리의 상기 생성된 표현을 상기 하나 이상의 문서들의 생성된 표현들과 비교하여, 각각의 문서 스코어들의 세트를 생성하는 단계; 및 상기 생성된 문서 스코어들의 세트에 기반하여 상기 하나 이상의 문서들을 순위화하는 단계를 포함하고, 상기 문서 인코더의 상기 사전 훈련된 언어 모델은 상기 언어 모델이 정보 검색을 위해 미세 조정되기 전에 중간 훈련에 의해 훈련되는,방법.</claim></claimInfo><claimInfo><claim>28. 제27 항에 있어서,상기 사전 훈련된 언어 모델은 예측을 위해 사전 훈련되고, 상기 중간 훈련은 예측을 위해 상기 사전 훈련된 모델을 사전 훈련한 후에 발생하거나, 상기 중간 훈련은 강화된 사전 훈련을 제공하기 위해 예측을 위해 상기 사전 훈련된 언어 모델을 사전 훈련하는 것과 동시에 발생하는,방법.</claim></claimInfo><claimInfo><claim>29. 제27 항에 있어서,상기 쿼리 인코더는 추가적인 사전 훈련된 언어 모델을 포함하고, 상기 쿼리 인코더의 상기 추가적인 사전 훈련된 언어 모델은 상기 언어 모델이 정보 검색을 위해 미세 조정되기 전에 중간 훈련 또는 강화된 사전 훈련에 의해 훈련되는,방법.</claim></claimInfo><claimInfo><claim>30. 제27 항에 있어서, 상기 중간 훈련 또는 강화된 사전 훈련은,표준 MLM 손실;로짓들의 희소 세트에 대한 MLM 손실; 및FLOPS 정규화 손실 을 포함하는 손실을 기반으로 하는, 방법.</claim></claimInfo><claimInfo><claim>31. 제27 항에 있어서,상기 중간 훈련 또는 강화된 사전 훈련은 FLOPS 정규화와 결합되는 마스크드 언어 모델(MLM) 훈련을 사용하여 상기 사전 훈련된 언어 모델을 훈련하는 것을 포함하는,방법.</claim></claimInfo><claimInfo><claim>32. 제27 항에 있어서,상기 문서 인코더 및 상기 쿼리 인코더는 각각 별도의 인코더들인,방법.</claim></claimInfo><claimInfo><claim>33. 제32 항에 있어서, 상기 문서 인코더 및 상기 쿼리 인코더는 상이한 정규화기들을 사용하여 훈련되고, 상기 쿼리 인코더는 L1 정규화를 사용하여 정규화되고, 상기 문서 인코더는 FLOPS 정규화를 사용하여 정규화되는,방법.</claim></claimInfo><claimInfo><claim>34. 정보 검색 모델의 뉴럴 랭커를 훈련하기 위한 컴퓨터 구현 방법에 있어서, 상기 방법은,상기 뉴럴 랭커의 파라미터들을 초기화하는 단계;문서들 및 쿼리들을 포함하는 데이터세트를 상기 랭커의 문서 인코더 및 쿼리 인코더에 제공하는 단계 - 상기 문서 인코더는 사전 훈련된 언어 모델 레이어를 포함하고, 상기 문서들을 수신하고 어휘를 통해 문서의 용어 중요도를 예측하는 상기 문서들의 각각에 대한 희소 표현을 생성하도록 구성되고, 상기 쿼리 인코더는 상기 문서 인코더와 별도로 존재하고, 상기 쿼리들을 수신하고 어휘를 통해 쿼리의 표현을 생성하도록 구성됨 -; 및상기 문서들 및 쿼리들의 상기 생성된 표현들을 기반으로 하는 랭킹 손실 및 적어도 하나의 정규화 손실을 포함하는 손실을 최적화하는 단계를 포함하고, 상기 랭킹 손실 및/또는 상기 적어도 하나의 정규화 손실은 가중 파라미터에 의해 가중되는,방법.</claim></claimInfo><claimInfo><claim>35. 제34 항에 있어서,상기 문서 인코더 및 상기 쿼리 인코더는 모델 아키텍처, 모델 크기, 모델 가중치들, 모델 훈련, 모델 정규화, 모델 하이퍼파라미터들, 또는 상기 랭커 내의 모델 위치 중 하나 이상에 의해 서로로부터 구별되는,방법.</claim></claimInfo><claimInfo><claim>36. 제34 항에 있어서,상기 적어도 하나의 정규화 손실은 상기 문서 인코더 및 상기 쿼리 인코더에 대한 상이한 정규화기들에 기반하여 결정되고, 상기 쿼리 인코더는 L1 정규화를 사용하여 정규화되고, 상기 문서 인코더는 FLOPS 정규화를 사용하여 정규화되고,상기 쿼리 인코더는 상기 문서 인코더의 상기 사전 훈련된 언어 모델보다 더 효율적인 사전 훈련된 모델을 포함하는,방법.</claim></claimInfo><claimInfo><claim>37. 제34 항에 있어서,상기 언어 모델이 정보 검색을 위해 미세 조정되기 전에 상기 문서 인코더의 상기 사전 훈련된 언어 모델을 중간 훈련 또는 강화된 사전 훈련하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>38. 제34 항에 있어서,상기 사전 훈련된 언어 모델은 예측을 위해 사전 훈련되고, 상기 중간 훈련은 FLOPS 정규화와 결합되는 마스크드 언어 모델(MLM) 훈련을 사용하여 상기 언어 모델을 훈련하는 것을 포함하고, 상기 사전 훈련 및 상기 중간 훈련은 공통 MLM 손실을 사용하는,방법.</claim></claimInfo><claimInfo><claim>39. 제34 항에 있어서,상기 랭커는 하나 이상의 하이퍼파라미터들을 포함하는 최적화를 사용하여 훈련되고, 상기 하이퍼파라미터들은 미리 결정된 쿼리 및 문서 크기들에 기반하여 선택되는,방법.</claim></claimInfo><claimInfo><claim>40. 제34 항에 있어서,상기 랭커는 증류를 사용하여 훈련되는,방법.</claim></claimInfo><claimInfo><claim>41. 인코더를 훈련하기 위한 컴퓨터 구현 방법에 있어서, 상기 방법은,상기 인코더의 사전 훈련된 언어 모델을 중간 훈련하는 단계; 및상기 중간 훈련하는 단계 후에 정보 검색을 위해 상기 인코더의 상기 사전 훈련된 언어 모델을 미세 조정하는 단계를 포함하고, 상기 미세 조정하는 단계 후의 상기 인코더는 어휘를 통해 문서의 용어 중요도를 예측하는 수신된 하나 이상의 문서들의 각각에 대한 희소 표현을 생성하도록 구성되는,방법. </claim></claimInfo><claimInfo><claim>42. 제41 항에 있어서, 상기 인코더의 상기 언어 모델을 사전 훈련하는 단계를 더 포함하고,상기 중간 훈련하는 단계는 상기 사전 훈련하는 단계 후에 발생하거나 상기 사전 훈련하는 단계와 동시에 발생하고,상기 인코더의 상기 사전 훈련된 언어 모델은 예측을 위해 사전 훈련되고, 상기 중간 훈련하는 단계는 FLOPS 정규화와 결합되는 마스크드 언어 모델(MLM) 훈련을 사용하여 상기 언어 모델을 훈련하는 단계를 포함하는,방법.</claim></claimInfo><claimInfo><claim>43. 제42 항에 있어서,상기 미세 조정하는 단계 후의 상기 사전 훈련된 언어 모델 레이어는 컨텍스트 특징들로 문서에 대한 토큰화된 입력 시퀀스에 각 토큰을 임베드하고, 하나 이상의 선형 레이어들을 사용하여 컨텍스트 임베드된 토큰들을 변환함으로써 상기 어휘를 통해 상기 임베드된 입력 시퀀스의 각 토큰에 관하여 중요도를 예측하도록 구성되는,방법.</claim></claimInfo><claimInfo><claim>44. 제43 항에 있어서, 상기 인코더는 상기 어휘를 통해 각 토큰에 관하여 상기 예측된 중요도를 수신하고 상기 어휘를 통해 상기 입력 시퀀스의 상기 예측된 용어 중요도를 획득하도록 구성되는 표현 레이어를 더 포함하고, 상기 표현 레이어는 상기 임베드된 입력 시퀀스에 대한 상기 예측된 중요도의 오목 활성화를 수행하도록 구성되는 오목 활성화 레이어를 포함하고,상기 표현 레이어는 상기 어휘를 통해 상기 입력 시퀀스의 표현으로서 상기 입력 시퀀스의 상기 예측된 용어 중요도를 출력하는,방법.</claim></claimInfo><claimInfo><claim>45. 제44 항에 있어서,상기 인코더는 문서 인코더를 포함하고, 상기 문서 인코더는 정보 검색을 위한 랭커에 통합되는,방법.</claim></claimInfo><claimInfo><claim>46. 제44 항에 있어서,상기 인코더는 쿼리 인코더를 포함하고,상기 쿼리 인코더는 정보 검색을 위한 랭커에 통합되는,방법. </claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 성남시 분당구...</address><code>119990373888</code><country>대한민국</country><engName>NAVER Corporation</engName><name>네이버 주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>프랑스 메일랑 ***** 체...</address><code> </code><country> </country><engName>LASSANCE, Carlos</engName><name>라상스, 카를로스</name></inventorInfo><inventorInfo><address>프랑스 메일랑 ***** 체...</address><code> </code><country> </country><engName>CLINCHANT, Stephane</engName><name>클린찬트, 스테판</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 선릉로***길 ** (논현동) 삼성빌딩 *층(피앤티특허법률사무소)</address><code>920050004530</code><country>대한민국</country><engName>Yang,Sung Bo</engName><name>양성보</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.07.07</priorityApplicationDate><priorityApplicationNumber>1020220083633</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.07.04</receiptDate><receiptNumber>1-1-2023-0736128-39</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230086678.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93a205213ecd519ec69e6d934f7f1ecf26c18379a71bbda2e8ef4b4ff8e2b3fedfefe92d370b92e52c4d8f30829a74a27314d58a09838f775b</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf04568f35b329a7ae89b90b8abdc009b4c8ec25cdb7feca46673e0ae60f8b8b9a8403b6b3c8bfccdb2899da557ba9fb91144794463e5bbe6d</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>