<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:04:35.435</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.09.14</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2025-7007760</applicationNumber><claimCount>31</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>자연어(NL) 명령어와 로봇이 있는 환경에 존재하는 객체의 설명자에 기초한 로봇 제어</inventionTitle><inventionTitleEng>ROBOT CONTROL BASED ON NATURAL LANGUAGE INSTRUCTIONS AND ON DESCRIPTORS OF OBJECTS THAT ARE PRESENT IN THE ENVIRONMENT OF THE ROBOT</inventionTitleEng><openDate>2025.04.04</openDate><openNumber>10-2025-0047803</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.03.07</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.03.07</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 11/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 5/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/279</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/284</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/30</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/35</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/25</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/70</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 일부 구현은 환경 전체를 통해 캡처된 비전 데이터 인스턴스를 프로세싱하는 것에 기초하여 관심 영역과 각각의 관심 영역에 대한 추정된 맵 위치 및 영역 임베딩(들)을 생성하는 것과 관련된다. 일부 구현은 추가적으로 또는 대안적으로 (1) 로봇이 작업을 수행하기 위한 자유형(FF) 자연어(NL) 명령어 및 (2) 환경에서 식별된 관심 영역에 대한 생성된 영역 임베딩(들)에 기초하여 작업을 수행하는 것과 관련되고 환경에 존재할 가능성이 있는 객체를 설명하는 객체 설명자를 결정하는 것과 관련된다. 일부 구현은 추가적으로 또는 대안적으로 FF NL 명령어에 특정된 작업을 수행하는 데 구현할 로봇(들)에 대한 로봇 기술(들)을 결정할 때 FF NL 명령어의 작업을 수행하는 것과 관련되고 환경에 포함될 가능성이 있는 객체(들)를 설명하는 것으로 결정된 객체 설명자(들)의 서브세트를 이용하는 것과 관련된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2024.03.21</internationOpenDate><internationOpenNumber>WO2024059179</internationOpenNumber><internationalApplicationDate>2023.09.14</internationalApplicationDate><internationalApplicationNumber>PCT/US2023/032714</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 하나 이상의 프로세서에 의해 구현되는 방법으로서,하나 이상의 비전(vision) 구성 요소를 사용하여, 적어도 하나의 로봇의 환경 전체를 통해 비전 데이터 인스턴스들을 캡처하는 단계;상기 환경의 관심 영역들을 식별하고, 상기 관심 영역들의 각각에 대해: 상기 관심 영역의 추정된 위치, 및 상기 관심 영역에 대해, 자연어 임베딩(embedding) 공간에 있고 상기 관심 영역의 시각적 피처들과 의미적으로 대응하는 영역 임베딩을 결정하기 위해 상기 비전 데이터 인스턴스들을 프로세싱하는 단계;상기 관심 영역들의 각각에 대해, 상기 관심 영역의 상기 추정된 위치와 상기 관심 영역에 대한 상기 영역 임베딩의 연관을 저장하는 단계;로봇이 작업을 수행하기 위한 명령어를 식별하는 단계 - 상기 명령어는 하나 이상의 사용자 인터페이스 입력 디바이스를 통해 사용자에 의해 제공된 사용자 인터페이스 입력에 기초하여 생성된 자유형 자연어 명령어임 -;상기 명령어에 기초하여, 상기 작업의 수행과 관련된 대응하는 후보 환경 객체를 각각 설명하는 객체 설명자들을 결정하는 단계;상기 환경에 존재할 가능성이 있는 대응하는 객체를 각각 설명하는 상기 객체 설명자들의 서브세트를 식별하기 위해,  상기 객체 설명자들에 대한 객체 설명자 임베딩들을 상기 관심 영역들에 대한 상기 영역 임베딩들과 비교하는 단계;상기 객체 설명자들의 서브세트를 식별하는 것에 응답하여, 상기 객체 설명자들과 상기 명령어에 따르는 후보 단어 구성들에 대한 확률 분포를 모델링하는 대규모 언어 모델(LLM: large language model) 출력을 생성하기 위해 대규모 언어 모델(LLM)을 사용하여 상기 객체 설명자들의 서브세트와 상기 명령어를 프로세싱하는 단계;상기 LLM 출력과 상기 로봇에 의해 수행될 수 있는 로봇 기술의 자연어 설명인 기술 설명에 기초하여, 상기 로봇 기술을 구현하는 것으로 결정하는 단계; 및상기 로봇 기술을 구현하는 것으로 결정하는 것에 응답하여,상기 로봇으로 하여금 상기 환경에서 상기 로봇 기술을 구현하게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 로봇 기술의 상기 자연어 설명은 기술 액션 설명자 및 기술 객체 설명자를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 기술 객체 설명자에 대한 기술 객체 설명자 임베딩을 주어진 관심 영역에 대한 상기 영역 임베딩과 비교하는 것에 기초하여, 상기 관심 영역들 중 상기 주어진 관심 영역을 식별하는 단계;상기 주어진 관심 영역을 식별하는 것에 응답하여, 상기 로봇으로 하여금 상기 환경에서 상기 로봇 기술을 구현하게 하는 데 상기 관심 영역의 상기 추정된 위치를 사용하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 로봇 기술은 내비게이션(navigation) 기술이고, 상기 로봇으로 하여금 상기 환경에서 상기 로봇 기술을 구현하게 하는 데 상기 관심 영역의 상기 추정된 위치를 사용하는 단계는,상기 로봇으로 하여금 상기 추정된 위치에 기초하여 결정된 특정 위치로 내비게이팅하게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제3항에 있어서,상기 기술 객체 설명자에 대한 상기 기술 객체 설명자 임베딩을 추가의 주어진 관심 영역에 대한 상기 영역 임베딩과 비교하는 것에 기초하여 상기 관심 영역들 중 상기 추가의 주어진 관심 영역을 식별하는 단계;상기 관심 영역의 상기 추정된 위치와 상기 추가의 관심 영역의 상기 추정된 위치에 기초하여, 상기 관심 영역과 상기 추가의 관심 영역이 동일한 객체에 대응하는 것으로 결정하는 단계; 및상기 관심 영역과 상기 추가의 관심 영역이 상기 동일한 객체에 대응하는 것으로 결정하는 것에 응답하여, 상기 로봇으로 하여금 상기 환경에서 상기 로봇 기술을 구현하게 하는 데 상기 관심 영역의 상기 추정된 위치와 상기 추가의 관심 영역의 상기 추정된 위치를 사용하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 로봇 기술은 내비게이션 기술이고, 상기 로봇으로 하여금 상기 환경에서 상기 로봇 기술을 구현하게 하는 데 상기 관심 영역의 상기 추정된 위치와 상기 추가의 관심 영역의 상기 추정된 위치를 사용하는 단계는,상기 관심 영역의 상기 추정된 위치와 상기 추가의 관심 영역의 상기 추정된 위치의 함수로서 특정 위치를 결정하는 단계; 및상기 로봇으로 하여금 상기 특정 위치로 내비게이팅하게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제5항에 있어서,상기 관심 영역과 상기 추가의 관심 영역이 상기 동일한 객체에 대응하는 것으로 결정하는 단계는 추가로 상기 제1 관심 영역의 제1 크기를 상기 제2 관심 영역의 제2 크기와 비교하는 것에 기초하는, 방법.</claim></claimInfo><claimInfo><claim>8. 제3항에 있어서,상기 기술 객체 설명자는 상기 서브세트의 상기 객체 설명자들 중 하나에 따르는, 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 환경의 상기 관심 영역들을 식별하고, 상기 관심 영역들의 각각에 대해 상기 추정된 위치 및 상기 영역 임베딩을 결정하기 위해 상기 비전 데이터 인스턴스들을 프로세싱하는 단계는,상기 비전 데이터 인스턴스들의 주어진 비전 데이터 인스턴스에 대해: 클래스 독립형(class-agnostic) 객체 검출 모델을 사용하여, 상기 비전 데이터 인스턴스의 주어진 관심 영역을 식별하기 위해 상기 주어진 비전 데이터 인스턴스를 프로세싱하는 단계; 상기 주어진 비전 데이터 인스턴스가 캡처되었을 때 상기 주어진 관심 영역과 비전 구성 요소의 포즈(pose)에 기초하여, 상기 주어진 관심 영역에 대한 상기 추정된 위치를 결정하는 단계; 및 상기 주어진 관심 영역에 대해, 상기 주어진 관심 영역에 대응하는 상기 주어진 비전 데이터 인스턴스의 일부를 프로세싱하는 것에 기초하여 상기 영역 임베딩을 생성하는 단계 - 상기 일부를 프로세싱하는 것은 이미지들의 자연어 설명들을 예측하기 위해 훈련된 시각적 언어 모델(VLM: visual language model) 인코더를 사용하는 것임 - 를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 로봇 기술을 구현하는 것으로 결정하는 것에 응답하여:상기 객체 설명자들, 상기 명령어 및 상기 기술 설명에 따르는 상기 후보 단어 구성들에 대한 추가의 확률 분포를 모델링하는 추가의 LLM 출력을 생성하기 위해 상기 LLM을 사용하여 상기 객체 설명자들의 서브세트, 상기 명령어 및 상기 로봇 기술의 상기 기술 설명을 프로세싱하는 단계;상기 추가의 LLM 출력과 상기 로봇에 의해 수행될 수 있는 추가의 로봇 기술의 추가의 자연어 설명인 추가의 기술 설명에 기초하여, 상기 추가의 로봇 기술을 구현하는 것으로 결정하는 단계; 및상기 추가의 로봇 기술을 구현하는 것으로 결정하는 것에 응답하여:상기 로봇으로 하여금 상기 환경에서 그리고 상기 환경에서 상기 로봇 기술의 구현 후에 상기 추가의 로봇 기술을 구현하게 하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 추가의 로봇 기술을 구현하는 것으로 결정하는 것에 응답하여:상기 객체 설명자들, 상기 명령어, 상기 기술 설명 및 상기 추가의 기술 설명에 따르는 상기 후보 단어 구성들에 대한 추가의 확률 분포를 모델링하는 추가의 LLM 출력을 생성하기 위해 상기 LLM을 사용하여 상기 객체 설명자들의 서브세트, 상기 명령어, 상기 로봇 기술의 상기 기술 설명 및 상기 추가의 로봇 기술의 상기 추가의 기술 설명을 프로세싱하는 단계; 및상기 추가의 LLM 출력에 기초하여 상기 로봇에 의한 상기 작업의 수행이 완료된 것으로 결정하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 제1항에 있어서,상기 로봇 기술을 구현하는 것으로 결정하는 것에 응답하여:상기 객체 설명자들, 상기 명령어 및 상기 기술 설명에 따르는 상기 후보 단어 구성들에 대한 추가의 확률 분포를 모델링하는 추가의 LLM 출력을 생성하기 위해 상기 LLM을 사용하여 상기 객체 설명자들의 서브세트, 상기 명령어 및 상기 로봇 기술의 상기 기술 설명을 프로세싱하는 단계; 및상기 추가의 LLM 출력에 기초하여 상기 로봇에 의한 상기 작업의 수행이 완료된 것으로 결정하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>13. 제1항에 있어서,상기 객체 설명자 임베딩들을 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 객체 설명자 임베딩들의 각각을 생성하는 단계는,상기 객체 설명자 임베딩들 중 대응하는 하나를 생성하기 위해 텍스트 인코딩 모델을 사용하여 상기 객체 설명자들 중 대응하는 하나를 프로세싱하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>15. 제1항에 있어서,상기 객체 설명자들은 상기 명령어에 명시적으로 특정되지 않은 하나 이상의 객체 설명자를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 명령어에 기초하여 상기 작업의 수행과 관련된 대응하는 후보 환경 객체를 각각 설명하는 객체 설명자들을 결정하는 단계는,대체 LLM 출력을 생성하기 위해 상기 LLM 또는 추가의 LLM을 사용하여 상기 명령어를 프로세싱하는 단계; 및상기 대체 LLM 출력에 기초하여 상기 객체 설명자들 중 하나 이상을 결정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 대체 LLM 출력에 기초하여 카테고리(category)의 카테고리 설명자를 결정하는 단계; 및상기 카테고리의 멤버들인 특정 객체들의 설명자들인 주어진 설명자들에 기초하고 상기 대체 LLM 출력에 기초하여 결정된 상기 카테고리 설명자에 기초하여, 상기 객체 설명자들의 상기 주어진 설명자들을 결정하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>18. 제16항에 있어서,상기 명령어에 존재하는 카테고리의 카테고리 설명자를 식별하는 단계; 및상기 카테고리의 멤버들인 특정 객체들의 설명자들인 주어진 설명자들에 기초하고 상기 명령어에 존재하는 상기 카테고리 설명자에 기초하여, 상기 객체 설명자들의 상기 주어진 설명자들을 결정하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>19. 제1항에 있어서,상기 LLM 출력과 상기 로봇 기술의 상기 자연어 설명인 상기 기술 설명에 기초하여, 상기 로봇 기술을 구현하는 것으로 결정하는 단계는,상기 LLM 출력의 상기 확률 분포가 확률의 임계 정도를 충족하는 확률로 상기 기술 설명을 나타내고, 상기 확률이 상기 로봇에 의해 수행될 수 있는 다른 후보 로봇 기술들의 다른 후보 기술 설명들에 대해 결정된 다른 확률들보다 큰 것으로 결정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>20. 제19항에 있어서,상기 로봇에 의해 수행될 수 있는 기술들의 슈퍼세트로부터 상기 로봇 기술과 상기 다른 후보 로봇 기술들만 선택하는 단계; 및상기 선택에 응답하여, 상기 로봇 기술과 상기 다른 후보 로봇 기술들만에 대한 상기 확률 및 상기 다른 확률들을 결정하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>21. 제20항에 있어서,상기 로봇 기술과 상기 다른 후보 로봇 기술들만 선택하는 단계는 상기 기술 설명자와 상기 다른 기술 설명자들을 상기 객체 설명자들의 서브세트 및/또는 상기 관심 영역들에 대한 상기 영역 임베딩들과 비교하는 것에 기초하는, 방법.</claim></claimInfo><claimInfo><claim>22. 방법으로서,하나 이상의 로봇의 환경 전체를 통해 캡처된 비전 데이터 인스턴스들을 프로세싱하는 것에 기초하여, 관심 영역들과 상기 관심 영역들의 각각에 대해 추정된 맵 위치 및 대응하는 영역 임베딩을 생성하는 단계;하나 이상의 사용자 인터페이스 입력 디바이스를 통해 제공되고 로봇에게 작업을 수행하도록 명령하는 자유형(FF: free form) 자연어(NL: natural language) 명령어를 수신하는 단계;상기 FF NL 명령어와 상기 관심 영역들에 대한 상기 영역 임베딩들에 기초하여, 상기 작업을 수행하는 것과 관련되고 상기 환경에 존재할 가능성이 있는 객체들을 각각 설명하는 객체 설명자들을 결정하는 단계; 및상기 작업을 수행할 때 구현할 상기 로봇(들) 중 적어도 하나에 대한 로봇 기술들을 결정하는 데 상기 결정된 객체 설명자들을 이용하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>23. 제22항에 있어서,상기 로봇들 중 적어도 하나로 하여금 상기 환경의 상기 로봇 기술들을 구현하게 하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 제22항에 있어서,상기 작업을 수행할 때 구현할 상기 로봇(들) 중 적어도 하나에 대한 상기 로봇 기술들을 결정하는 데 상기 결정된 객체 설명자들을 이용하는 단계는,대규모 언어 모델(LLM) 기반 로봇 계획에서 상기 결정된 객체 설명자들을 이용하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>25. 제24항에 있어서,상기 작업을 수행할 때 구현할 상기 로봇(들) 중 적어도 하나에 대한 상기 로봇 기술들을 결정하는 데 상기 결정된 객체 설명자들을 이용하는 단계는,LLM을 사용하여, 상기 결정된 객체 설명자들과 상기 FF NL 명령어들을 프로세싱하는 것에 기초하여 LLM 출력의 인스턴스들을 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>26. 제24항에 있어서,상기 작업을 수행할 때 구현할 상기 로봇(들) 중 적어도 하나에 대한 상기 로봇 기술들을 결정하는 데 상기 결정된 객체 설명자들을 이용하는 단계는,상기 환경에서 상기 작업을 수행하는 데 구현할 로봇(들)에 대한 로봇 기술들을 결정하는 데 LLM 출력의 상기 인스턴스(들)를 사용하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>27. 제22항에 있어서,상기 결정된 로봇 기술들 중 하나 이상을 구현하는 데 상기 결정된 맵 위치들 중 적어도 하나를 이용하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>28. 하나 이상의 프로세서에 의해 구현되는 방법으로서,하나 이상의 비전 구성 요소를 사용하여 적어도 하나의 로봇의 환경 전체를 통해 비전 데이터 인스턴스들을 캡처하는 단계;상기 비전 데이터 인스턴스들의 각각에 대해: 클래스 독립형 객체 검출 모델을 사용하여, 상기 비전 데이터 인스턴스의 임의의 관심 영역들을 식별하기 위해 상기 비전 데이터 인스턴스를 프로세싱하는 단계;상기 비전 데이터 인스턴스들로부터 식별된 복수의 관심 영역의 각각에 대해: 상기 관심 영역의 추정된 위치를 결정하는 단계; 비전 데이터를 프로세싱하는 것에 기초하여, 상기 비전 데이터 인스턴스들에 대응하고 상기 관심 영역에 대응하는 것 중 하나로부터 상기 관심 영역에 대한 영역 임베딩을 생성하는 단계 - 상기 영역 임베딩을 생성하는 단계는 이미지들의 자연어 설명들을 예측하기 위해 훈련된 시각적 언어 모델 인코더를 사용하여 상기 비전 데이터를 프로세싱하는 단계를 포함함 -; 및 상기 추정된 위치를 상기 영역 임베딩과 연관시키는 엔트리를 생성하는 단계;상기 엔트리들을 생성하는 단계에 후속하여: 작업을 수행하기 위해 로봇에 대한 명령어를 식별하는 단계 - 상기 명령어는 하나 이상의 사용자 인터페이스 입력 디바이스를 통해 사용자에 의해 제공된 사용자 인터페이스 입력에 기초하여 생성된 자유형 자연어 명령어임 -; 상기 명령어에 기초하여, 상기 작업의 수행과 관련된 대응하는 객체를 각각 설명하는 하나 이상의 객체 설명자를 결정하는 단계; 대응하는 객체 설명자 임베딩을 생성하기 위해 텍스트 인코딩 모델을 사용하여 상기 하나 이상의 객체 설명자의 각각을 프로세싱하는 단계; 상기 엔트리들 중 적어도 하나에 각각 대응하는 상기 객체 설명자 임베딩들의 서브세트를 식별하기 위해, 상기 객체 설명자 임베딩들을 상기 엔트리들의 상기 영역 임베딩들과 비교하는 단계; 상기 객체 설명자들의 서브세트를 식별하는 것에 응답하여, 상기 객체 설명자들과 상기 명령어에 따르는 후보 단어 구성들에 대한 확률 분포를 모델링하는 LLM 출력을 생성하기 위해 대규모 언어 모델(LLM)을 사용하여 상기 객체 설명자들의 서브세트와 상기 명령어를 프로세싱하는 단계;상기 LLM 출력과 상기 로봇에 의해 수행될 수 있는 로봇 기술의 자연어 설명인 기술 설명에 기초하여, 상기 로봇 기술을 구현하는 것으로 결정하는 단계; 및상기 로봇 기술을 구현하는 것으로 결정하는 것에 응답하여: 상기 로봇으로 하여금 상기 환경에서 상기 로봇 기술을 구현하게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>29. 명령어들을 저장하는 메모리와 제1항 내지 제28항 중 어느 한 항의 방법을 수행하기 위해 상기 명령어들을 실행하도록 동작 가능한 하나 이상의 프로세서를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>30. 제29항에 있어서,상기 시스템은 하나 이상의 로봇을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>31. 하나 이상의 프로세서에 의해 실행 시 제1항 내지 제28항 중 어느 한 항의 방법을 수행하는 명령어들을 저장한, 하나 이상의 컴퓨터 판독 가능 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 캘리포니아 마운틴 뷰 엠피시어터 파크웨이 **** (우:*****)</address><code>520050013456</code><country>미국</country><engName>Google LLC</engName><name>구글 엘엘씨</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>중국</country><engName>CHEN, Boyuan</engName><name>천, 보위안</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>미국</country><engName>KAPPLER, Daniel</engName><name>카플러, 다니엘</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>중국</country><engName>XIA, Fei</engName><name>샤, 페이</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920020002981</code><country>대한민국</country><engName>Lee Min Ho</engName><name>이민호</name></agentInfo><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.09.15</priorityApplicationDate><priorityApplicationNumber>63/407,019</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2025.03.07</receiptDate><receiptNumber>1-1-2025-0262512-09</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2025.03.07</receiptDate><receiptNumber>1-1-2025-0263066-15</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2025.03.11</receiptDate><receiptNumber>1-5-2025-0042051-41</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257007760.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9317d341e8aaf4f7705f1c1dd9f9d6aa6fea1328f349a180d23576061b766f61bb6c436a5c2be19b0b1edb60963ec92daaa95de2eb0de9324a</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf1a91218f53c20b43c7b904822b506144278ed86d2cc595bf7d7292a3553b1bef31f70cda3f149446f755410157d91f08d5240bb8cc60fa92</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>