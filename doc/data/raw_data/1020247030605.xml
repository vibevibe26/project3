<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:54:36.5436</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.03.02</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-7030605</applicationNumber><claimCount>24</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>구조화된 비디오 문서</inventionTitle><inventionTitleEng>STRUCTURED VIDEO DOCUMENTS</inventionTitleEng><openDate>2024.10.17</openDate><openNumber>10-2024-0151201</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.09.11</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.09.11</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 16/783</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 16/74</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G10L 15/04</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G10L 15/26</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G10L 15/22</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/096</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 방법(500)은 음성 발화에 대응하는 오디오 데이터(122)를 포함하는 콘텐츠 피드(120)를 수신하고, 콘텐츠 피드를 프로세싱하여 의미론적으로 풍부한, 구조화된 문서(300)를 생성하는 단계를 포함한다. 구조화된 문서는 음성 발화(123)의 전사(310)를 포함하고, 각각 오디오 데이터에서 단어가 인식된 시간을 나타내는 대응하는 오디오 세그먼트(222)와 정렬된 복수의 단어(123)를 포함한다. 콘텐츠 피드를 재생하는 동안, 방법은 또한 콘텐츠 피드에 포함된 정보를 요청하는 사용자로부터 질의(112)를 수신하고, 대규모 언어 모델(180)에 의해 질의와 구조화된 문서를 프로세싱하여 질의에 대한 응답(182)을 생성하는 단계를 포함한다. 응답은 콘텐츠 피드에 포함된 요청된 정보를 전달한다. 방법은 또한 사용자와 연관된 사용자 디바이스(102)로부터의 출력을 위해, 질의에 대한 응답을 제공하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2023.09.07</internationOpenDate><internationOpenNumber>WO2023168373</internationOpenNumber><internationalApplicationDate>2023.03.02</internationalApplicationDate><internationalApplicationNumber>PCT/US2023/063630</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 데이터 프로세싱 하드웨어(data processing hardware)(134)에서 실행될 때 상기 데이터 프로세싱 하드웨어(134)로 하여금 동작들을 수행하게 하는 컴퓨터 구현(computer-implemented) 방법(500)에 있어서, 상기 동작들은:오디오 데이터(122)를 포함하는 콘텐츠 피드(content feed)(120)를 수신하는 동작-상기 오디오 데이터(122)는 음성 발화(speech utterance)(123)에 대응함-;의미론적으로 풍부한(semantically-rich), 구조화된 문서(structured document)(300)를 생성하기 위해 상기 콘텐츠 피드(120)를 프로세싱하는 동작-상기 구조화된 문서(300)는 상기 음성 발화(123)의 전사(transcription)(310)를 포함하고 상기 전사(310)는 각각 상기 오디오 데이터(122)에서 단어(312)가 인식된 시간을 나타내는 상기 오디오 데이터(122)의 대응하는 오디오 세그먼트(222)와 정렬된 복수의 단어(312)를 포함함-;상기 콘텐츠 피드(120)의 재생 동안: 상기 콘텐츠 피드(120)에 포함된 정보를 요청하는 사용자로부터 질의(112)를 수신하는 동작; 및 대규모 언어 모델(large language model)(180)에 의해, 상기 질의(112) 및 상기 구조화된 문서(300)를 프로세싱하여 상기 질의(112)에 대한 응답(182)을 생성하는 동작-상기 응답(182)은 상기 콘텐츠 피드(120)에 포함된 상기 요청된 정보를 전달함-; 및상기 사용자와 연관된 사용자 디바이스(10)로부터의 출력을 위해, 상기 질의(112)에 대한 상기 응답(182)을 제공하는 동작을 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 동작들은:상기 질의(112)에 대한 상기 응답(182)에 의해 전달된 상기 요청된 정보를 포함하는 상기 전사(310)의 세그먼트를 추출하는 동작-상기 전사(310)의 세그먼트는 시작 단어(312)와 종료 단어(312)로 경계가 지정됨-;상기 오디오 데이터(122)의 시작 오디오 세그먼트를 상기 전사(310)의 상기 세그먼트의 경계를 지정하는 상기 시작 단어(312)와 정렬된 상기 오디오 데이터(122)의 상기 대응하는 오디오 세그먼트(222)로서 식별하는 동작; 및상기 오디오 데이터(122)의 종료 오디오 세그먼트를 상기 전사(310)의 상기 세그먼트의 경계를 지정하는 상기 종료 단어(312)와 정렬된 상기 오디오 데이터(122)의 상기 대응하는 오디오 세그먼트(222)로서 식별하는 동작을 더 포함하고,상기 질의(112)에 대한 상기 응답(182)을 제공하는 동작은, 상기 사용자와 연관된 상기 사용자 디바이스(10)로부터, 상기 오디오 데이터(122)의 상기 시작 오디오 세그먼트부터 상기 오디오 데이터(122)의 상기 종료 오디오 세그먼트까지 상기 오디오 데이터(122)를 재생하는 동작을 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 콘텐츠 피드(120)는 복수의 이미지 프레임(125)을 포함하는 이미지 데이터(124)를 더 포함하고; 및상기 동작들은, 상기 오디오 데이터(122)의 상기 시작 오디오 세그먼트로부터 상기 오디오 데이터(122)의 상기 종료 오디오 세그먼트까지 상기 오디오 데이터(122)를 재생하는 동안, 상기 이미지 데이터(124)의 상기 복수의 이미지 프레임(125)의 재생을 일시 중지하는 동작을 더 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>4. 제1항 내지 제3항 중 어느 한 항에 있어서,상기 콘텐츠 피드(120)는 복수의 이미지 프레임(125)을 포함하는 이미지 데이터(124)를 더포함하고; 및상기 의미적으로 풍부한, 구조화된 문서(300)는 상기 복수의 이미지 프레임(125) 중 하나 이상의 이미지 프레임(125)에서 인식된 크리에이터 제공 텍스트를 더 포함하고, 상기 크리에이터 제공 텍스트는 상기 오디오 데이터(122)의 대응하는 오디오 세그먼트(222)와 정렬되어 상기 크리에이터 제공 텍스트가 상기 하나 이상의 이미지 프레임(125)에서 인식된 시간을 나타내는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서, 상기 의미적으로 풍부한, 구조화된 문서(300)를 생성하기 위해 상기 콘텐츠 피드(120)를 프로세싱하는 동작은 상기 하나 이상의 이미지 프레임(125)에서 인식된 상기 크리에이터 제공 텍스트와 정렬된 상기 오디오 데이터(122)의 상기 대응하는 오디오 세그먼트(222)를 기초로 상기 크리에이터 제공 텍스트를 상기 전사(310)의 인접한 단어 쌍(312) 사이에 삽입함으로써 상기 음성 발화(123)의 상기 전사(310)에 상기 크리에이터 제공 텍스트로 주석을 다는 동작을 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5항 중 어느 한 항에 있어서, 상기 질의(112)에 대한 상기 응답(182)은 상기 요청된 정보를 상기 질의(112)에 대한 일관되고, 포커싱된 응답(182)으로서 전달하는 텍스트 응답(182)을 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서, 상기 동작들은:상기 질의(112)에 대한 상기 응답(182)의 합성 음성 표현(synthesized speech representation)을 생성하기 위해 상기 텍스트 응답(182)에 대한 텍스트-음성 변환을 수행하는 동작을 더 포함하고,상기 사용자 디바이스(10)로부터의 출력에 대해 상기 질의(112)에 대한 상기 응답(182)을 제공하는 동작은 상기 사용자 디바이스(10)로부터의 상기 질의(112)에 대한 상기 응답(182)의 상기 합성 음성 표현을 청각적으로 출력하는 동작을 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서, 상기 동작들은, 상기 사용자 디바이스(10)로부터의 상기 질의(112)에 대한 상기 응답(182)의 상기 합성 음성 표현을 청각적으로 출력하는 동안, 상기 콘텐츠 피드(120)의 재생을 일시 중지하는 동작을 더 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>9. 제6항 내지 제8항 중 어느 한 항에 있어서, 상기 질의(112)에 대한 상기 텍스트 응답(182)은 상기 요청된 정보와 관련된 소스 재료에 대한 하나 이상의 참조를 더 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>10. 제1항 내지 제9항 중 어느 한 항에 있어서, 상기 대규모 언어 모델(180)은 사전 트레이닝된 대규모 언어 모델(180)을 포함하고 상기 구조화된 문서(300)를 상기 질의(112)에 대한 콘텍스트로 사용하여 퓨 샷 러닝(few-shot learning)을 수행하여 상기 질의(112)에 대한 상기 응답(182)을 생성하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>11. 제1항 내지 제10항 중 어느 한 항에 있어서,상기 질의(112)는 자연어로 된 질문을 포함하고; 및상기 질의(112)에 대한 상기 응답(182)은 상기 질문에 대한 자연어 응답(182)을 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>12. 제1항 내지 제11항 중 어느 한 항에 있어서, 상기 의미적으로 풍부한, 구조화된 문서(300)를 생성하기 위해 상기 콘텐츠 피드(120)를 프로세싱하는 동작은:상기 오디오 데이터(122)를 복수의 오디오 세그먼트(222)로 세그먼트화하는 동작;각각의 오디오 세그먼트(222)에 할당된 대응하는 화자 라벨(226)을 포함하는 분할 결과(224)를 예측하기 위해 상기 복수의 오디오 세그먼트(222)에 대해 화자 분할(speaker diarization)을 수행하는 동작; 및상기 오디오 데이터(122)로부터 세그먼트화된 각각의 오디오 세그먼트에 할당된 상기 대응하는 스피커 라벨(226)을 사용하여 상기 음성 발화(123)의 상기 전사(310)를 인덱싱하는 동작을 포함하는, 컴퓨터 구현 방법(500).</claim></claimInfo><claimInfo><claim>13. 시스템(100)에 있어서,데이터 프로세싱 하드웨어(134); 및상기 데이터 프로세싱 하드웨어(134)와 통신하는 메모리 하드웨어(136)를 포함하고, 상기 메모리 하드웨어(136)는, 상기 데이터 프로세싱 하드웨어(134)에서 실행될 때, 상기 데이터 프로세싱 하드웨어(134)로 하여금 동작들을 수행하게 하는 명령어를 저장하고, 상기 동작들은: 오디오 데이터(122)를 포함하는 콘텐츠 피드(120)를 수신하는 동작-상기 오디오 데이터(122)는 음성 발화(123)에 대응함-; 의미론적으로 풍부한, 구조화된 문서(300)를 생성하기 위해 상기 콘텐츠 피드(120)를 프로세싱하는 동작-상기 구조화된 문서(300)는 상기 음성 발화(123)의 전사(310)를 포함하고 상기 전사(310)는 각각 상기 오디오 데이터(122)에서 단어(312)가 인식된 시간을 나타내는 상기 오디오 데이터(122)의 대응하는 오디오 세그먼트(222)와 정렬된 복수의 단어(312)를 포함함-; 상기 콘텐츠 피드(120)의 재생 동안:  상기 콘텐츠 피드(120)에 포함된 정보를 요청하는 사용자로부터 질의(112)를 수신하는 동작; 및  대규모 언어 모델(180)에 의해, 상기 질의(112) 및 상기 구조화된 문서(300)를 프로세싱하여 상기 질의(112)에 대한 응답(182)을 생성하는 동작-상기 응답(182)은 상기 콘텐츠 피드(120)에 포함된 상기 요청된 정보를 전달함-; 및 상기 사용자와 연관된 사용자 디바이스(10)로부터의 출력을 위해, 상기 질의(112)에 대한 상기 응답(182)을 제공하는 동작을 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서, 상기 동작들은:상기 질의(112)에 대한 상기 응답(182)에 의해 전달된 상기 요청된 정보를 포함하는 상기 전사(310)의 세그먼트를 추출하는 동작-상기 전사(310)의 세그먼트는 시작 단어(312)와 종료 단어(312)로 경계가 지정됨-;상기 오디오 데이터(122)의 시작 오디오 세그먼트를 상기 전사(310)의 상기 세그먼트의 경계를 지정하는 상기 시작 단어(312)와 정렬된 상기 오디오 데이터(122)의 상기 대응하는 오디오 세그먼트(222)로서 식별하는 동작; 및상기 오디오 데이터(122)의 종료 오디오 세그먼트를 상기 전사(310)의 상기 세그먼트의 경계를 지정하는 상기 종료 단어(312)와 정렬된 상기 오디오 데이터(122)의 상기 대응하는 오디오 세그먼트(222)로서 식별하는 동작을 더 포함하고,상기 질의(112)에 대한 상기 응답(182)을 제공하는 동작은, 상기 사용자와 연관된 상기 사용자 디바이스(10)로부터, 상기 오디오 데이터(122)의 상기 시작 오디오 세그먼트부터 상기 오디오 데이터(122)의 상기 종료 오디오 세그먼트까지 상기 오디오 데이터(122)를 재생하는 동작을 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 콘텐츠 피드(120)는 복수의 이미지 프레임(125)을 포함하는 이미지 데이터(124)를 더 포함하고; 및상기 동작들은, 상기 오디오 데이터(122)의 상기 시작 오디오 세그먼트로부터 상기 오디오 데이터(122)의 상기 종료 오디오 세그먼트까지 상기 오디오 데이터(122)를 재생하는 동안, 상기 이미지 데이터(124)의 상기 복수의 이미지 프레임(125)의 재생을 일시 중지하는 동작을 더 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>16. 제13항 내지 제15항 중 어느 한 항에 있어서,상기 콘텐츠 피드(120)는 복수의 이미지 프레임(125)을 포함하는 이미지 데이터(124)를 더포함하고; 및상기 의미적으로 풍부한, 구조화된 문서(300)는 상기 복수의 이미지 프레임(125) 중 하나 이상의 이미지 프레임(125)에서 인식된 크리에이터 제공 텍스트를 더 포함하고, 상기 크리에이터 제공 텍스트는 상기 오디오 데이터(122)의 대응하는 오디오 세그먼트(222)와 정렬되어 상기 크리에이터 제공 텍스트가 상기 하나 이상의 이미지 프레임(125)에서 인식된 시간을 나타내는, 시스템(100).</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서, 상기 의미적으로 풍부한, 구조화된 문서(300)를 생성하기 위해 상기 콘텐츠 피드(120)를 프로세싱하는 동작은 상기 하나 이상의 이미지 프레임(125)에서 인식된 상기 크리에이터 제공 텍스트와 정렬된 상기 오디오 데이터(122)의 상기 대응하는 오디오 세그먼트(222)를 기초로 상기 크리에이터 제공 텍스트를 상기 전사(310)의 인접한 단어 쌍(312) 사이에 삽입함으로써 상기 음성 발화(123)의 상기 전사(310)에 상기 크리에이터 제공 텍스트로 주석을 다는 동작을 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>18. 제13항 내지 제17항 중 어느 한 항에 있어서, 상기 질의(112)에 대한 상기 응답(182)은 상기 요청된 정보를 상기 질의(112)에 대한 일관되고, 포커싱된 응답(182)으로서 전달하는 텍스트 응답(182)을 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서, 상기 동작들은:상기 질의(112)에 대한 상기 응답(182)의 합성 음성 표현을 생성하기 위해 상기 텍스트 응답(182)에 대한 텍스트-음성 변환을 수행하는 동작을 더 포함하고,상기 사용자 디바이스(10)로부터의 출력에 대해 상기 질의(112)에 대한 상기 응답(182)을 제공하는 동작은 상기 사용자 디바이스(10)로부터의 상기 질의(112)에 대한 상기 응답(182)의 상기 합성 음성 표현을 청각적으로 출력하는 동작을 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>20. 제19항에 있어서, 상기 동작들은, 상기 사용자 디바이스(10)로부터의 상기 질의(112)에 대한 상기 응답(182)의 상기 합성 음성 표현을 청각적으로 출력하는 동안, 상기 콘텐츠 피드(120)의 재생을 일시 중지하는 동작을 더 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>21. 제18항 내지 제20항 중 어느 한 항에 있어서, 상기 질의(112)에 대한 상기 텍스트 응답(182)은 상기 요청된 정보와 관련된 소스 재료에 대한 하나 이상의 참조를 더 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>22. 제13항 내지 제21항 중 어느 한 항에 있어서, 상기 대규모 언어 모델(180)은 사전 트레이닝된 대규모 언어 모델(180)을 포함하고 상기 구조화된 문서(300)를 상기 질의(112)에 대한 콘텍스트로 사용하여 퓨 샷 러닝(을 수행하여 상기 질의(112)에 대한 상기 응답(182)을 생성하는, 시스템(100).</claim></claimInfo><claimInfo><claim>23. 제13항 내지 제22항 중 어느 한 항에 있어서,상기 질의(112)는 자연어로 된 질문을 포함하고; 및상기 질의(112)에 대한 상기 응답(182)은 상기 질문에 대한 자연어 응답(182)을 포함하는, 시스템(100).</claim></claimInfo><claimInfo><claim>24. 제13항 내지 제23항 중 어느 한 항에 있어서, 상기 의미적으로 풍부한, 구조화된 문서(300)를 생성하기 위해 상기 콘텐츠 피드(120)를 프로세싱하는 동작은:상기 오디오 데이터(122)를 복수의 오디오 세그먼트(222)로 세그먼트화하는 동작;각각의 오디오 세그먼트(222)에 할당된 대응하는 화자 라벨(226)을 포함하는 분할 결과(224)를 예측하기 위해 상기 복수의 오디오 세그먼트(222)에 대해 화자 분할을 수행하는 동작; 및상기 오디오 데이터(122)로부터 세그먼트화된 각각의 오디오 세그먼트에 할당된 상기 대응하는 스피커 라벨(226)을 사용하여 상기 음성 발화(123)의 상기 전사(310)를 인덱싱하는 동작을 포함하는, 시스템(100).</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 캘리포니아 마운틴 뷰 엠피시어터 파크웨이 **** (우:*****)</address><code>520050013456</code><country>미국</country><engName>Google LLC</engName><name>구글 엘엘씨</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>SCHALKWYK, Johan</engName><name>스칼크위크 조한</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>BEAUFAYS, Francoise</engName><name>뷰페이스 프랑코이스</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 강남대로 *** (논현동) *-*F(박장원특허법률사무소)</address><code>919980002023</code><country>대한민국</country><engName>PARK, Jang Won</engName><name>박장원</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.03.04</priorityApplicationDate><priorityApplicationNumber>63/268,921</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2024.09.11</receiptDate><receiptNumber>1-1-2024-1000529-96</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2024.09.20</receiptDate><receiptNumber>1-5-2024-0151809-31</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247030605.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c930c58305ac6705dabffd16cbe9a48f9c505a8c1fcc66d028ea34670d06c14dfb0b273f008c6e8f7c8bf0daf13a7ed743705da3f37133d44d4</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cffaf27346142aebe878a762e9aa3b6dcd31a267b76264908be6acda58f6a8e2e19b7cf3bdd7ff49f46997b4178fa6277460bbc6f185efba17</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>