<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:41.4141</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.11.07</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0147117</applicationNumber><claimCount>19</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>신경망을 위한 배치놈 파라미터 학습 방법 및 이를 수행하는 장치</inventionTitle><inventionTitleEng>BATCH NORM PARAMETERS TRAINING METHOD FOR NEURAL  NETWORK AND APPARATUS OF THEREOF</inventionTitleEng><openDate>2024.05.16</openDate><openNumber>10-2024-0067175</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2022.11.07</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/084</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/04</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 실시예는, 신경망을 위한 배치놈 파라미터 학습 방법 및 이를 위한 장치에 대한 것이다. 실시예에 따른 학습 방법은, 신경망에 포함된 제1 레이어의 출력되는 활성화 데이터와 제1 레이어에 연결되는 제2 레이어의 양자화 스케일을 이용하여 채널 별 양자화 오류를 계산하는 단계; 채널 별 양자화 오류에 기초하여 결정되는 규칙화(regularization) 손실항을 이용하여 최종 손실 함수를 도출하는 단계; 및 최종 손실 함수의 결과가 감소하도록 제1 레이어의 배치놈 파라미터를 업데이트하는 단계를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 신경망을 위한 배치놈(Batch Norm) 파라미터 학습 방법에 있어서,상기 신경망에 포함된 제1 레이어의 출력되는 활성화 데이터와 상기 제1 레이어에 연결되는 제2 레이어의 양자화 스케일을 이용하여 채널 별 양자화 오류를 계산하는 단계;상기 채널 별 양자화 오류에 기초하여 결정되는 규칙화(regularization) 손실항을 이용하여 최종 손실 함수를 도출하는 단계; 및상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 채널 별 양자화 오류를 계산하는 단계는,상기 채널 별로, 해당하는 스케일에 대한 신호 대 잡음비(SQNR)의 역수로 상기 양자화 오류를 정량화하는 단계를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 최종 손실 함수를 도출하는 단계는,상기 채널 별 양자화 오류의 평균을 계산하여 상기 규칙화 손실항을 구성하는 단계 를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 최종 손실 함수를 도출하는 단계는,상기 규칙화 손실항 및 크로스 엔트로피 손실항을 합산하여 상기 최종 손실 함수를 도출하는 단계를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계는,상기 최종 손실 함수의 결과에 대한 확률적 경사 하강법(Stochastic gradient descent)으로 상기 규칙화 손실항의 값을 감소시키는 상기 배치놈 파라미터를 도출하는 단계를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 배치놈 파라미터를 업데이트하는 단계는,상기 배치놈 파라미터 이외의 모든 파라미터의 값을 고정하는 단계를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계는,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 양자화 스케일을 학습하는 단계를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계는,상기 업데이트된 배치놈 파라미터에 대응하는 상기 신경망의 성능을 측정하는 단계를 포함하는,배치놈 파라미터 학습 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 규칙화 손실항은, 아래의 수학식으로 계산되는,배치놈 파라미터 학습 방법.[수학식]-여기서, C는 채널, f 및 X는 상기 활성화 데이터, l은 레이어, α는 양자화 스케일을 의미함-</claim></claimInfo><claimInfo><claim>10. 하드웨어와 결합되어 제1항 내지 제9항 중 어느 하나의 항의 방법을 실행시키기 위하여 기록 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>11. 신경망을 위한 배치놈 파라미터를 학습하는 장치에 있어서,하나 이상의 프로세서;메모리; 및상기 메모리에 저장되어 있으며 상기 하나 이상의 프로세서에 의하여 실행되도록 구성되는 하나 이상의 프로그램을 포함하고,상기 프로그램은,상기 신경망에 포함된 제1 레이어의 출력되는 활성화 데이터와 상기 제1 레이어에 연결되는 제2 레이어의 양자화 스케일을 이용하여 채널 별 양자화 오류를 계산하는 단계;상기 채널 별 양자화 오류에 기초하여 결정되는 규칙화(regularization) 손실항을 이용하여 최종 손실함수를 도출하는 단계; 및상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계를 실행하는,장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 채널 별 양자화 오류를 계산하는 단계는,상기 채널 별로, 해당하는 스케일에 대한 신호 대 잡음비(SQNR)의 역수로 상기 양자화 오류를 정량화하는 단계를 포함하는,장치.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 최종 손실 함수를 도출하는 단계는,상기 채널 별 양자화 오류의 평균을 계산하여 상기 규칙화 손실항을 구성하는 단계 를 포함하는,장치.</claim></claimInfo><claimInfo><claim>14. 제11항에 있어서,상기 최종 손실 함수를 도출하는 단계는,상기 규칙화 손실항 및 크로스 엔트로피 손실항을 합산하여 상기 최종 손실 함수를 도출하는 단계를 포함하는,장치.</claim></claimInfo><claimInfo><claim>15. 제11항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계는,상기 최종 손실 함수의 결과에 대한 확률적 경사 하강법(Stochastic gradient descent)으로 상기 규칙화 손실항의 값을 감소시키는 상기 배치놈 파라미터를 도출하는 단계를 포함하는,장치.</claim></claimInfo><claimInfo><claim>16. 제11항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 배치놈 파라미터를 업데이트하는 단계는,상기 배치놈 파라미터 이외의 모든 파라미터의 값을 고정하는 단계를 포함하는,장치.</claim></claimInfo><claimInfo><claim>17. 제11항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계는,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 양자화 스케일을 학습하는 단계를 포함하는,장치.</claim></claimInfo><claimInfo><claim>18. 제11항에 있어서,상기 최종 손실 함수의 결과가 감소하도록 상기 제1 레이어의 배치놈 파라미터를 업데이트하는 단계는,상기 업데이트된 배치놈 파라미터에 대응하는 상기 신경망의 성능을 측정하는 단계를 포함하는,장치.</claim></claimInfo><claimInfo><claim>19. 제11항에 있어서,상기 규칙화 손실항은, 아래의 수학식으로 계산되는,장치.[수학식]-여기서, C는 채널, f 및 X는 상기 활성화 데이터, l은 레이어, α는 양자화 스케일을 의미함-</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo><applicantInfo><address>서울특별시 성동구...</address><code>220040114276</code><country>대한민국</country><engName>IUCF-HYU (Industry-University Cooperation Foundation Hanyang University)</engName><name>한양대학교 산학협력단</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>서울특별시 강남구...</address><code> </code><country> </country><engName>CHOI Jung Wook</engName><name>최정욱</name></inventorInfo><inventorInfo><address>서울특별시 성동구...</address><code> </code><country> </country><engName>PARK Seong Min</engName><name>박성민</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.11.07</receiptDate><receiptNumber>1-1-2022-1180940-45</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220147117.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93db8b76472a45e38fb6a0c794f62d46ea1781adeafc6b41b02af183c63a81d1595e542112cd40f1b0b67e93385ef64a7eaeea74a9c31c4529</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf9940cf183a4cd1ef5938f72d2a17a0513bb715cca72d4d824486813c09ad7349f64c38a03ae4e2a6424a42734cff717b5208e5287326af9f</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>