<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:05:36.536</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2024.07.16</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-0093793</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>전자 장치에 의해 수행되는 방법, 전자 장치 및 저장 매체</inventionTitle><inventionTitleEng>METHOD PERFORMED BY ELECTRONIC DEVICE, ELECTRONIC DEVICE AND  STORAGE MEDIA</inventionTitleEng><openDate>2025.03.17</openDate><openNumber>10-2025-0037344</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2010.01.01)</ipcDate><ipcNumber>G01S 19/49</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/55</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 개시는 비디오의 프레임 이미지 및 상기 프레임 이미지에 대응하는 관성 측정 유닛의 데이터를 획득하고, 상기 프레임 이미지 및 상기 관성 측정 유닛의 데이터를 기반으로 상기 프레임 이미지에 대응하는 카메라 위치와 포즈(Position0026#Pose), 스파스 맵(sparse map) 및 고밀도 맵을 획득하는 전자 장치에 의해 수행되는 방법, 전자 장치 및 저장 매체에 관한 것이다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 전자 장치에 의해 수행되는 방법에 있어서,비디오의 프레임 이미지 및 상기 프레임 이미지에 대응하는 관성 측정 유닛의 데이터를 획득하는 단계; 및상기 프레임 이미지 및 상기 관성 측정 유닛의 데이터를 기반으로 상기 프레임 이미지에 대응하는 카메라 위치와 포즈(Position0026#Pose), 스파스 맵(sparse map) 및 고밀도 맵을 획득하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계는,상기 프레임 이미지가 키 프레임인 경우,상기 프레임 이미지 및 상기 관성 측정 유닛의 데이터를 기반으로 상기 프레임 이미지에 대응하는 카메라 위치와 포즈, 3차원 랜드마크 포인트(Three-Dimensional Landmark Point), 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 획득하는 단계;상기 프레임 이미지를 포함하는 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈, 3차원 랜드마크 포인트, 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 기반으로 전역 최적화를 수행하는 단계;최적화된 3차원 랜드마크 포인트로부터 상기 프레임 이미지에 대응하는 스파스 맵을 생성하는 단계; 및상기 전역 최적화의 결과, 상기 프레임 이미지에 대응하는 깊이 맵 및 상기 프레임 이미지에 기초하여 신경망을 통해 상기 고밀도 맵을 얻는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계는,상기 프레임 이미지가 비 키 프레임인 경우,상기 프레임 이미지 및 상기 프레임 이미지에 대응하는 상기 관성 측정 유닛의 데이터를 기반으로 상기 프레임 이미지의 상기 카메라 위치와 포즈를 획득하는 단계; 및상기 프레임 이미지 이전의 이전 키 프레임에 대응하는 상기 스파스 맵과 상기 고밀도 맵을 상기 프레임 이미지에 대응하는 상기 스파스 맵과 상기 고밀도 맵으로 결정하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서, 상기 프레임 이미지를 포함하는 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈, 상기 3차원 랜드마크 포인트, 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 기반으로 전역 최적화를 수행하는 단계는,상기 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈 및 상기 3차원 랜드마크 포인트를 기반으로 강건 커널(robust kernel) 함수 기반의 재투영 오차 함수를 구성하는 단계;상기 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈와 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 기반으로 상기 관성 측정 유닛의 오차 함수를 구성하는 단계; 및상기 강건 커널 함수 기반의 재투영 오차 함수 및 상기 관성 측정 유닛의 오차 함수로 구성된 전역 최적화 타겟 함수를 최소화하여, 상기 적어도 하나의 키 프레임에 대응하는 최적화된 카메라 위치와 포즈, 최적화된 3차원 랜드마크 포인트 및 상기 관성 측정 유닛의 최적화된 속도 및 상기 관성 측정 유닛의 최적회된 편차를 획득하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 강건 커널 함수는,후버(Huber) 커널 함수를 포함하는방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계는,상기 프레임 이미지가 비 키 프레임인 경우,상기 프레임 이미지에 대응하는 깊이 맵을 결정하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 프레임 이미지에 대응하는 상기 깊이 맵을 결정하는 단계는,상기 프레임 이미지 중 왼쪽 눈 이미지와 오른쪽 눈 이미지에 대해 스테레오 매칭을 수행하여 상기 프레임 이미지에 대응하는 양안 시차 맵을 획득하는 단계; 및상기 양안 시차 맵을 변환하여 상기 프레임 이미지에 대응하는 깊이 맵을 획득하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 프레임 이미지 중 상기 왼쪽 눈 이미지와 상기 오른쪽 눈 이미지에 대해 스테레오 매칭을 수행하여 상기 프레임 이미지에 대응하는 상기 양안 시차 맵을 획득하는 단계는,상기 프레임 이미지 이전의 이전 키 프레임에 대응하는 고밀도 맵과 상기 프레임 이미지의 상기 왼쪽 눈 이미지 및 상기 오른쪽 눈 이미지에 따라 스테레오 매칭을 수행하여 상기 양안 시차 맵을 얻는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>9. 제2항에 있어서, 상기 전역 최적화의 결과, 상기 프레임 이미지에 대응하는 깊이 맵 및 상기 프레임 이미지를 기반으로, 상기 신경망을 통해 상기 고밀도 맵을 획득하는 단계는,상기 전역 최적화의 결과, 상기 프레임 이미지에 대응하는 깊이 맵 및 상기 프레임 이미지를 기반으로 상기 신경망을 훈련하여, 양안 비디오에 대응하는 장면의 암시적 고밀도 맵 표현을 획득하는 단계; 및상기 획득한 암시적 고밀도 맵 표현에 상기 프레임 이미지에 대응하는 최적화된 카메라 위치와 포즈를 입력하여 상기 고밀도 맵 및 상기 고밀도 맵 내 각 3차원 랜드마크 포인트의 신뢰도를 획득하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>10. 제9항에 있어서, 상기 전역 최적화의 결과, 상기 프레임 이미지에 대응하는 깊이 맵 및 상기 프레임 이미지를 기반으로 상기 신경망을 훈련하여, 상기 양안 비디오에 대응하는 장면의 상기 암시적 고밀도 맵 표현을 획득하는 단계는,상기 프레임 이미지에 대응하는 최적화된 카메라 위치와 포즈를 기반으로, 렌더링된 컬러 이미지와 렌더링된 깊이 맵을 획득하는 단계;상기 렌더링된 컬러 이미지 및 상기 프레임 이미지의 컬러 이미지를 기반으로 제1 손실 함수를 결정하는 단계;상기 렌더링된 깊이 맵 및 상기 깊이 맵을 기반으로 제2 손실 함수를 결정하는 단계;상기 깊이 맵 및 상기 프레임 이미지의 컬러 이미지를 기반으로 제3 손실 함수를 결정하는 단계; 및제1 손실 함수, 제2 손실 함수 및 제3 손실 함수의 가중치 합을 기반으로 상기 신경망을 훈련하여 상기 장면의 암시적 고밀도 맵 표현을 획득하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>11. 제1항에 있어서,상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계는,상기 프레임 이미지가 키 프레임인 경우,상기 고밀도 맵과 상기 고밀도 맵 내의 각 3차원 랜드마크 포인트의 신뢰도를 기반으로 상기 스파스 맵을 업데이트하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 스파스 맵을 업데이트하는 단계는, 상기 스파스 맵 내 하나의 3차원 랜드마크 포인트에 대해, 상기 고밀도 맵 중 상기 하나의 3차원 랜드마크 포인트에 대응하는 3차원 랜드마크 포인트의 신뢰도에 따라, 상기 하나의 3차원 랜드마크 포인트의 제1 가중치 및 상기 고밀도 맵 중 상기 하나의 3차원 랜드마크 포인트에 대응하는 3차원 랜드마크 포인트의 제2 가중치를 결정하는 단계;결정된 상기 제1 가중치 및 상기 제2 가중치에 기초하여, 상기 하나의 3차원 랜드마크 포인트와 상기 고밀도 맵 중 상기 하나의 3차원 랜드마크 포인트에 대응하는 3차원 랜드마크 포인트를 융합하여, 상기 하나의 3차원 랜드마크 포인트를 업데이트하는 단계; 및상기 스파스 맵 내 각 3차원 랜드마크 포인트에 대해 상기 제1 가중치와 상기 제2 가중치를 결정하는 단계와 상기 하나의 3차원 랜드마크 포인트를 업데이트하는 단계를 수행하여 상기 스파스 맵을 업데이트하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>13. 컴퓨터 판독가능 기록매체에 있어서,명령들을 저장하고,상기 명령들은, 하나 이상의 프로세서에 의하여 실행되는 경우, 제1항 내지 제12항의 방법을 수행하도록 하는컴퓨터 판독가능 기록매체.</claim></claimInfo><claimInfo><claim>14. 전자 장치에 있어서,하나 이상의 프로세서, 및명령들을 저장하는 메모리를 포함하고,상기 명령들은, 상기 하나 이상의 프로세서에 의하여 실행되는 경우, 상기 장치로 하여금,비디오의 프레임 이미지 및 상기 프레임 이미지에 대응하는 관성 측정 유닛의 데이터를 획득하는 단계; 및상기 프레임 이미지 및 상기 관성 측정 유닛의 데이터를 기반으로 상기 프레임 이미지에 대응하는 카메라 위치와 포즈(Position0026#Pose), 스파스 맵(sparse map) 및 고밀도 맵을 획득하는 단계를 수행하도록 하는 전자 장치.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 명령들은, 상기 하나 이상의 프로세서에 의하여 실행되는 경우, 상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계에서,상기 프레임 이미지가 키 프레임인 경우,상기 프레임 이미지 및 상기 관성 측정 유닛의 데이터를 기반으로 상기 프레임 이미지에 대응하는 카메라 위치와 포즈, 3차원 랜드마크 포인트(Three-Dimensional Landmark Point), 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 획득하는 단계;상기 프레임 이미지를 포함하는 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈, 3차원 랜드마크 포인트, 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 기반으로 전역 최적화를 수행하는 단계;최적화된 3차원 랜드마크 포인트로부터 상기 프레임 이미지에 대응하는 스파스 맵을 생성하는 단계; 및상기 전역 최적화의 결과, 상기 프레임 이미지에 대응하는 깊이 맵 및 상기 프레임 이미지에 기초하여 신경망을 통해 상기 고밀도 맵을 얻는 단계를 수행하도록 하는 전자 장치.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서, 상기 명령들은, 상기 하나 이상의 프로세서에 의하여 실행되는 경우, 상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계에서,상기 프레임 이미지가 비 키 프레임인 경우,상기 프레임 이미지 및 상기 프레임 이미지에 대응하는 상기 관성 측정 유닛의 데이터를 기반으로 상기 프레임 이미지의 상기 카메라 위치와 포즈를 획득하는 단계; 및상기 프레임 이미지 이전의 이전 키 프레임에 대응하는 상기 스파스 맵과 상기 고밀도 맵을 상기 프레임 이미지에 대응하는 상기 스파스 맵과 상기 고밀도 맵으로 결정하는 단계를 수행하도록 하는 전자 장치.</claim></claimInfo><claimInfo><claim>17. 제15항에 있어서, 상기 명령들은, 상기 하나 이상의 프로세서에 의하여 실행되는 경우,상기 프레임 이미지를 포함하는 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈, 상기 3차원 랜드마크 포인트, 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 기반으로 전역 최적화를 수행하는 단계에서,상기 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈 및 상기 3차원 랜드마크 포인트를 기반으로 강건 커널(robust kernel) 함수 기반의 재투영 오차 함수를 구성하는 단계;상기 적어도 하나의 키 프레임에 대응하는 상기 카메라 위치와 포즈와 상기 관성 측정 유닛의 속도 및 상기 관성 측정 유닛의 편차를 기반으로 상기 관성 측정 유닛의 오차 함수를 구성하는 단계; 및상기 강건 커널 함수 기반의 재투영 오차 함수 및 상기 관성 측정 유닛의 오차 함수로 구성된 전역 최적화 타겟 함수를 최소화하여, 상기 적어도 하나의 키 프레임에 대응하는 최적화된 카메라 위치와 포즈, 최적화된 3차원 랜드마크 포인트 및 상기 관성 측정 유닛의 최적화된 속도 및 상기 관성 측정 유닛의 최적회된 편차를 획득하는 단계를 수행하도록 하는 전자 장치.</claim></claimInfo><claimInfo><claim>18. 제14항에 있어서, 상기 명령들은, 상기 하나 이상의 프로세서에 의하여 실행되는 경우, 상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계에서,상기 프레임 이미지가 비 키 프레임인 경우,상기 프레임 이미지 중 왼쪽 눈 이미지와 오른쪽 눈 이미지에 대해 스테레오 매칭을 수행하여 상기 프레임 이미지에 대응하는 양안 시차 맵을 획득하는 단계; 및상기 양안 시차 맵을 변환하여 상기 프레임 이미지에 대응하는 깊이 맵을 획득하는 단계를 수행하도록 하는 전자 장치.</claim></claimInfo><claimInfo><claim>19. 제15항에 있어서, 상기 명령들은, 상기 하나 이상의 프로세서에 의하여 실행되는 경우, 상기 전역 최적화의 결과, 상기 프레임 이미지에 대응하는 깊이 맵 및 상기 프레임 이미지를 기반으로, 상기 신경망을 통해 상기 고밀도 맵을 획득하는 단계는,상기 전역 최적화의 결과, 상기 프레임 이미지에 대응하는 깊이 맵 및 상기 프레임 이미지를 기반으로 상기 신경망을 훈련하여, 양안 비디오에 대응하는 장면의 암시적 고밀도 맵 표현을 획득하는 단계; 및상기 획득한 암시적 고밀도 맵 표현에 상기 프레임 이미지에 대응하는 최적화된 카메라 위치와 포즈를 입력하여 상기 고밀도 맵 및 상기 고밀도 맵 내 각 3차원 랜드마크 포인트의 신뢰도를 획득하는 단계를 수행하도록 하는 전자 장치.</claim></claimInfo><claimInfo><claim>20. 제14항에 있어서,상기 명령들은, 상기 하나 이상의 프로세서에 의하여 실행되는 경우, 상기 프레임 이미지에 대응하는 상기 카메라 위치와 포즈, 상기 스파스 맵 및 상기 고밀도 맵을 획득하는 단계는,상기 프레임 이미지가 키 프레임인 경우,상기 스파스 맵 내 하나의 3차원 랜드마크 포인트에 대해, 상기 고밀도 맵 중 상기 하나의 3차원 랜드마크 포인트에 대응하는 3차원 랜드마크 포인트의 신뢰도에 따라, 상기 하나의 3차원 랜드마크 포인트의 제1 가중치 및 상기 고밀도 맵 중 상기 하나의 3차원 랜드마크 포인트에 대응하는 3차원 랜드마크 포인트의 제2 가중치를 결정하는 단계;결정된 상기 제1 가중치 및 상기 제2 가중치에 기초하여, 상기 하나의 3차원 랜드마크 포인트와 상기 고밀도 맵 중 상기 하나의 3차원 랜드마크 포인트에 대응하는 3차원 랜드마크 포인트를 융합하여, 상기 하나의 3차원 랜드마크 포인트를 업데이트하는 단계; 및상기 스파스 맵 내 각 3차원 랜드마크 포인트에 대해 상기 제1 가중치와 상기 제2 가중치를 결정하는 단계와 상기 하나의 3차원 랜드마크 포인트를 업데이트하는 단계를 수행하여 상기 스파스 맵을 업데이트하는 단계를 수행하도록 하는 전자 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>중화인민공화국 ****** 베이징 차...</address><code> </code><country> </country><engName>Zhihua LIU</engName><name>지화 리우</name></inventorInfo><inventorInfo><address>중화인민공화국 ****** 베이징 차...</address><code> </code><country> </country><engName>Xiongfeng Peng</engName><name>시옹펑 펭</name></inventorInfo><inventorInfo><address>중화인민공화국 ****** 베이징 차...</address><code> </code><country> </country><engName>Zhitong Ye</engName><name>지통 예</name></inventorInfo><inventorInfo><address>중화인민공화국 ****** 베이징 차...</address><code> </code><country> </country><engName>Qiang Wang</engName><name>창 왕</name></inventorInfo><inventorInfo><address>경기도 화성...</address><code>420220324846</code><country>대한민국</country><engName>SOONYONG CHO</engName><name>조순용</name></inventorInfo><inventorInfo><address>경기도 화성시 여울로...</address><code>420170480468</code><country>대한민국</country><engName>Sung, Younghun</engName><name>성영훈</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>중국</priorityApplicationCountry><priorityApplicationDate>2023.09.08</priorityApplicationDate><priorityApplicationNumber>202311160890.8</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2024.07.16</receiptDate><receiptNumber>1-1-2024-0771341-58</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2024.07.22</receiptDate><receiptNumber>9-1-2024-9007943-66</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020240093793.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c930b050ef8c0efa3527794666b3085169bf93b3300d90aad46457331323feb702b9c3b683fb1f617e54b4b53295f332710037110fd824dda7a</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf1109d49fde13e1805fe037cc68de62165d3363f2c4c0f6fde315c52427cb97e7467e973d62327aa6ccb1a5cafecdba87273d490ec2245060</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>