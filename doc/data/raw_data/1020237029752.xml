<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:54:32.5432</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2021.04.01</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7029752</applicationNumber><claimCount>30</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>신경망 기반 픽처 프로세싱에서의 보조 정보의 독립적 위치결정</inventionTitle><inventionTitleEng>INDEPENDENT POSITIONING OF AUXILIARY INFORMATION IN NEURAL NETWORK BASED PICTURE PROCESSING</inventionTitleEng><openDate>2023.10.17</openDate><openNumber>10-2023-0145096</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.08.31</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.08.31</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/513</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/13</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/105</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/184</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/46</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/139</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/132</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/147</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/513</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/91</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/04</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/77</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/40</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 이 출원은 2개 이상의 계층을 갖는 신경망을 이용하여 픽처 데이터 또는 픽처 특징 데이터의 프로세싱을 위한 방법 및 장치를 제공한다. 본 개시내용은 인공 지능(AI)-기반 비디오 또는 픽처 압축 기술의 분야에서, 그리고 특히, 신경망-기반 비디오 압축 기술의 분야에 적용될 수 있다. 일부 실시예에 따르면, 2개의 종류의 데이터는 신경망에 의한 프로세싱을 포함하는 프로세싱 동안에 조합된다. 2개의 종류의 데이터는 네트워크에 의한 프로세싱의 상이한 스테이지로부터 획득된다. 장점의 일부는 더 양호한 인코딩/디코딩 성능을 추가로 초래할 수 있는 신경망 아키텍처의 더 큰 확장가능성 및 더 유연한 설계를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.10.06</internationOpenDate><internationOpenNumber>WO2022211658</internationOpenNumber><internationalApplicationDate>2021.04.01</internationalApplicationDate><internationalApplicationNumber>PCT/RU2021/000137</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 제1 스테이지(i+r) 및 제2 스테이지(i)를 포함하는 2개 이상의 스테이지(K 내지 0)를 포함하는 신경망(1786)을 이용하여 비트스트림(1772)으로부터의 하나 이상의 픽처의 특징 데이터를 프로세싱하기 위한 방법으로서,상기 비트스트림(1772)에 기초하여 제1 데이터(1760)를 획득하는 단계,상기 신경망(1786)을 이용하여 상기 제1 데이터(1760)를 프로세싱하는 단계 - 상기 프로세싱하는 단계는, 상기 신경망의 상기 제2 스테이지로부터, 상기 신경망(1786)에 의해 이전에 프로세싱된 데이터에 기초하는 제2 데이터(1765)를 획득하는 것, 상기 신경망의 상기 제1 스테이지에 대한 입력을 생성하기 위하여 상기 제1 데이터(1760)를 상기 제2 데이터(1765)와 함께 이용하는 것을 포함하고, 상기 제1 스테이지(i+r)는 신경망(1765)의 특징 데이터 프로세싱에서 상기 제2 스테이지(i)에 선행함 -;상기 프로세싱의 결과(1702)를 출력하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 제1 데이터(1760)는 상기 신경망(1786)의 상기 특징 데이터 프로세싱에서 상기 제1 스테이지 및 상기 제2 스테이지에 선행하거나 상기 제1 스테이지와 동일한 상기 신경망(1786)의 제3 스테이지에서 획득되는, 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 제1 데이터(1760)는 예측 오차를 나타내고, 상기 제2 데이터(1765)는 예측을 나타내는, 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 예측은,- 상기 제2 스테이지에서 상기 신경망(1786)에 의해 출력된 특징 데이터인 참조 특징 데이터를 획득하는 것,- 비트스트림(1790)에 기초하여, 상기 참조 특징 데이터에 관련된 모션 정보 또는 공간적 정보를 포함하는 예측 정보를 획득하는 것,- 상기 참조 특징 데이터 및 상기 예측 정보에 기초하여 상기 예측을 생성하는 것에 의해 획득되는, 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 예측 오차는 상기 신경망(1786)으로 현재 픽처를 프로세싱함으로써 획득되고,상기 예측 정보는 모션 정보이고, 상기 참조 특징 데이터는 디코딩 순서에서 상기 현재 픽처에 선행하는 픽처를 나타내는 픽처 데이터의 상기 신경망(1786) 프로세싱에 의해 생성되는, 방법.</claim></claimInfo><claimInfo><claim>6. 제3항 내지 제5항 중 어느 한 항에 있어서,상기 제1 데이터를 상기 제2 데이터(1765)와 함께 이용하는 것은, 상기 예측 또는 리스케일링된(re-scaled) 예측과, 상기 예측 오차 또는 리스케일링된 예측 오차와의 엘리먼트별(element-wise) 가산을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제5항 또는 제6항에 있어서,상기 모션 정보는 모션 벡터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 제1항 내지 제7항 중 어느 한 항에 있어서,상기 제2 스테이지는 상기 신경망(1786)의 출력인, 방법.</claim></claimInfo><claimInfo><claim>9. 제2항 내지 제8항 중 어느 한 항에 있어서,상기 제1 스테이지는 상기 신경망(1786)의 입력인, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항 또는 제2항에 있어서,상기 제1 스테이지는 상기 신경망(1786)의 입력이고,상기 제1 데이터는 엔트로피 인코딩된 데이터이고,상기 제2 스테이지는 상기 신경망(1786)의 출력 스테이지와는 상이한 스테이지이고,상기 제2 데이터는 상기 제2 스테이지의 상기 특징 데이터에 관련된 확률 모델 데이터인, 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 제2 데이터는 상기 제1 데이터의 엔트로피 디코딩(entropy decoding)을 위한 확률 모델 데이터인, 방법.</claim></claimInfo><claimInfo><claim>12. 제1항 내지 제11항 중 어느 한 항에 있어서,상기 제1 스테이지 및/또는 상기 제2 스테이지의 위치는 상기 신경망(1786) 내에서 구성가능하고,상기 방법은 하나 이상의 픽처 코딩 파라미터에 기초하여 수집 조건에 따라 상기 제1 스테이지 및/또는 상기 제2 스테이지의 위치를 구성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>13. 제1항 내지 제12항 중 어느 한 항에 있어서,상기 방법은, 상기 비트스트림(1772)으로부터, 상기 제1 스테이지 및/또는 상기 제2 스테이지를 특정하는 스테이지 선택 지시자를 파싱하는 단계를 더 포함하고,상기 신경망(1786) 내에서의 상기 제1 스테이지 및 상기 제2 스테이지의 위치는 파싱된 스테이지 선택 지시자에 따라 구성되는, 방법.</claim></claimInfo><claimInfo><claim>14. 비트스트림(1772)을 생성하기 위하여, 제1 스테이지 및 제2 스테이지를 포함하는 2개 이상의 스테이지를 포함하는 신경망(1706)을 이용하여 적어도 하나의 픽처를 프로세싱하기 위한 방법으로서,상기 신경망(1706)으로 상기 적어도 하나의 픽처를 프로세싱하는 단계 - 상기 프로세싱하는 단계는, 상기 적어도 하나의 픽처에 기초하여 제1 데이터(1760)를 획득하는 것, 상기 프로세싱의 상기 제2 스테이지에서 제2 데이터(1765)를 획득하되, 상기 제2 데이터(1765)는 상기 신경망(1706)에 의해 이전에 프로세싱된 데이터에 기초하는 것, 상기 신경망(1706)의 상기 제1 스테이지에서의 입력을 생성하기 위하여 상기 제1 데이터(1760)를 상기 제2 데이터(1765)와 함께 이용하는 것을 포함하고, 상기 제1 스테이지는 상기 신경망(1706)의 특징 데이터 프로세싱에서 상기 제2 스테이지에 선행함 -;상기 프로세싱에 의해 획득된 특징 데이터를 상기 비트스트림(1772) 내로 포함하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 제1 데이터(1760)는 상기 신경망(1706)의 상기 특징 데이터 프로세싱에서 상기 제1 스테이지 및 상기 제2 스테이지에 선행하거나 상기 제1 스테이지와 동일한 상기 신경망(1706)의 제3 스테이지에서 획득되는, 방법.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 제1 데이터(1760)는 예측 오차를 나타내고, 상기 제2 데이터(1765)는 예측을 나타내는, 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 예측은,- 상기 제2 스테이지에서 상기 신경망(1706)에 의해 출력된 특징 데이터인 참조 특징 데이터를 획득하는 것,- 상기 적어도 하나의 픽처에 기초하여, 상기 참조 특징 데이터에 관련된 모션 정보 또는 공간적 정보를 포함하는 예측 정보를 획득하는 것,- 상기 참조 특징 데이터 및 상기 예측 정보에 기초하여 상기 예측을 생성하는 것, 및- 획득된 예측 정보를 비트스트림(1790) 내로 삽입하는 것에 의해 획득되는, 방법.</claim></claimInfo><claimInfo><claim>18. 제17항에 있어서,상기 예측 오차는 상기 신경망(1706)으로 현재 픽처를 프로세싱함으로써 획득되고,상기 예측 정보는 모션 정보이고, 상기 참조 특징 데이터는 디코딩 순서에서 상기 현재 픽처에 선행하는 픽처를 나타내는 픽처 데이터의 상기 신경망(1706) 프로세싱에 의해 생성되는, 방법.</claim></claimInfo><claimInfo><claim>19. 제15항 내지 제18항 중 어느 한 항에 있어서,상기 제1 데이터(1760)를 상기 제2 데이터(1765)와 함께 이용하는 것은, 상기 제1 데이터(1760) 또는 리스케일링된 제1 데이터(1760)로부터 상기 예측 또는 리스케일링된 예측의 엘리먼트별 감산을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>20. 제18항 또는 제19항에 있어서,상기 모션 정보는 모션 벡터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>21. 제14항 내지 제20항 중 어느 한 항에 있어서,상기 제2 스테이지는 재구성된 픽처 데이터를 나타내는 디코딩 신경망(1706)의 출력인, 방법.</claim></claimInfo><claimInfo><claim>22. 제15항 내지 제21항 중 어느 한 항에 있어서,상기 제1 스테이지는 상기 신경망(1706)의 출력인, 방법.</claim></claimInfo><claimInfo><claim>23. 제14항 또는 제15항에 있어서,상기 제1 스테이지는 상기 신경망(1706)의 출력이고,상기 제1 데이터는 엔트로피 인코딩되어야 할 프로세싱된 데이터이고,상기 제2 스테이지는 상기 신경망(1706)의 입력 스테이지와는 상이한 스테이지이고,상기 제2 데이터는 상기 제2 스테이지의 상기 특징 데이터에 관련된 확률 모델 데이터인, 방법.</claim></claimInfo><claimInfo><claim>24. 제23항에 있어서,상기 제2 데이터는 상기 제1 데이터의 엔트로피 인코딩(entropy encoding)을 위한 확률 모델 데이터인, 방법.</claim></claimInfo><claimInfo><claim>25. 제1항 내지 제24항 중 어느 한 항에 있어서,상기 제1 스테이지 및/또는 상기 제2 스테이지의 위치는 상기 신경망(1706) 내에서 구성가능하고,상기 방법은 하나 이상의 픽처 코딩 파라미터에 기초하여 수집 조건에 따라 상기 제1 스테이지 및/또는 상기 제2 스테이지의 위치를 구성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>26. 제1항 내지 제25항 중 어느 한 항에 있어서,상기 방법은, 상기 제1 스테이지 및/또는 상기 제2 스테이지를 특정하는 스테이지 선택 지시자를 결정하고 상기 스테이지 선택 지시자를 상기 비트스트림(1772) 내로 포함하는 단계를 더 포함하고,상기 신경망(1706) 내에서의 상기 제1 스테이지 및 상기 제2 스테이지의 위치는 결정된 스테이지 선택 지시자에 따라 구성되는, 방법.</claim></claimInfo><claimInfo><claim>27. 제26항에 있어서,상기 스테이지 선택 지시자의 결정은 레이트(rate), 왜곡(distortion), 레이턴시(latency), 정확도(accuracy), 및 복잡도(complexity) 중의 하나 이상을 포함하는 비용 함수(cost function)에 따라 수행된 최적화 절차에 기초하는, 방법.</claim></claimInfo><claimInfo><claim>28. 컴퓨터-판독가능 비-일시적 매체(540) 상에 저장되고 코드 명령을 포함하는 컴퓨터 프로그램(510)으로서,상기 코드 명령은, 하나 이상의 프로세서 상에서 실행될 때, 상기 하나 이상의 프로세서로 하여금, 제1항 내지 제27항 중 어느 한 항에 따른 상기 방법을 수행하게 하는, 컴퓨터 프로그램(510).</claim></claimInfo><claimInfo><claim>29. 제1 스테이지 및 제2 스테이지를 포함하는 2개 이상의 스테이지를 포함하는 신경망(1786)을 이용하여 비트스트림(1772)으로부터의 하나 이상의 픽처의 특징 데이터를 프로세싱하기 위한 장치로서,프로세싱 회로부를 포함하고, 상기 프로세싱 회로부는,상기 비트스트림(1772)에 기초하여 제1 데이터(1760)를 획득하고,상기 신경망(1786)을 이용하여 상기 제1 데이터(1760)를 프로세싱하고 - 상기 프로세싱하는 것은, 상기 신경망(1786)의 상기 제2 스테이지로부터, 상기 신경망(1786)에 의해 이전에 프로세싱된 데이터에 기초하는 제2 데이터(1765)를 획득하는 것, 상기 신경망(1786)의 상기 제1 스테이지에 대한 입력을 생성하기 위하여 상기 제1 데이터(1760)를 상기 제2 데이터(1765)와 함께 이용하는 것을 포함하고, 상기 제1 스테이지는 상기 신경망(1786)의 특징 데이터 프로세싱에서 상기 제2 스테이지에 선행함 -;상기 프로세싱의 결과를 출력하도록구성되는, 장치.</claim></claimInfo><claimInfo><claim>30. 비트스트림(1772)을 생성하기 위하여, 제1 스테이지 및 제2 스테이지를 포함하는 2개 이상의 스테이지를 포함하는 신경망(1706)을 이용하여 적어도 하나의 픽처를 프로세싱하기 위한 장치로서,프로세싱 회로부를 포함하고, 상기 프로세싱 회로부는,상기 신경망(1706)으로 상기 적어도 하나의 픽처를 프로세싱하고 - 상기 프로세싱하는 것은, 상기 적어도 하나의 픽처에 기초하여 제1 데이터(1760)를 획득하는 것, 상기 프로세싱의 상기 제2 스테이지에서 제2 데이터(1765)를 획득하되, 상기 제2 데이터(1765)는 상기 신경망(1706)에 의해 이전에 프로세싱된 데이터에 기초하는 것, 상기 신경망(1706)의 상기 제1 스테이지에서의 입력을 생성하기 위하여 상기 제1 데이터(1760)를 상기 제2 데이터(1765)와 함께 이용하는 것을 포함하고, 상기 제1 스테이지는 상기 신경망(1706)의 특징 데이터 프로세싱에서 상기 제2 스테이지에 선행함 -,상기 프로세싱에 의해 획득된 특징 데이터를 상기 비트스트림(1772) 내로 포함하도록구성되는, 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>중국 ****** 광동성 셴젠 롱강 디스트릭트 반티안 후아웨이 어드미니스트레이션 빌딩</address><code>520000572466</code><country>중국</country><engName>HUAWEI TECHNOLOGIES CO., LTD.</engName><name>후아웨이 테크놀러지 컴퍼니 리미티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>중국 ******...</address><code> </code><country> </country><engName>SOLOVYEV, Timofey Mikhailovich</engName><name>솔로비예프 티모페이 미카일로비치</name></inventorInfo><inventorInfo><address>독일 ***** 뮌헨 리스트라쎄...</address><code> </code><country> </country><engName>ALSHINA, Elena, Alexandrovna</engName><name>알시나 엘레나 알렉산드로브나</name></inventorInfo><inventorInfo><address>독일 ***** 뮌헨 리스스트라...</address><code> </code><country> </country><engName>WANG, Biao</engName><name>왕 뱌오</name></inventorInfo><inventorInfo><address>중국 ******...</address><code> </code><country> </country><engName>KARABUTOV, Alexander Alexandrovich</engName><name>카라부토브 알렉산더 알렉산드로비치</name></inventorInfo><inventorInfo><address>중국 ******...</address><code> </code><country> </country><engName>SOSULNIKOV, Mikhail Vyacheslavovich</engName><name>소술니코프 미하일 뱌체슬라보비치</name></inventorInfo><inventorInfo><address>중국 ******...</address><code> </code><country> </country><engName>GAIKOV, Georgy Petrovich</engName><name>가이코프 게오르기 페트로비치</name></inventorInfo><inventorInfo><address>독일 ***** 뮌헨 리스스트라...</address><code> </code><country> </country><engName>GAO, Han</engName><name>가오 한</name></inventorInfo><inventorInfo><address>독일 ***** 뮌헨 리스스트라...</address><code> </code><country> </country><engName>JIA, Panqi</engName><name>지아 판키</name></inventorInfo><inventorInfo><address>독일 ***** 뮌헨 리스스트라...</address><code> </code><country> </country><engName>KOYUNCU, Esin</engName><name>코윤쿠 에신</name></inventorInfo><inventorInfo><address>중국 ******...</address><code> </code><country> </country><engName>IKONIN, Sergey Yurievich</engName><name>아이코닌 세르게이 유리에비치</name></inventorInfo><inventorInfo><address>독일 ***** 뮌헨 리스스트라...</address><code> </code><country> </country><engName>ESENLIK, Semih</engName><name>에센리크 세미흐</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 테헤란로 ***, 서림빌딩 **층 (역삼동)</address><code>920011000036</code><country>대한민국</country><engName>YOU ME PATENT &amp; LAW FIRM</engName><name>유미특허법인</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.08.31</receiptDate><receiptNumber>1-1-2023-0963096-24</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Request for Amendment</documentEngName><documentName>보정요구서</documentName><receiptDate>2023.09.06</receiptDate><receiptNumber>1-5-2023-0142651-79</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Amendment to Patent Application, etc.] Amendment</documentEngName><documentName>[출원서 등 보정]보정서</documentName><receiptDate>2023.09.12</receiptDate><receiptNumber>1-1-2023-1009050-48</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.09.14</receiptDate><receiptNumber>1-5-2023-0147311-22</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Request for Prior Art Search</documentEngName><documentName>선행기술조사의뢰서</documentName><receiptDate>2025.08.06</receiptDate><receiptNumber>9-1-9999-9999999-89</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Report of Prior Art Search</documentEngName><documentName>선행기술조사보고서</documentName><receiptDate>2025.09.23</receiptDate><receiptNumber>9-6-2025-0179434-24</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for refusal</documentEngName><documentName>의견제출통지서</documentName><receiptDate>2025.09.25</receiptDate><receiptNumber>9-5-2025-0932837-50</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237029752.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93537cbed247757550d662a08143a76a487b844d73ab4861b4beb05068fded37cd85d17a5b6106f0efc804caa45c1f163134707842601ef47e</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf23e458609d65af87b54b227f0971878ef435f8138337d796035d4f8f10a306995e2685091ce101f80da9782be17ff7eec840693ff3039d4c</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>