<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:05:14.514</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2024.09.03</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-0119364</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>장면 흐름 추정 장치 및 방법</inventionTitle><inventionTitleEng>SCENE FLOW ESTIMATION APPARATUS AND METHOD</inventionTitleEng><openDate>2025.05.22</openDate><openNumber>10-2025-0071822</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/269</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0442</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 일 실시 예에 따른 전자 장치는 AI 모델에 기초하여 프레임 쌍에 대응하는 장면 흐름을 추정할 수 있다. 더 나아가, 전자 장치는 장면 흐름을 추정하기 위한 AI 모델을 트레이닝 시킬 수 있다. 구체적으로, 일 실시예에 따른 프로세서에 의해 수행되는 장면 흐름(scene flow) 추정 방법은 프레임 쌍을 인공지능(artificial intelligence; AI) 네트워크에 입력하여 프레임 쌍에 포함된 픽셀들 각각에 대응하는 모션 임베딩 특징(motion embedding feature) 및 비차폐 카테고리 라벨 임베딩 특징(non-occluded category label embedding feature)을 획득하는 단계; 및 모션 임베딩 특징 및 비차폐 카테고리 라벨 임베딩 특징에 기초하여 프레임 쌍에 대응하는 장면 흐름을 추정하는 단계를 포함할 수 있다. 이 때, 비차폐 카테고리 라벨 임베딩 특징은 프레임 쌍 중 픽셀 쌍에 대응하는 객체의 카테고리 정보를 포함하고, 픽셀 쌍은 프레임 쌍 중 하나의 이미지에 포함된 픽셀 및 다른 이미지에 포함된 픽셀들 중 상기 하나의 이미지에 포함된 픽셀에 대응하는 픽셀로 구성될 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 프로세서에 의해 수행되는 장면 흐름(scene flow) 추정 방법에 있어서,프레임 쌍을 인공지능(artificial intelligence; AI) 네트워크에 입력하여 상기 프레임 쌍에 포함된 픽셀들 중 대상 픽셀에 대응하는 대응하는 모션 임베딩 특징(motion embedding feature) 및 비차폐 카테고리 라벨 임베딩 특징(non-occluded category label embedding feature)을 획득하는 단계; 및 상기 모션 임베딩 특징 및 상기 비차폐 카테고리 라벨 임베딩 특징에 기초하여 상기 프레임 쌍에 대응하는 장면 흐름을 추정하는 단계를 포함하고,상기 프레임 쌍은 제1 프레임 데이터 및 제2 프레임 데이터를 포함하며,상기 제1 프레임 데이터 및 상기 제2 프레임 데이터는 각각 컬러 이미지 및 상기 컬러 이미지에 대응하는 깊이 이미지(depth image)를 포함하고,상기 비차폐 카테고리 라벨 임베딩 특징은 상기 프레임 쌍 중 픽셀 쌍에 대응하는 객체의 카테고리 정보를 포함하고, 상기 픽셀 쌍은 상기 제1 프레임 데이터 중 어느 하나의 픽셀 및 상기 제2 프레임 데이터에 포함된 픽셀들 중 상기 어느 하나의 픽셀에 대응하는 픽셀로 구성된,장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 프레임 쌍에 대응하는 장면 흐름을 추정하는 단계는,상기 프레임 쌍에 대응하는 모션 필드(motion field)를 획득하는 단계;상기 모션 필드를 상기 AI 네트워크에 입력하여 상기 프레임 쌍에 대응하는 모션 임베딩 특징 및 비차폐 카테고리 라벨 임베딩 특징을 획득하는 단계;상기 모션 임베딩 특징 및 상기 비차폐 카테고리 라벨 임베딩 특징을 융합한 혼합 임베딩 특징을 획득하는 단계;상기 프레임 쌍 중 상기 픽셀 쌍 사이의 재투영 오차가 감소하도록, 상기 혼합 임베딩 특징에 기초하여 상기 모션 필드를 업데이트함으로써 타겟 모션 필드(target motion filed)를 획득하는 단계; 및상기 타겟 모션 필드에 기초하여, 상기 프레임 쌍에 대응하는 장면 흐름을 추정하는 단계를 포함하는 장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 타겟 모션 필드를 획득하는 단계는,상기 제2 프레임 데이터에서, 상기 제2 프레임 데이터에 포함된 픽셀들 중 상기 제1 프레임 데이터에 포함된 대상 픽셀에 대응하는 픽셀을 포함하는 이웃 포인트 세트를 결정하는 단계;상기 대상 픽셀에 대응하는 혼합 임베딩 특징 및 상기 결정된 이웃 포인트 세트에 포함된 픽셀들의 혼합 임베딩 특징들에 기초하여, 상기 대상 픽셀 및 상기 이웃 포인트 세트에 포함된 픽셀들 사이의 매칭 정도를 결정하는 단계; 및상기 매칭 정도에 따라 상기 대상 픽셀과 상기 이웃 포인트 세트에 포함된 픽셀들 사이의 재투영 오차가 감소하도록, 상기 모션 필드를 업데이트함으로써, 상기 타겟 모션 필드를 획득하는 단계를 포함하는 장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서,상기 프레임 쌍에 대응하는 장면 흐름을 추정하는 단계는,상기 제1 프레임 데이터에 포함된 컬러 이미지의 픽셀들 사이의 유사성에 기초하여, 상기 모션 필드를 조정하는 가중치를 획득하는 단계; 및상기 가중치를 상기 모션 필드에 적용함으로써 획득한 상기 타겟 모션 필드에 기초하여, 상기 프레임 쌍에 대응하는 장면 흐름을 추정하는 단계를 포함하는 장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서, 상기 가중치를 획득하는 단계는,상기 AI 네트워크의 어텐션 인코더에 상기 제1 프레임 데이터에 포함된 컬러 이미지를 입력하여, 상기 제1 프레임 데이터에 포함된 컬러 이미지의 픽셀들 사이의 제1 상관성(correlation)을 획득하는 단계;상기 제1 상관성에 기초하여 상기 제1 프레임 데이터에 포함된 컬러 이미지에 대응하는 제1 가중치를 결정하는 단계를 포함하는 장면 흐름 추정 방법. </claim></claimInfo><claimInfo><claim>6. 제5항에 있어서, 상기 가중치를 획득하는 단계는, 상기 어텐션 인코더에 상기 제2 프레임 데이터에 포함된 컬러 이미지를 입력하여, 상기 제2 프레임 데이터에 포함된 컬러 이미지의 픽셀들 사이의 제2 상관성을 획득하는 단계;상기 제2 상관성에 기초하여 상기 제2 프레임 데이터에 포함된 컬러 이미지에 대응하는 제2 가중치를 결정하는 단계; 및상기 제1 가중치 및 상기 제2 가중치를 융합하여 상기 모션 필드를 조정하는 융합 가중치를 획득하는 단계를 포함하는 장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>7. 제2항에 있어서,상기 모션 필드를 상기 AI 네트워크에 입력하여 상기 프레임 쌍에 대응하는 모션 임베딩 특징 및 비차폐 카테고리 라벨 임베딩 특징을 획득하는 단계는,상기 AI 네트워크에 포함된 특징 인코더에 기초하여, 상기 제1 프레임 데이터로부터 제1 프레임 데이터 특징을 추출하고, 상기 제2 프레임 데이터로부터 제2 프레임 데이터 특징을 추출하는 단계;상기 제1 프레임 데이터 특징 및 상기 제2 프레임 데이터 특징 사이의 상관성에 기초하여, 상기 프레임 쌍에 대응하는 상관 볼륨(correlation volume)을 생성하는 단계;상기 AI 네트워크에 포함된 컨텍스트 인코더에 기초하여, 상기 제1 프레임 데이터로부터, 상기 제1 프레임 데이터에 대응하는 컨텍스트 특징 및 히든 상태를 추출하는 단계; 및상기 컨텍스트 특징, 히든 상태, 상기 모션 필드 및 상기 상관 볼륨에 기초하여, 상기 AI 네트워크에 포함된 컨볼루션 게이트 순환 유닛 기반의 업데이트 네트워크로 상기 프레임 쌍에 대응하는 상기 모션 임베딩 특징 및 상기 비차폐 카테고리 라벨 임베딩 특징을 획득하는 단계를 포함하는 장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,훈련할 AI 네트워크 및 훈련 세트를 획득하는 단계; 및상기 훈련 세트에 기초하여, 훈련할 AI 네트워크에 대해 미리 결정된 훈련 종료 조건이 충족될 때까지 훈련 동작을 반복 수행함으로써, 훈련된 AI 네트워크를 획득하는 단계를 더 포함하는,장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서,상기 훈련 세트는 샘플 프레임 쌍, 상기 샘플 프레임 쌍에 대응하는 장면 흐름 참값 및 비차폐 카테고리 라벨 마스크 참값을 포함하며, 상기 샘플 프레임 쌍에 포함된 각각의 샘플 프레임 데이터는 샘플 컬러 이미지 및 상기 샘플 컬러 이미지에 대응하는 샘플 깊이 이미지를 포함하고, 상기 비차폐 카테고리 라벨 마스크 참값은 샘플 프레임 쌍 중 샘플 픽셀 쌍에 대응하는 카테고리 정보를 포함하며상기 샘플 픽셀 쌍은 상기 샘플 프레임 쌍 중 제1 샘플 프레임 데이터에 포함된 어느 하나의 픽셀 및 상기 픽셀이 상기 샘플 프레임 쌍 중 제2 샘플 프레임 데이터에 포함된 픽셀들 중 상기 제1 샘플 프레임 데이터에 포함된 어느 하나의 픽셀에 대응하는 픽셀로 구성되는장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>10. 제9항에 있어서,상기 훈련 동작을 반복 수행함으로써, 상기 훈련된 AI 네트워크를 획득하는 단계는,상기 샘플 프레임 쌍에 상기 훈련할 AI 네트워크를 적용함으로써, 상기 샘플 프레임 쌍에 대응하는 샘플 모션 임베딩 특징 및 샘플 비차폐 카테고리 라벨 임베딩 특징을 획득하는 단계;상기 샘플 모션 임베딩 특징 및 상기 샘플 비차폐 카테고리 라벨 임베딩 특징에 기초하여, 상기 샘플 프레임 쌍에 대응하는 장면 흐름 예측 값을 획득하는 단계;상기 샘플 비차폐 카테고리 라벨 임베딩 특징에 기초하여 상기 샘플 프레임 쌍에 대응하는 비차폐 카테고리 라벨 마스크 예측 값을 획득하는 단계; 상기 샘플 프레임 쌍에 대응하는 미리 결정된 장면 흐름 참값 및 상기 장면 흐름 예측 값에 기초하여, 제1 훈련 손실을 결정하는 단계;상기 샘플 프레임 쌍에 대응하는 미리 결정된 비차폐 카테고리 라벨 마스크 참값 및 상기 비차폐 카테고리 라벨 마스크 예측 값에 기초하여, 제2 훈련 손실을 결정하는 단계;상기 제1 훈련 손실 및 상기 제2 훈련 손실에 기초하여, 결합 훈련 손실을 결정하는 단계; 및상기 결합 훈련 손실에 기초하여, 상기 훈련할 AI 네트워크의 모델 파라미터를 조정하는 단계를 포함하는 장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서, 상기 샘플 비차폐 카테고리 라벨 임베딩 특징에 기초하여, 상기 샘플 프레임 쌍에 대응하는 상기 비차폐 카테고리 라벨 마스크 예측 값을 획득하는 단계는,상기 비차폐 카테고리 라벨 마스크 참값 및 상기 샘플 비차폐 카테고리 라벨 임베딩 특징에 기초하여, 상기 샘플 프레임 쌍에 포함된 카테고리에 대응하는 픽셀들의 평균 비차폐 카테고리 라벨 특징을 결정하는 단계; 및상기 제1 샘플 프레임 데이터에 포함된 픽셀에 대응하는 샘플 비차폐 카테고리 라벨 임베딩 특징 및 상기 제1 샘플 프레임 데이터에 포함된 픽셀이 속하는 카테고리에 대응하는 픽셀들의 상기 평균 비차폐 카테고리 라벨 특징 사이의 차이에 기초하여, 상기 제1 샘플 프레임 데이터에 포함된 픽셀이 상기 카테고리에 대응하는 상기 비차폐 카테고리 라벨 마스크 예측 값을 획득하는 단계를 포함하는 장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>12. 제9항에 있어서, 상기 샘플 프레임 쌍에 대응하는 상기 비차폐 카테고리 라벨 마스크 참값은,상기 샘플 프레임 쌍에 포함된 객체 인스턴스들에 대한 객체 인스턴스 분할 결과를 획득하고;상기 샘플 프레임 쌍에 대응하는 상기 장면 흐름 참값에 기초하여, 상기 샘플 프레임 쌍 중 매치된 픽셀 쌍 사이의 제1 광도 오차를 결정하며;상기 제1 광도 오차 및 상기 객체 인스턴스 분할 결과에 기초하여, 상기 샘플 프레임 쌍에 포함된 객체 인스턴스들 각각에 대응하는 비차폐 카테고리 라벨 마스크 참값을 결정하고;상기 샘플 프레임 쌍에 포함된 객체 인스턴스들 각각에 대응하는 비차폐 카테고리 라벨 마스크 참값에 기초하여, 상기 샘플 프레임 쌍에 대응하는 비차폐 카테고리 라벨 마스크 참값을 획득하는것에 기초하여 결정되는장면 흐름 추정 방법. </claim></claimInfo><claimInfo><claim>13. 제12항에 있어서, 상기 샘플 프레임 쌍에 대응하는 상기 비차폐 카테고리 라벨 마스크 참값은,상기 샘플 프레임 쌍에 대응하는 모션 필드 참값을 획득하고,상기 모션 필드 참값에 기초하여, 상기 샘플 프레임 쌍 중 매치된 픽셀 쌍 사이의 제2 광도 오차 및 깊이 오차를 결정하며,상기 제2 광도 오차 및 상기 깊이 오차에 기초하여, 상기 샘플 프레임 쌍 중 비차폐 배경 영역에 대응하는 비차폐 카테고리 라벨 마스크 참값을 결정하고,상기 비차폐 배경 영역에 대응하는 비차폐 카테고리 라벨 마스크 참값 및 상기 샘플 프레임 쌍에 포함된 객체 인스턴스들 각각에 대응하는 비차폐 카테고리 라벨 마스크 참값을 융합함으로써 상기 샘플 프레임 쌍의 비차폐 카테고리 라벨 마스크 참값을 획득하는 것에 기초하여 결정되는장면 흐름 추정 방법.</claim></claimInfo><claimInfo><claim>14. 제1항 내지 제13항 중 어느 한 항의 방법을 수행하기 위한 명령어를 포함하는 하나 이상의 컴퓨터 프로그램을 저장한 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>15. 장면 흐름(scene flow) 추정 장치에 있어서,프레임 쌍을 인공지능(artificial intelligence; AI) 네트워크에 입력하여 상기 프레임 쌍에 대응하는 모션 임베딩 특징(motion embedding feature) 및 비차폐 카테고리 라벨 임베딩 특징(non-occluded category label embedding feature)을 획득하고, 상기 모션 임베딩 특징 및 상기 비차폐 카테고리 라벨 임베딩 특징에 기초하여 상기 프레임 쌍에 대응하는 장면 흐름을 추정하는 프로세서를 포함하고,상기 프레임 쌍은 제1 프레임 데이터 및 제2 프레임 데이터를 포함하며,상기 제1 프레임 데이터 및 상기 제2 프레임 데이터는 각각 컬러 이미지 및 상기 컬러 이미지에 대응하는 깊이 이미지(depth image)를 포함하고,상기 비차폐 카테고리 라벨 임베딩 특징은 상기 프레임 쌍 중 픽셀 쌍에 대응하는 객체의 카테고리 정보를 포함하며, 상기 픽셀 쌍은 상기 제1 프레임 데이터 중 어느 하나의 픽셀 및 상기 제2 프레임 데이터에 포함된 픽셀들 중 상기 어느 하나의 픽셀에 대응하는 픽셀로 구성된,장면 흐름 추정 장치.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 프로세서는,상기 프레임 쌍에 대응하는 모션 필드(motion field)를 획득하고, 상기 모션 필드를 상기 AI 네트워크에 입력하여 상기 프레임 쌍에 대응하는 모션 임베딩 특징 및 비차폐 카테고리 라벨 임베딩 특징을 획득하며, 상기 모션 임베딩 특징 및 상기 비차폐 카테고리 라벨 임베딩 특징을 융합한 혼합 임베딩 특징을 획득하고, 상기 프레임 쌍 중 상기 픽셀 쌍 사이의 재투영 오차가 감소하도록, 상기 혼합 임베딩 특징에 기초하여 상기 모션 필드를 업데이트함으로써 타겟 모션 필드(target motion filed)를 획득하며, 상기 타겟 모션 필드에 기초하여, 상기 프레임 쌍에 대응하는 장면 흐름을 추정하는장면 흐름 추정 장치.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 프로세서는,상기 제2 프레임 데이터에서, 상기 제2 프레임 데이터에 포함된 픽셀들 중 상기 제1 프레임 데이터에 포함된 대상 픽셀에 대응하는 픽셀을 포함하는 이웃 포인트 세트를 결정하고, 상기 대상 픽셀에 대응하는 혼합 임베딩 특징 및 상기 결정된 이웃 포인트 세트에 포함된 픽셀들의 혼합 임베딩 특징들에 기초하여, 상기 대상 픽셀 및 상기 이웃 포인트 세트에 포함된 픽셀들 사이의 매칭 정도를 결정하며, 상기 매칭 정도에 따라 상기 대상 픽셀과 상기 이웃 포인트 세트에 포함된 픽셀들 사이의 재투영 오차가 감소하도록, 상기 모션 필드를 업데이트함으로써, 상기 타겟 모션 필드를 획득하는장면 흐름 추정 장치.</claim></claimInfo><claimInfo><claim>18. 제16항에 있어서,상기 프로세서는상기 제1 프레임 데이터에 포함된 컬러 이미지의 픽셀들 사이의 유사성에 기초하여, 상기 모션 필드를 조정하는 가중치를 획득하고, 상기 가중치를 상기 모션 필드에 적용함으로써 획득한 상기 타겟 모션 필드에 기초하여, 상기 프레임 쌍에 대응하는 장면 흐름을 추정하는장면 흐름 추정 장치.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서,상기 프로세서는,상기 AI 네트워크의 어텐션 인코더에 상기 제1 프레임 데이터에 포함된 컬러 이미지를 입력하여, 상기 제1 프레임 데이터에 포함된 컬러 이미지의 픽셀들 사이의 제1 상관성(correlation)을 획득하고, 상기 제1 상관성에 기초하여 상기 제1 프레임 데이터에 포함된 컬러 이미지에 대응하는 제1 가중치를 결정하는장면 흐름 추정 장치.</claim></claimInfo><claimInfo><claim>20. 제19항에 있어서,상기 프로세서는,상기 어텐션 인코더에 상기 제2 프레임 데이터에 포함된 컬러 이미지를 입력하여, 상기 제2 프레임 데이터에 포함된 컬러 이미지의 픽셀들 사이의 제2 상관성을 획득하고, 상기 제2 상관성에 기초하여 상기 제2 프레임 데이터에 포함된 컬러 이미지에 대응하는 제2 가중치를 결정하며, 상기 제1 가중치 및 상기 제2 가중치를 융합하여 상기 모션 필드를 조정하는 융합 가중치를 획득하는장면 흐름 추정 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>중국 ****** 베이징 차오양구 타...</address><code> </code><country> </country><engName>Xiongfeng PENG</engName><name>슝펑 펑</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 차오양구 타...</address><code> </code><country> </country><engName>Zhihua LIU</engName><name>즈화 류</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 차오양구 타...</address><code> </code><country> </country><engName>Qiang WANG</engName><name>창 왕</name></inventorInfo><inventorInfo><address>경기도 화성...</address><code>420220324846</code><country>대한민국</country><engName>SOONYONG CHO</engName><name>조순용</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>중국</priorityApplicationCountry><priorityApplicationDate>2023.11.15</priorityApplicationDate><priorityApplicationNumber>202311527401.8</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2024.09.03</receiptDate><receiptNumber>1-1-2024-0967196-19</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2024.10.10</receiptDate><receiptNumber>9-1-2024-9011011-89</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2024.10.12</receiptDate><receiptNumber>9-1-2024-9011202-03</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2024.10.12</receiptDate><receiptNumber>9-1-2024-9011206-85</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2024.10.12</receiptDate><receiptNumber>9-1-2024-9011387-29</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2024.10.14</receiptDate><receiptNumber>9-1-2024-9011440-52</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020240119364.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c934b34252d898ad57cb5aa51986c437fd6b561eb74cfb2350f068d396dda103d02bdd2235e665a587bb1a753e0ccdbf66b9d10675b65934eba</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf71563c4f64df90026482c6e4386451d659e7facaf28162990d1fe287308f4eb879053c0acc715cda88a141611fdf1ce0de9bdfdb70d87c93</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>