<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:11:02.112</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.12.06</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0169100</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>신경망 모델을 학습시키는 방법 및 전자 장치</inventionTitle><inventionTitleEng>METHOD AND ELECTRONIC DEVICE FOR TRAINING NUERAL  NETWORK MODEL</inventionTitleEng><openDate>2024.05.02</openDate><openNumber>10-2024-0057297</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/774</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/64</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/12</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/80</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득하고, 제1 카메라에 대응되는 제1 카메라 좌표계와 공간을 제2 시점으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환하고, 제1 영상에 대응되는, 제2 카메라에서 촬영된 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하고, 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시키는, 신경망 모델을 학습시키는 방법 및 전자 장치가 개시된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측(prediction)된 제1 객체 인식 결과를 획득하는 단계(S1310);상기 제1 카메라에 대응되는 제1 카메라 좌표계와 상기 공간을 제2 시점으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하는 단계(S1320);상기 제1 영상에 대응되는, 상기 제2 카메라에서 촬영된 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하는 단계(S1330); 및상기 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시키는 단계(S1340);를 포함하는, 신경망 모델을 학습시키는 방법.</claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는,상기 변환 관계에 기초하여, 상기 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현하는 특징점들의 공간 좌표들로 이루어진 3차원 포즈 정보를 상기 제2 영상 내에서 검출 및 식별되는 상기 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환하고,상기 학습 데이터를 생성하는 단계(S1330)는상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보가 상기 제2 영상 내의 상기 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 상기 학습 데이터를 생성하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1 항 또는 제2 항에 있어서,상기 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는,상기 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 상기 제1 카메라 좌표계에서의 상기 3차원 포즈 정보를 상기 월드 좌표계에서의 3차원 포즈 정보로 변환하는 단계;상기 월드 좌표계와 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 월드 좌표계에서의 3차원 포즈 정보를 상기 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환하는 단계; 및상기 제2 카메라 좌표계에서의 공간 좌표를 제2 카메라의 제2 영상 좌표계로 투영함으로써, 상기 제2 카메라 좌표계에서의 3차원 포즈 정보를 상기 제2 영상 좌표계에서의 상기 2차원 포즈 정보로 변환하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1 항 내지 제3 항 중 어느 한 항에 있어서,상기 제1 카메라 좌표계와 상기 월드 좌표계 간의 변환 관계는,제1 카메라 캘리브레이션을 수행하여, 상기 제1 카메라의 내부 파라미터와 소정의 개수의 특징점들에 대한 상기 월드 좌표계의 공간 좌표와 상기 제1 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍들을 이용하여 상기 제1 카메라의 외부 파라미터를 구함으로써 획득되고,상기 특징점들은 상기 제1 카메라의 관측 시야 내에서 전자 장치(100)를 이동시켜, 소정의 검출 영역의 중심점이나, 상기 전자 장치(100)에 부착한 마커 또는 상기 전자 장치(100)의 외관 특징을 검출한 것인, 방법.</claim></claimInfo><claimInfo><claim>5. 제1 항 내지 제4 항 중 어느 한 항에 있어서,상기 학습 데이터를 생성하는 단계(S1330)는,상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보 중 상기 제2 영상 내에 해당하는 제1 부분과 상기 제2 영상 내에 해당하지 않는 제2 부분을 구분하여 라벨링을 수행함으로써, 상기 학습 데이터를 생성하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제1 항 내지 제5 항 중 어느 한 항에 있어서,상기 제2 카메라에 의해 상기 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경우,상기 학습 데이터를 생성하는 단계(S1330)는,상기 복수 개의 제2 영상들 각각에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1 항 내지 제6 항 중 어느 한 항에 있어서,상기 제2 시점으로 촬영된 복수 개의 제2 영상들은,상기 제2 카메라가 상기 제2 시점을 유지한 상태에서 동적 객체를 소정의 시간 간격으로 촬영된 영상인, 방법.</claim></claimInfo><claimInfo><claim>8. 제1 항 내지 제7 항 중 어느 한 항에 있어서,상기 제2 카메라에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들이 있는 경우,상기 변환하는 단계(S1320)는,상기 제1 카메라 좌표계와 상기 복수 개의 제2 시점들 각각에서의 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고,상기 학습 데이터를 생성하는 단계(S1330)는,상기 복수 개의 제2 영상들 각각에 대하여, 상기 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제1 항 내지 제8 항 중 어느 한 항에 있어서,상기 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들은,상기 제2 카메라가 전자 장치(100)에 탑재된 상태로 정적 객체 주위를 돌거나 상기 정적 객체와의 거리를 변경하면서 상기 서로 다른 복수 개의 제2 시점들로 촬영된 것인, 방법.</claim></claimInfo><claimInfo><claim>10. 제1 항 내지 제9 항 중 어느 한 항에 있어서,상기 학습 데이터를 생성하는 단계(S1330)는,상기 제2 카메라를 탑재한 전자 장치(100)가 상기 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 상기 전자 장치(100)가 이용하는 공간 맵에서 특정된 위치로 상기 전자 장치(100)가 이동한 경우, 상기 학습 데이터를 생성하는, 방법.</claim></claimInfo><claimInfo><claim>11. 하나 이상의 인스트럭션을 저장하는 메모리(110);상기 메모리(110)에 저장된 상기 하나 이상의 인스트럭션을 실행하는 프로세서(120); 카메라(131); 및통신부(140)를 포함하고,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 통신부(140)를 통해, 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 외부 장치(300)의 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측(prediction)된 제1 객체 인식 결과를 상기 외부 장치(300)로부터 획득하고,상기 외부 장치(300)의 카메라에 대응되는 제1 카메라 좌표계와 상기 공간을 제2 시점으로 촬영하는 상기 카메라(131)에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고,상기 제1 영상에 대응되는, 상기 카메라(131)에서 촬영된 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하고, 상기 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시키는, 전자 장치(100).</claim></claimInfo><claimInfo><claim>12. 제11 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 변환 관계에 기초하여, 상기 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현하는 특징점들의 공간 좌표들로 이루어진 3차원 포즈 정보를 상기 제2 영상 내에서 검출 및 식별되는 상기 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환하고,상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보가 상기 제2 영상 내의 상기 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 상기 학습 데이터를 생성하는, 전자 장치(100).</claim></claimInfo><claimInfo><claim>13. 제11 항 또는 제12 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 상기 제1 카메라 좌표계에서의 상기 3차원 포즈 정보를 상기 월드 좌표계에서의 3차원 포즈 정보로 변환하고,상기 월드 좌표계와 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 월드 좌표계에서의 3차원 포즈 정보를 상기 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환하고,상기 제2 카메라 좌표계에서의 공간 좌표를 카메라의 제2 영상 좌표계로 투영함으로써, 상기 제2 카메라 좌표계에서의 3차원 포즈 정보를 상기 제2 영상 좌표계에서의 상기 2차원 포즈 정보로 변환하는, 전자 장치(100).</claim></claimInfo><claimInfo><claim>14. 제11 항 내지 제13 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,제1 카메라 캘리브레이션을 수행하여, 상기 외부 장치(300)의 카메라의 내부 파라미터와 소정의 개수의 특징점들에 대한 상기 월드 좌표계의 공간 좌표와 상기 외부 장치(300)의 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍들을 이용하여 상기 외부 장치(300)의 외부 파라미터를 구함으로써 상기 제1 카메라 좌표계와 상기 월드 좌표계 간의 변환 관계를 획득하고,상기 특징점들은 상기 외부 장치(300)의 카메라의 관측 시야 내에서 상기 전자 장치(100)를 이동시켜, 소정의 검출 영역의 중심점이나, 상기 전자 장치(100)에 부착한 마커 또는 상기 전자 장치(100)의 외관 특징을 검출한 것인, 전자 장치(100).</claim></claimInfo><claimInfo><claim>15. 제11 항 내지 제14 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보 중 상기 제2 영상 내에 해당하는 제1 부분과 상기 제2 영상 내에 해당하지 않는 제2 부분을 구분하여 라벨링을 수행함으로써, 상기 학습 데이터를 생성하는, 전자 장치(100).</claim></claimInfo><claimInfo><claim>16. 제11 항 내지 제15 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 카메라(131)에 의해 상기 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경우, 상기 복수 개의 제2 영상들 각각에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하는, 전자 장치(100).</claim></claimInfo><claimInfo><claim>17. 제11 항 내지 제16 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 카메라(131)에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들이 있는 경우, 상기 제1 카메라 좌표계와 상기 복수 개의 제2 시점들 각각에서의 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고, 상기 복수 개의 제2 영상들 각각에 대하여, 상기 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하는, 전자 장치(100).</claim></claimInfo><claimInfo><claim>18. 제11 항 내지 제17 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 전자 장치(100)가 상기 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 상기 전자 장치(100)가 이용하는 공간 맵에서 특정된 위치로 상기 전자 장치(100)가 이동한 경우, 상기 학습 데이터를 생성하는, 전자 장치(100).</claim></claimInfo><claimInfo><claim>19. 하나 이상의 인스트럭션을 저장하는 메모리(210);상기 메모리(210)에 저장된 상기 하나 이상의 인스트럭션을 실행하는 프로세서(220); 및통신부(230)를 포함하고,상기 프로세서(220)는 상기 하나 이상의 인스트럭션을 실행함으로써,적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득하고,상기 통신부(230)를 통해, 상기 공간을 제2 시점으로 촬영하는 제2 카메라에서 촬영된 제2 영상을 획득하고,상기 제1 카메라에 대응되는 제1 카메라 좌표계와 상기 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고,상기 제1 영상에 대응되는 상기 제2 영상에 대하여, 상기 제2 시점 기준으로 변환된 제1객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하고, 상기 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시키는, 클라우드 서버(200).</claim></claimInfo><claimInfo><claim>20. 제19 항에 있어서,상기 프로세서(220)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 통신부(230)를 통해, 상기 학습된 제2 신경망 모델의 네트워크 파라미터 값들을 상기 제2 신경망 모델을 탑재한 전자 장치(100)에 전송하는, 클라우드 서버(200).</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>JU, Jae Yong</engName><name>주재용</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.10.24</priorityApplicationDate><priorityApplicationNumber>1020220137781</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.12.06</receiptDate><receiptNumber>1-1-2022-1311845-45</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220169100.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9355b1c9ef5e4edb144c79f2f2238021699d80fc303f905587331052a7a3e99e3844467d90b96f482f9443de9341c414a85a4d268184675f08</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf468f609106abddb288433aa916f97c6752516adca0c801a14e2f2595f98d01c120943383d90bcb596fdf8b4d730a89060da8d7639ce8058d</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>