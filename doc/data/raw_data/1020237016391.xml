<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:54:54.5454</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2021.11.19</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7016391</applicationNumber><claimCount>30</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>사운드 이벤트 분류를 위한 전이 학습</inventionTitle><inventionTitleEng>TRANSFER LEARNING FOR SOUND EVENT CLASSIFICATION</inventionTitleEng><openDate>2023.07.24</openDate><openNumber>10-2023-0110512</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.11.04</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.05.15</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/096</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/084</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06F 18/25</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06F 18/2431</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G10L 15/16</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 방법은, 제1 세트의 사운드 클래스들을 검출하도록 트레이닝된 제1 뉴럴 네트워크에 기반하여 제2 뉴럴 네트워크를 초기화하는 단계, 및 제1 뉴럴 네트워크의 출력 및 제2 뉴럴 네트워크의 출력을 하나 이상의 커플링 네트워크들에 링크하는 단계를 포함한다. 방법은 또한, 제2 뉴럴 네트워크 및 하나 이상의 커플링 네트워크들을 트레이닝한 후에, 제2 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도 및 제1 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도에 기반하여 제1 뉴럴 네트워크를 폐기할지 여부를 결정하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.06.02</internationOpenDate><internationOpenNumber>WO2022115840</internationOpenNumber><internationalApplicationDate>2021.11.19</internationalApplicationDate><internationalApplicationNumber>PCT/US2021/072523</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 디바이스로서,하나 이상의 프로세서들을 포함하며, 상기 하나 이상의 프로세서들은, 제1 세트의 사운드 클래스들을 검출하도록 트레이닝된 제1 뉴럴 네트워크에 기반하여 제2 뉴럴 네트워크를 초기화하고;  하나 이상의 커플링 네트워크들에 대한 입력으로서 상기 제1 뉴럴 네트워크의 출력 및 상기 제2 뉴럴 네트워크의 출력을 링크하고; 그리고 상기 제2 뉴럴 네트워크 및 상기 하나 이상의 커플링 네트워크들이 트레이닝된 후에, 상기 제2 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도 및 상기 제1 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도에 기반하여 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하도록 구성되는, 디바이스.</claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서, 상기 하나 이상의 프로세서들은 추가로, 상기 제1 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도와 비교할 때, 상기 제2 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도를 표시하는 메트릭의 값을 결정하도록 구성되고, 상기 하나 이상의 프로세서들은 상기 메트릭의 값에 추가로 기반하여 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하도록 구성되는, 디바이스.</claim></claimInfo><claimInfo><claim>3. 제1 항에 있어서, 상기 제1 뉴럴 네트워크의 출력은 상기 제1 뉴럴 네트워크에 의해 특정 오디오 데이터 샘플들에 할당된 사운드 클래스를 표시하고, 상기 제2 뉴럴 네트워크의 출력은 상기 제2 뉴럴 네트워크에 의해 상기 특정 오디오 데이터 샘플들에 할당된 사운드 클래스를 표시하는, 디바이스.</claim></claimInfo><claimInfo><claim>4. 제1 항에 있어서, 상기 제1 뉴럴 네트워크의 출력은 상기 제1 세트의 사운드 클래스들 중 제1 카운트의 사운드 클래스들에 대응하는 제1 카운트의 데이터 엘리먼트들을 포함하고, 상기 제2 뉴럴 네트워크의 출력은 제2 세트의 사운드 클래스들 중 제2 카운트의 사운드 클래스들에 대응하는 제2 카운트의 데이터 엘리먼트들을 포함하고, 그리고 상기 하나 이상의 커플링 네트워크들은, 상기 제1 뉴럴 네트워크의 출력에 기반하여, 상기 제2 카운트의 데이터 엘리먼트들을 갖는 제3 출력을 생성하도록 구성된 하나 이상의 어댑터 계층들을 포함하는 뉴럴 어댑터를 포함하는, 디바이스.</claim></claimInfo><claimInfo><claim>5. 제4 항에 있어서, 상기 하나 이상의 커플링 네트워크들은, 상기 뉴럴 어댑터로부터의 상기 제3 출력과 상기 제2 뉴럴 네트워크의 출력을 병합(merge)하도록 구성된 하나 이상의 어그리게이션 계층(aggregation layer)들을 포함하고 그리고 병합된 출력을 생성하기 위한 출력 계층을 포함하는 병합기 어댑터를 포함하는, 디바이스.</claim></claimInfo><claimInfo><claim>6. 제1 항에 있어서, 상기 제1 뉴럴 네트워크의 출력 계층은 N 개의 출력 노드들을 포함하고, 상기 제2 뉴럴 네트워크의 출력 계층은 N+K 개의 출력 노드들을 포함하며, 상기 N은 1 이상의 정수이고, 상기 K는 1 이상의 정수인,디바이스.</claim></claimInfo><claimInfo><claim>7. 제6 항에 있어서, 상기 N 개의 출력 노드들은 상기 제1 뉴럴 네트워크가 트레이닝되어 인식할 N 개의 사운드 이벤트 클래스들에 대응하고, 상기 N+K 개의 출력 노드들은, 상기 N 개의 사운드 이벤트 클래스들에 대응하는 상기 N 개의 출력 노드들 및 K 개의 부가적인 사운드 이벤트 클래스들에 대응하는 K 개의 출력 노드들을 포함하는,디바이스.</claim></claimInfo><claimInfo><claim>8. 제1 항에 있어서, 상기 제2 뉴럴 네트워크를 초기화하기 전에, 상기 제1 뉴럴 네트워크는 활성 사운드 이벤트 분류기로서 지정되고, 상기 하나 이상의 프로세서들은, 상기 제1 뉴럴 네트워크를 폐기하기 위한 결정에 기반하여 상기 제2 뉴럴 네트워크를 상기 활성 사운드 이벤트 분류기로서 지정하도록 구성되는,디바이스.</claim></claimInfo><claimInfo><claim>9. 제1 항에 있어서, 상기 제2 뉴럴 네트워크를 초기화하기 전에, 상기 제1 뉴럴 네트워크는 활성 사운드 이벤트 분류기로서 지정되고, 상기 하나 이상의 프로세서들은, 상기 제1 뉴럴 네트워크를 폐기하지 않는다는 결정에 기반하여 상기 제1 뉴럴 네트워크, 상기 제2 뉴럴 네트워크, 및 상기 하나 이상의 커플링 네트워크들을 함께 상기 활성 사운드 이벤트 분류기로서 지정하도록 구성되는,디바이스.</claim></claimInfo><claimInfo><claim>10. 제1 항에 있어서, 상기 하나 이상의 프로세서들은 모바일 컴퓨팅 디바이스 내에 통합되는, 디바이스.</claim></claimInfo><claimInfo><claim>11. 제1 항에 있어서, 상기 하나 이상의 프로세서들은 차량 내에 통합되는, 디바이스.</claim></claimInfo><claimInfo><claim>12. 제1 항에 있어서, 상기 하나 이상의 프로세서들은, 증강 현실 헤드셋, 혼합 현실 헤드셋, 가상 현실 헤드셋, 또는 웨어러블 디바이스 중 하나 이상 내에 통합되는, 디바이스.</claim></claimInfo><claimInfo><claim>13. 제1 항에 있어서, 상기 하나 이상의 프로세서들은 집적 회로에 포함되는, 디바이스.</claim></claimInfo><claimInfo><claim>14. 제1 세트의 사운드 클래스들을 검출하도록 트레이닝된 제1 뉴럴 네트워크에 기반하여 제2 뉴럴 네트워크를 초기화하는 단계;상기 제1 뉴럴 네트워크의 출력 및 상기 제2 뉴럴 네트워크의 출력을 하나 이상의 커플링 네트워크들에 링크하는 단계; 및상기 제2 뉴럴 네트워크 및 상기 하나 이상의 커플링 네트워크들이 트레이닝된 후에, 상기 제2 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도 및 상기 제1 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도에 기반하여 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>15. 제14 항에 있어서, 상기 제1 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도와 비교할 때, 상기 제2 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도를 표시하는 메트릭의 값을 결정하는 단계를 더 포함하고, 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하는 단계는 상기 메트릭의 값에 추가로 기반하는, 방법.</claim></claimInfo><claimInfo><claim>16. 제14 항에 있어서, 상기 제2 뉴럴 네트워크는 초기화되고, 상기 링크하는 단계는 트리거 이벤트(trigger event)를 검출하는 것에 기반하여 자동으로 수행되는, 방법.</claim></claimInfo><claimInfo><claim>17. 제16 항에 있어서, 상기 트리거 이벤트는 인식되지 않은 사운드 클래스들의 임계 수량(threshold quantity)에 직면하는 것에 기반하는, 방법.</claim></claimInfo><claimInfo><claim>18. 제16 항에 있어서, 상기 트리거 이벤트는 사용자 세팅에 의해 특정되는, 방법.</claim></claimInfo><claimInfo><claim>19. 제14 항에 있어서, 상기 제1 뉴럴 네트워크는 입력 계층, 은닉 계층들, 및 제1 출력 계층을 포함하고, 상기 제1 뉴럴 네트워크에 기반하여 상기 제2 뉴럴 네트워크를 초기화하는 단계는, 상기 제1 뉴럴 네트워크의 상기 입력 계층 및 상기 은닉 계층들의 사본(copy)들을 생성하는 단계; 및 상기 입력 계층 및 상기 은닉 계층들의 사본들에 제2 출력 계층을 연결하는 단계를 포함하고, 상기 제1 출력 계층은 상기 제1 세트의 사운드 클래스들 중 일정 카운트의 사운드 클래스들에 대응하는 제1 카운트의 출력 노드들을 포함하고, 상기 제2 출력 계층은 제2 세트의 사운드 클래스들 중 일정 카운트의 사운드 클래스들에 대응하는 제2 카운트의 출력 노드를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>20. 제14 항에 있어서, 상기 제1 뉴럴 네트워크의 출력은 상기 제1 뉴럴 네트워크에 의해 특정 오디오 데이터 샘플들에 할당된 사운드 클래스를 표시하고, 상기 제2 뉴럴 네트워크의 출력은 상기 제2 뉴럴 네트워크에 의해 상기 특정 오디오 데이터 샘플들에 할당된 사운드 클래스를 표시하는, 방법.</claim></claimInfo><claimInfo><claim>21. 제20 항에 있어서, 상기 하나 이상의 커플링 네트워크들은, 상기 제1 뉴럴 네트워크의 출력 및 상기 제2 뉴럴 네트워크의 출력에 기반하여 상기 하나 이상의 커플링 네트워크들에 의해 상기 특정 오디오 데이터 샘플들에 할당된 사운드 클래스를 표시하는 병합된 출력을 생성하도록 구성되는, 방법. </claim></claimInfo><claimInfo><claim>22. 제14 항에 있어서,상기 제1 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도를 표시하는 제1 값을 결정하는 단계; 및상기 제2 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도를 표시하는 제2 값을 결정하는 단계를 더 포함하고,상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하는 단계는 상기 제1 값과 상기 제2 값의 비교에 기반하는,방법.</claim></claimInfo><claimInfo><claim>23. 제14 항에 있어서, 상기 제1 뉴럴 네트워크의 출력은 상기 제1 세트의 사운드 클래스들 중 제1 카운트의 사운드 클래스들에 대응하는 제1 카운트의 데이터 엘리먼트들을 포함하고, 상기 제2 뉴럴 네트워크의 출력은 제2 세트의 사운드 클래스들 중 제2 카운트의 사운드 클래스들에 대응하는 제2 카운트의 데이터 엘리먼트들을 포함하고, 그리고 상기 하나 이상의 커플링 네트워크들은, 상기 제1 뉴럴 네트워크의 출력에 기반하여, 상기 제2 카운트의 데이터 엘리먼트들을 갖는 제3 출력을 생성하도록 구성된 하나 이상의 어댑터 계층들을 포함하는 뉴럴 어댑터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 제23 항에 있어서, 상기 하나 이상의 커플링 네트워크들은, 상기 뉴럴 어댑터로부터의 상기 제3 출력과 상기 제2 뉴럴 네트워크의 출력을 병합하도록 구성된 하나 이상의 어그리게이션 계층들을 포함하고 그리고 병합된 출력을 생성하기 위한 출력 계층을 포함하는 병합기 어댑터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>25. 제14 항에 있어서, 상기 제1 뉴럴 네트워크의 링크 가중치들은 상기 제2 뉴럴 네트워크 및 상기 하나 이상의 커플링 네트워크들의 트레이닝 동안 업데이트되지 않는, 방법.</claim></claimInfo><claimInfo><claim>26. 제14 항에 있어서, 상기 제2 뉴럴 네트워크를 초기화하는 단계 전에, 상기 제1 뉴럴 네트워크는 활성 사운드 이벤트 분류기로서 지정되고, 상기 방법은, 상기 제1 뉴럴 네트워크를 폐기하기 위한 결정에 기반하여 상기 제2 뉴럴 네트워크를 상기 활성 사운드 이벤트 분류기로서 지정하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>27. 제14 항에 있어서, 상기 제2 뉴럴 네트워크를 초기화하는 단계 전에, 상기 제1 뉴럴 네트워크는 활성 사운드 이벤트 분류기로서 지정되고, 상기 방법은, 상기 제1 뉴럴 네트워크를 폐기하지 않는다는 결정에 기반하여 상기 제1 뉴럴 네트워크, 상기 제2 뉴럴 네트워크, 및 상기 하나 이상의 커플링 네트워크들을 함께 상기 활성 사운드 이벤트 분류기로서 지정하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>28. 디바이스로서,제1 세트의 사운드 클래스들을 검출하도록 트레이닝된 제1 뉴럴 네트워크에 기반하여 제2 뉴럴 네트워크를 초기화하기 위한 수단;상기 제1 뉴럴 네트워크의 출력 및 상기 제2 뉴럴 네트워크의 출력을 하나 이상의 커플링 네트워크들에 링크하기 위한 수단; 및상기 제2 뉴럴 네트워크 및 상기 하나 이상의 커플링 네트워크들이 트레이닝된 후에, 상기 제2 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도 및 상기 제1 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도에 기반하여 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하기 위한 수단을 포함하는, 디바이스.</claim></claimInfo><claimInfo><claim>29. 제28 항에 있어서, 상기 제1 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도와 비교할 때, 상기 제2 뉴럴 네트워크에 의해 상기 제1 세트의 사운드 클래스들의 오디오 데이터 샘플들에 할당된 사운드 클래스들의 정확도를 표시하는 메트릭의 값을 결정하기 위한 수단을 더 포함하고, 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하기 위한 수단은 상기 메트릭의 값에 기반하여 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하도록 구성되는, 디바이스.</claim></claimInfo><claimInfo><claim>30. 비-일시적인 컴퓨터-판독가능 저장 매체로서, 상기 컴퓨터-판독가능 저장 매체는 명령들을 포함하고, 상기 명령들은, 프로세서에 의해 실행될 때, 상기 프로세서로 하여금, 제1 세트의 사운드 클래스들을 검출하도록 트레이닝된 제1 뉴럴 네트워크에 기반하여 제2 뉴럴 네트워크를 초기화하게 하고; 상기 제1 뉴럴 네트워크의 출력 및 상기 제2 뉴럴 네트워크의 출력을 하나 이상의 커플링 네트워크들에 링크하게 하고; 그리고 상기 제2 뉴럴 네트워크 및 상기 하나 이상의 커플링 네트워크들을 트레이닝한 후에, 상기 제2 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도 및 상기 제1 뉴럴 네트워크에 의해 할당된 사운드 클래스들의 정확도에 기반하여 상기 제1 뉴럴 네트워크를 폐기할지 여부를 결정하게 하는, 비-일시적인 컴퓨터-판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 *****-**** 캘리포니아주 샌 디에고 모어하우스 드라이브 ****</address><code>519980798710</code><country>미국</country><engName>QUALCOMM INCORPORATED</engName><name>퀄컴 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아주...</address><code> </code><country> </country><engName>SAKI, Fatemeh</engName><name>사키, 파트메</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아주...</address><code> </code><country> </country><engName>GUO, Yinyi</engName><name>구오, 인이</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아주...</address><code> </code><country> </country><engName>VISSER, Erik</engName><name>비저, 에릭</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 중구 서소문로**(서소문동, 정안빌딩*층)</address><code>920121001826</code><country>대한민국</country><engName>NAM &amp; NAM</engName><name>특허법인 남앤남</name></agentInfo><agentInfo><address>서울 중구 서소문로 **, *층(서소문동)</address><code>920241000417</code><country>대한민국</country><engName>NAM IP GROUP</engName><name>특허법인(유)남아이피그룹</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2020.11.24</priorityApplicationDate><priorityApplicationNumber>17/102,776</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.05.15</receiptDate><receiptNumber>1-1-2023-0535059-73</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.06.28</receiptDate><receiptNumber>1-5-2023-0102614-63</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName> </commonCodeName><documentEngName>[Appointment of Agent] Report on Agent (Representative)</documentEngName><documentName>[대리인선임]대리인(대표자)에 관한 신고서</documentName><receiptDate>2024.03.22</receiptDate><receiptNumber>1-1-2024-0326863-53</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[반환신청]서류 반려요청서·반환신청서</documentName><receiptDate>2024.03.25</receiptDate><receiptNumber>1-1-2024-0329219-95</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Appointment of Agent] Report on Agent (Representative)</documentEngName><documentName>[대리인선임]대리인(대표자)에 관한 신고서</documentName><receiptDate>2024.03.25</receiptDate><receiptNumber>1-1-2024-0329305-13</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2024.11.04</receiptDate><receiptNumber>1-1-2024-1206465-37</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237016391.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9373db39d46ed5abc47b7785476ceb73d22fa36a298573ded3f45686afff2a3265600c6b3435d69c195bbf79164a66e58bee335801fc5f5e7f</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf8abe72dadd45f0650523a99a102ee40c8adf82f12b225b2469d6e6256c00fab3ec8196e91240e2c7d289f8f3d5b76ecd0592c59a96e18aae</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>