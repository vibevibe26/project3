<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:04:46.446</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2024.02.19</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2025-7029625</applicationNumber><claimCount>202</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩을 위한 방법 및 데이터 처리 시스템</inventionTitle><inventionTitleEng>METHOD AND DATA PROCESSING SYSTEM FOR LOSSY IMAGE OR VIDEO ENCODING, TRANSMISSION AND DECODING</inventionTitleEng><openDate>2025.10.21</openDate><openNumber>10-2025-0151420</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.09.04</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/537</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/46</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/088</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 발명의 일 실시예에 따른 손실 비디오(lossy video)의 인코딩, 전송 및 디코딩을 위한 방법은 제1 컴퓨터 시스템에서 입력 프레임과 이전 프레임을 수신하는 단계, 제1 학습된 신경망을 사용해 상기 입력 프레임과 상기 이전 프레임을 기반으로 한 입력을 인코딩하여 제1 잠재 표현을 생성하는 단계, 제2 학습된 신경망을 사용해 상기 입력 프레임을 인코딩하여 제2 잠재 표현을 생성하는 단계, 상기 제1 및 제2 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계, 제3 학습된 신경망을 사용해 상기 제1 잠재 표현을 디코딩하여 상기 입력 프레임과 이전 프레임 간의 출력 플로우 맵(output flow map)을 획득하는 단계, 제4 학습된 신경망을 사용해 상기 제2 잠재 표현과 상기 출력 플로우 맵을 기반으로 한 입력을 디코딩하여 출력 프레임을 생성하되, 여기서 출력 프레임은 상기 입력 프레임의 근사값인 것을 특징으로 하는 단계를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2024.08.22</internationOpenDate><internationOpenNumber>WO2024170794</internationOpenNumber><internationalApplicationDate>2024.02.19</internationalApplicationDate><internationalApplicationNumber>PCT/EP2024/054169</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 손실 비디오의 인코딩, 전송 및 디코딩을 위한 방법으로서,제1 컴퓨터 시스템에서 입력 프레임과 이전 프레임을 수신하는 단계;제1 학습된 신경망을 사용해 상기 입력 프레임과 상기 이전 프레임을 기반으로 한 입력을 인코딩하여 제1 잠재 표현을 생성하는 단계;제2 학습된 신경망을 사용해 상기 입력 프레임을 인코딩하여 제2 잠재 표현을 생성하는 단계;상기 제1 및 제2 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;제3 학습된 신경망을 사용해 상기 제1 잠재 표현을 인코딩하여 상기 입력 프레임과 이전 프레임 간의 출력 플로우 맵을 획득하는 단계; 및제4 학습된 신경망을 사용해 상기 제2 잠재 표현과 상기 출력 플로우 맵을 기반으로 한 입력을 디코딩하여 출력 프레임을 생성하는 단계로서, 여기서 출력 프레임은 상기 입력 프레임의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 출력 플로우 맵을 이용하여 이전 프레임에 해당하는, 이전에 디코딩된 프레임으로부터 출력 워핑된 프레임을 얻을 수 있으며, 상기 출력 플로우 맵을 기반으로 한 입력은 상기 출력 워핑된 프레임을 기반으로 하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 출력 워핑된 프레임은 해당 출력 워핑된 프레임을 기반으로 한 입력을 얻기 위해 변환되며, 여기서 변환 시 다운샘플링, 컨볼루션, 공간-깊이 변환, 분해, Haar 변환, 신경망 방식 중 적어도 하나 이상의 변환 방식을 사용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>4. 제1항 내지 제3항에 있어서,추가적으로, 제1 컴퓨터 시스템에서, 제5 학습된 신경망을 사용해 상기 제1 잠재 표현을 디코딩하여 입력 프레임과 이전 프레임 간의 예측된 플로우 맵을 획득하는 단계;를 더 포함하며,여기서 제2 학습된 신경망은 상기 예측된 플로우 맵을 기반으로 한 입력을 추가적으로 인코딩하여 제2 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 예측된 플로우 맵을 이용하여 이전 프레임에 해당하는, 이전에 디코딩된 프레임으로부터 입력 워핑된 프레임을 획득하며, 상기 예측된 플로우 맵을 기반으로 한 입력은 상기 입력 워핑된 프레임에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5항에 있어서,상기 이전 프레임을 기반으로 한 입력이 입력 프레임과 이전 프레임 간의 플로우 맵을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>7. 제1항 내지 제6항 중 어느 한 항에 있어서,상기한 입력 프레임과 이전 프레임 간의 플로우 맵을 이용하여 이전 프레임에 해당하는, 이전에 디코딩된 프레임으로부터 제1 워핑된 프레임을 획득하며;상기한 이전 프레임 기반의 입력은 상기 제1 워핑된 프레임을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>8. 제1항 내지 제7항 중 어느 한 항에 있어서,이전 프레임에 해당하는, 이전에 디코딩된 프레임이 제1 학습 신경망에 의해 추가적으로 인코딩되어 제1 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>9. 제1항 내지 제8항 중 어느 한 항에 있어서,제1 컴퓨터 시스템에 더 이전 프레임이 수신될 수 있으며;상기 더 이전 프레임에 해당하는 더 이전에 디코딩된 프레임이 제1 학습 신경망에 의해 추가적으로 인코딩되어 제1 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>10. 제9항에 있어서,더 이전 프레임 및 이전 프레임 간의 예측된 플로우 맵이 제1 학습 신경망에 의해 추가적으로 인코딩되어 제1 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>11. 제9항 내지 제10항에 있어서,이전 프레임에 해당하는 이전에 디코딩된 프레임, 더 이전 프레임에 해당하는 더 이전에 디코딩된 프레임, 그리고 더 이전 프레임과 이전 프레임 간의 예측된 플로우 맵 중 적어도 둘 이상은 제1 학습 신경망에 의해 인코딩되기 전에 연결되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>12. 제8항 내지 제11항 중 어느 한 항에 있어서,이전 프레임에 해당하는 이전에 디코딩된 프레임은 제3 학습 신경망에 의해 추가적으로 디코딩되어 출력 플로우 맵을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>13. 제9항 내지 제12항 중 어느 한 항에 있어서,더 이전 프레임에 해당하는 더 이전에 디코딩된 프레임은 제3 학습 신경망에 의해 추가적으로 디코딩되어 출력 플로우 맵을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>14. 제9항 내지 제13항 중 어느 한 항에 있어서,더 이전 프레임과 이전 프레임 간의 예측된 플로우 맵은 제3 학습 신경망에 의해 추가적으로 디코딩되어 출력 플로우 맵을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>15. 제13항 내지 제14항에 있어서,이전 프레임에 해당하는 이전에 디코딩된 프레임, 더 이전 프레임에 해당하는 더 이전에 디코딩된 프레임, 그리고 더 이전 프레임과 이전 프레임 간의 예측된 플로우 맵 중 적어도 둘 이상은 제3 학습 신경망에 의해 디코딩되기 전에 연결되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>16. 제12항 내지 제15항 중 어느 한 항에 있어서,이전 프레임에 해당하는 이전에 디코딩된 프레임, 더 이전 프레임에 해당하는 더 이전에 디코딩된 프레임, 그리고 더 이전 프레임과 이전 프레임 간의 예측된 플로우 맵 중 적어도 하나 이상은 제3 신경망에 의해 디코딩되기 전에 변환될 수 있으며, 여기서 변환은 다운샘플링, 컨볼루션, 공간-깊이 변환, 분해, Haar 변환 및 신경망 방식 중 적어도 하나 이상의 변환 방식을 사용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,이전 프레임에 해당하는 이전에 디코딩된 프레임, 더 이전 프레임에 해당하는 더 이전에 디코딩된 프레임, 그리고 더 이전 프레임과 이전 프레임 간의 예측된 플로우 맵 중 적어도 하나 이상에 대해 각각 별도의 변환이 사용되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>18. 제1항 내지 제17항 중 어느 한 항에 있어서,제1 컴퓨터 시스템에서 미래 프레임이 수신될 수 있으며;상기 미래 프레임을 기반으로 한 입력이 제1 학습 신경망에 의해 추가적으로 인코딩되어 제1 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서,이후 프레임을 기반으로 한 입력은 입력 프레임과 이후 프레임 간의 플로우 맵을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>20. 제18항 또는 제19항에 있어서,상기한 입력 프레임과 이후 프레임 간의 플로우 맵을 이용하여 이후 프레임에 해당하는 이전에 디코딩된 이후 프레임으로부터 제2 워핑된 프레임을 획득하며;상기한 이후 프레임 기반의 입력은 상기 제2 워핑된 프레임을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>21. 제18항 내지 제20항 중 어느 한 항에 있어서,이후 프레임에 해당하는 이전에 디코딩된 이후 프레임은 제1 학습 신경망에 의해 추가적으로 인코딩되어 제1 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>22. 제18항 내지 제21항 중 어느 한 항에 있어서,미래 프레임에 해당하는 이전에 디코딩된 미래 프레임은 제3 학습 신경망에 의해 추가적으로 디코딩되어 출력 플로우 맵을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>23. 제18항 내지 제22항 중 어느 한 항에 있어서,제1 컴퓨터 시스템에서 더 미래 프레임이 수신되며;상기한 더 미래 프레임에 해당하는 더 이전에 디코딩된 미래 프레임이 제1 학습 신경망에 의해 추가적으로 인코딩되어 제1 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>24. 제23항에 있어서,더 미래 프레임 및 미래 프레임 간의 예측된 플로우 맵이 제1 학습 신경망에 의해 추가적으로 인코딩되어 제1 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>25. 제23항 내지 제24항에 있어서,미래 프레임에 해당하는 이전에 디코딩된 미래 프레임, 더 미래 프레임에 해당하는 더 이전에 디코딩된 미래 프레임, 그리고 더 미래 프레임과 미래 프레임 간의 예측된 플로우 맵 중 적어도 둘 이상은 제1 학습 신경망에 의해 인코딩되기 전에 연결되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>26. 제22항 내지 제25항 중 어느 한 항에 있어서,미래 프레임에 해당하는 이전에 디코딩된 미래 프레임은 제3 학습 신경망에 의해 추가적으로 디코딩되어 출력 플로우 맵을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>27. 제23항 내지 제26항 중 어느 한 항에 있어서,더 미래 프레임에 해당하는 더 이전에 디코딩된 미래 프레임이 제3 학습 신경망에 의해 추가적으로 디코딩되어 출력 플로우 맵을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>28. 제23항 내지 제27항 중 어느 한 항에 있어서,더 미래 프레임과 미래 프레임 간의 예측된 플로우 맵이 제3 학습 신경망에 의해 추가적으로 디코딩되어 출력 플로우 맵을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>29. 제27항 또는 제28항에 있어서,미래 프레임에 해당하는 이전에 디코딩된 미래 프레임, 더 미래 프레임에 해당하는 더 이전에 디코딩된 미래 프레임, 그리고 더 미래 프레임과 미래 프레임 간의 예측된 플로우 맵 중 적어도 둘 이상은 제3 학습 신경망에 의해 디코딩되기 전에 연결되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>30. 제27항 내지 제29항 중 어느 한 항에 있어서,미래 프레임에 해당하는 이전에 디코딩된 미래 프레임, 더 미래 프레임에 해당하는 더 이전에 디코딩된 미래 프레임, 그리고 더 미래 프레임과 미래 프레임 간의 예측된 플로우 맵 중 적어도 하나 이상은 제3 신경망에 의해 디코딩되기 전에 변환되며,여기서 변환은 다운샘플링, 컨볼루션, 공간-깊이 변환, 분해, Haar 변환 및 신경망 방식 중 적어도 하나 이상의 변환 방식을 사용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>31. 제30항에 있어서,미래 프레임에 해당하는 이전에 디코딩된 미래 프레임, 더 미래 프레임에 해당하는 더 이전에 디코딩된 미래 프레임, 그리고 더 미래 프레임과 미래 프레임 간의 예측된 플로우 맵 중 적어도 하나 이상에 대해 각각 별도의 변환이 사용되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>32. 제1항 내지 제31항 중 어느 한 항에 있어서,추가적으로, 제6 학습 신경망을 사용해 제2 잠재 표현을 인코딩하여 하이퍼 잠재 표현을 생성하는 단계;상기 하이퍼 잠재 표현을 제2 컴퓨터 시스템으로 추가적으로 전송하는 단계; 및 제7 학습 신경망을 사용해 상기 하이퍼 잠재 표현을 디코딩하여 출력 잠재 표현을 획득하는 단계로서, 상기 출력 잠재 표현은 출력 프레임을 얻는 데 사용되고, 출력 플로우 맵을 기반으로 한 입력이 제6 학습 신경망에 의해 추가적으로 인코딩되어 하이퍼 잠재 표현을 생성하는 것을 특징으로 하는 단계;를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>33. 제32항에 있어서,상기 출력 플로우 맵을 이용하여 이전 프레임에 해당하는 이전에 디코딩된 프레임으로부터 출력 워핑된 프레임을 획득하고, 상기한 제6 학습 신경망에 의해 추가적으로 인코딩된 출력 플로우 맵 기반의 입력은 상기 출력 워핑된 프레임을 기반으로 하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>34. 제33항에 있어서,상기 출력 워핑된 프레임은, 제6 학습 신경망에 의해 추가적으로 인코딩된 출력 플로우 맵 기반의 입력을 얻기 위해 변환되며, 여기서 변환은 다운샘플링, 컨볼루션, 공간-깊이 변환, 분해, Haar 변환 및 신경망 방식 중 적어도 하나 이상의 변환 방식을 사용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>35. 제34항에 있어서,상기 변환을 여러 횟수로 반복 적용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>36. 제32항 내지 제35항 중 어느 한 항에 있어서,상기 출력 플로우 맵 기반의 입력이 제7 학습 신경망에 의해 추가적으로 디코딩되어 출력 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>37. 제36항에 있어서,상기 출력 플로우 맵을 이용하여 이전 프레임에 해당하는 이전에 디코딩된 프레임으로부터 출력 워핑된 프레임을 획득하고, 상기한 제7 학습 신경망에 의해 추가적으로 디코딩된 출력 플로우 맵 기반의 입력은 상기 출력 워핑된 프레임을 기반으로 하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>38. 제37항에 있어서,상기 출력 워핑된 프레임이 제7 학습 신경망에 의해 추가적으로 디코딩된 출력 플로우 맵 기반의 입력을 얻기 위해 변환되며, 여기서 변환은 다운샘플링, 컨볼루션, 공간-깊이 변환, 분해, Haar 변환 및 신경망 방식 중 적어도 하나 이상의 변환 방식을 사용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>39. 제38항에 있어서,상기 변환을 여러 횟수로 반복 적용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>40. 제32항 내지 39항 중 어느 한 항에 있어서,추가적으로, 제8 학습 신경망을 사용해 하이퍼 잠재 표현을 인코딩하여 하이퍼-하이퍼 잠재 표현을 생성하는 단계;상기 하이퍼-하이퍼 잠재 표현을 제2 컴퓨터 시스템으로 추가적으로 전송하는 단계; 및제9 학습 신경망을 사용해 상기 하이퍼-하이퍼 잠재 표현을 디코딩하여 출력 하이퍼 잠재 표현을 획득하는 단계로서, 상기 출력 하이퍼 잠재 표현은 출력 프레임을 얻는 데 사용되고, 출력 플로우 맵을 기반으로 한 입력은 제8 학습 신경망에 의해 추가적으로 인코딩되어 하이퍼 잠재 표현을 생성하는 것을 특징으로 하는 단계;를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>41. 제40항에 있어서,상기 출력 플로우 맵을 이용하여 이전 프레임에 해당하는 이전에 디코딩된 프레임으로부터 출력 워핑된 프레임을 획득하고, 상기한 제8 학습 신경망에 의해 추가적으로 인코딩된 출력 플로우 맵 기반의 입력은 상기 출력 워핑된 프레임을 기반으로 하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>42. 제41항에 있어서,상기 출력 워핑된 프레임은, 제8 학습 신경망에 의해 추가적으로 인코딩된 출력 플로우 맵 기반의 입력을 얻기 위해 변환되며, 여기서 변환은 다운샘플링, 컨볼루션, 공간-깊이 변환, 분해, Haar 변환 및 신경망 방식 중 적어도 하나 이상의 변환 방식을 사용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>43. 제42항에 있어서,상기 변환을 여러 횟수로 반복 적용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>44. 제40항 내지 제43항 중 어느 한 항에 있어서,상기 출력 플로우 맵 기반의 입력이 제9 학습 신경망에 의해 추가적으로 디코딩되어 출력 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>45. 제44항에 있어서,상기 출력 플로우 맵을 이용하여 이전 프레임에 해당하는 이전에 디코딩된 프레임으로부터 출력 워핑된 프레임을 획득하고, 상기한 제9 학습 신경망에 의해 추가적으로 디코딩된 출력 플로우 맵 기반의 입력은 상기 출력 워핑된 프레임을 기반으로 하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>46. 제45항에 있어서,상기 출력 워핑된 프레임이 제9 학습 신경망에 의해 추가적으로 디코딩된 출력 플로우 맵 기반의 입력을 얻기 위해 변환되며, 여기서 변환은 다운샘플링, 컨볼루션, 공간-깊이 변환, 분해, Haar 변환 및 신경망 방식 중 적어도 하나 이상의 변환 방식을 사용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>47. 제46항에 있어서,상기 변환을 여러 횟수로 반복 적용하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>48. 손실 비디오의 인코딩, 전송 및 디코딩을 위한 방법으로서,제1 컴퓨터 시스템에서 입력 프레임과 이전 프레임을 수신하는 단계;제1 학습 신경망을 사용해 상기 입력 프레임과 상기 이전 프레임을 기반으로 한 입력을 인코딩하여 제1 잠재 표현을 생성하는 단계;상기 제1 및 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;제2 학습 신경망을 사용해 상기 제1 잠재 표현을 디코딩하고 상기 입력 프레임과 이전 프레임 간의 출력 플로우 맵을 획득하는 단계; 및상기 출력 플로우 맵을 사용하여 출력 프레임을 생성하는 단계로서, 상기 출력 프레임은 상기 입력 프레임의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>49. 손실 비디오의 인코딩, 전송 및 디코딩에 사용하기 위한 하나 이상의 네트워크를 학습시키는 방법으로서,제1 컴퓨터 시스템에서 입력 프레임과 이전 프레임을 수신하는 단계;제1 신경망을 사용해 상기 입력 프레임 및 상기 이전 프레임을 기반으로 한 입력을 인코딩하여 제 1 잠재 표현을 생성하는 단계;제2 신경망을 사용해 상기 입력 프레임을 인코딩하여 제2 잠재 표현을 생성하는 단계;제3 신경망을 사용해 상기 제1 잠재 표현을 디코딩하여 상기 입력 프레임 및 상기 이전 프레임 간의 출력 플로우 맵을 얻는 단계;제4 신경망을 사용해 상기 제2 잠재 표현 및 상기 출력 플로우 맵 기반의 입력을 디코딩하여 출력 프레임을 생성하는 단계로서, 상기 출력 프레임은 상기 입력 프레임의 근사값인 단계;상기 출력 프레임과 입력 프레임 간의 차이에 기반한 함수를 평가하는 단계;상기 평가된 차이에 기반하여 제1, 제2, 제3, 제4 신경망 중 적어도 하나 이상의 신경망의 파라미터를 업데이트하는 단계; 및제1 입력 이미지 세트를 사용해 상기 단계를 반복 수행함으로써 제1, 제2, 제3, 제4 학습 신경망을 생성하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>50. 제49항에 있어서,상기 함수는 출력 플로우 맵 및 입력 프레임과 이전 프레임 간의 플로우 맵 간 차이에 추가적으로 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>51. 제49항 또는 제50항에 있어서,상기 출력 플로우 맵을 이용하여 이전 프레임에 해당하는, 이전에 디코딩된 프레임으로부터 출력 워핑된 프레임을 획득하며;상기 함수는 상기 출력 워핑된 프레임과 입력 프레임 간의 차이에 추가적으로 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>52. 제51항에 있어서,출력 플로우 맵과 잠재 플로우 맵 간의 차이가 함수에 기여하는 중요도에 제1 가중치가 적용되며, 출력 워핑된 프레임과 입력 프레임 간의 차이가 함수에 기여하는 중요도에 제2 가중치가 적용되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>53. 제52항에 있어서,제1 가중치 및 제2 가중치 중 적어도 하나는 반복 수행되는 단계 중 적어도 하나 이상의 단계에서 변경되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>54. 제53항에 있어서,제2 가중치는 반복 수행되는 단계 중 적어도 하나 이상의 단계에서 증가하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>55. 손실 비디오의 인코딩 및 전송을 위한 방법으로서,제1 컴퓨터 시스템에서 입력 프레임과 이전 프레임을 수신하는 단계;제1 학습 신경망을 사용해 상기 입력 프레임과 상기 이전 프레임을 기반으로 한 입력을 인코딩하여 제1 잠재 표현을 생성하는 단계;제2 학습 신경망을 사용해 상기 입력 프레임을 인코딩하여 제2 잠재 표현을 생성하는 단계; 및상기 제1 및 제2 잠재 표현을 전송하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>56. 손실 비디오의 수신 및 디코딩을 위한 방법으로서,제55항의 방법에 따라 전송된 제1 및 제2 잠재 표현을 제2 컴퓨터 시스템에서 수신하는 단계; 제3 학습된 신경망을 사용해 상기 제1 잠재 표현을 디코딩하여 입력 프레임과 이전 프레임 간의 출력 플로우 맵을 획득하는 단계; 및제4 학습된 신경망을 사용해 상기 제2 잠재 표현과 상기 출력 플로우 맵을 기반으로 한 입력을 디코딩하여 출력 프레임을 생성하는 단계로서, 여기서 출력 프레임은 상기 입력 프레임의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>57. 제1항 내지 제54항 중 어느 한 항의 방법을 수행하도록 구성된 데이터 처리 시스템.</claim></claimInfo><claimInfo><claim>58. 제55항 또는 제56항의 방법을 수행하도록 구성된 데이터 처리 장치.</claim></claimInfo><claimInfo><claim>59. 컴퓨터가 해당 프로그램을 실행할 때 제55항 또는 제56항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>60. 컴퓨터에 의해 실행되었을 때 제55항 또는 제56항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 판독가능 저장매체.</claim></claimInfo><claimInfo><claim>61. 손실 비디오의 인코딩, 전송 및 디코딩을 위한 방법으로서,제1 컴퓨터 시스템에서 제1 프레임, 제2 프레임 및 제3 프레임을 수신하는 단계;상기 제1 프레임과 제2 프레임 사이의 제1 플로우를 결정하는 단계;상기 제2 프레임과 제3 프레임 사이의 제2 플로우를 결정하는 단계;상기 제1 플로우 및 제2 플로우를 제1 학습 신경망을 사용해 인코딩하여 잠재 표현을 생성하는 단계;상기 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;상기 잠재 표현을 제2 학습 신경망을 사용해 디코딩하여 제1 출력 플로우 및 제2 출력 플로우를 생성하는 단계; 및상기 제1 출력 플로우 및 제2 출력 플로우를 이용하여 출력 프레임을 획득하되, 여기서 출력 프레임은 상기 제2 프레임의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>62. 제61항에 있어서,추가적으로, 제3 학습 신경망을 사용해 상기 제2 프레임을 인코딩하여 잠재 프레임 표현을 생성하는 단계;상기 잠재 프레임 표현을 제2 컴퓨터 시스템으로 추가 전송하는 단계; 및제4 학습 신경망을 사용해 상기 잠재 프레임 표현을 디코딩하여 출력 프레임을 생성하는 단계;를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>63. 제62항에 있어서,상기 제1 플로우를 이용해 제1 프레임에 해당하는 이전 디코딩된 프레임을 워핑하여 제1 워핑된 프레임을 생성하고; 상기 제2 플로우를 이용해 제3 프레임에 해당하는 이전 디코딩된 프레임을 워핑하여 제3 워핑된 프레임을 생성하며;상기 제1 워핑된 프레임 및 제3 워핑된 프레임은 제3 학습 신경망에 의해 추가로 인코딩되어 잠재 프레임 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>64. 제63항에 있어서,상기 제1 워핑된 프레임 및 제3 워핑된 프레임은 제4 학습 신경망에 의해 추가로 디코딩되어 출력 프레임을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>65. 제61항 내지 64항 중 어느 한 항에 있어서,결합 플로우를 획득하는 단계를 더 포함하며;여기서 결합 플로우는 제1 출력 플로우 및 제2 출력 플로우를 기반으로 하고;상기 결합 플로우는 출력 프레임을 생성하는 데 사용되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>66. 제64항 또는 제65항에 있어서,제1 출력 플로우에 제1 가중치를 적용하고, 제2 출력 플로우에 제2 가중치를 적용하여 상기 결합 플로우를 획득하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>67. 제63항 내지 제66항 중 어느 한 항에 있어서,결합 워핑된 프레임을 획득하는 단계를 더 포함하며;여기서 결합 워핑된 프레임은 제1 워핑된 프레임 및 제3 워핑된 프레임을 기반으로 하고;상기 결합 워핑된 프레임을 사용하여 출력 프레임을 획득하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>68. 제67항에 있어서,제1 워핑된 프레임에 제1 가중치를 적용하고, 제3 워핑된 프레임에 제2 가중치를 적용하여 상기 결합 워핑된 프레임을 획득하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>69. 제61항 내지 제68항 중 어느 한 항에 있어서,제1 프레임과 제3 프레임 간의 제3 플로우를 결정하는 단계를 더 포함하며;여기서 제3 플로우를 기반으로 한 입력은 제1 학습 신경망에 의해 추가적으로 인코딩되어 잠재 표현을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>70. 제62항 내지 제69항 중 어느 한 항에 있어서,상기 제3 플로우를 기반으로 한 입력은 제3 학습 신경망에 의해 추가적으로 인코딩되어 잠재 프레임 표현을 획득하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>71. 제62항 내지 제70항 중 어느 한 항에 있어서,상기 제3 플로우를 기반으로 한 입력은 제4 학습 신경망에 의해 추가적으로 디코딩되어 출력 프레임을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>72. 제59항 내지 제71항 중 어느 한 항에 있어서,상기 제3 플로우를 기반으로 한 입력은 제3 플로우에 상응하는 제3 출력 플로우일 수 있으며, 상기 제3 출력 플로우는 제2 학습 신경망에 의해 이전에 디코딩되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>73. 손실 비디오의 인코딩, 전송 및 디코딩에 사용하기 위한 하나 이상의 네트워크를 학습시키는 방법으로서:제1 컴퓨터 시스템에서 제1 프레임, 제2 프레임 및 제3 프레임을 수신하는 단계;상기 제1 프레임과 제2 프레임 사이의 제1 플로우를 결정하는 단계;상기 제2 프레임과 제3 프레임 사이의 제2 플로우를 결정하는 단계;상기 제1 플로우 및 제2 플로우를 제1 학습 신경망을 사용해 인코딩하여 잠재 표현을 생성하는 단계;상기 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;상기 잠재 표현을 제2 학습 신경망을 사용해 디코딩하여 제1 출력 플로우(first output flow) 및 제2 출력 플로우(second output flow)를 생성하는 단계;상기 제1 출력 플로우 및 제2 출력 플로우를 이용하여 출력 프레임을 획득하되, 여기서 출력 프레임은 상기 제2 프레임의 근사값인 것을 특징으로 하는 단계;상기 출력 프레임과 제2 프레임 간의 차이에 기반한 함수를 평가하는 단계;상기 평가된 함수에 따라 제1 신경망 및 제2 신경망의 파라미터를 업데이트하는 단계; 및제1 입력 프레임 세트(first set of input frames)를 이용하여 상기 단계를 반복 수행함으로써, 제1 학습 신경망 및 제2 학습 신경망을 생성하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>74. 제73항에 있어서,결합 플로우를 획득하는 단계로서, 상기 결합 플로우는 제1 출력 플로우 및 제2 출력 플로우를 기반으로 하는 단계;제1 출력 플로우에 제1 가중치를 적용하고 제2 출력 플로우에 제2 가중치를 적용하여 결합 플로우를 획득하는 단계; 및상기 결합 플로우를 사용하여 출력 프레임을 획득하는 단계를 더 포함하며;여기서 상기 제1 가중치 및 제2 가중치는 상기 평가된 차이를 기반으로 더 업데이트되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>75. 손실 비디오의 인코딩 및 전송을 위한 방법으로서,제1 컴퓨터 시스템에서 제1 프레임, 제2 프레임 및 제3 프레임을 수신하는 단계;상기 제1 프레임과 제2 프레임 사이의 제1 플로우를 결정하는 단계;상기 제2 프레임과 제3 프레임 사이의 제2 플로우를 결정하는 단계;상기 제1 플로우 및 제2 플로우를 제1 학습 신경망을 사용해 인코딩하여 잠재 표현을 생성하는 단계; 및상기 잠재 표현을 전송하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>76. 손실 비디오의 수신 및 디코딩을 위한 방법으로서,제75항의 방법에 따라 전송된 잠재 표현을 제2 컴퓨터 시스템에서 수신하는 단계; 상기 잠재 표현을 제2 학습 신경망을 사용해 디코딩하여 제1 출력 플로우 및 제2 출력 플로우를 생성하는 단계; 및상기 제1 출력 플로우 및 제2 출력 플로우를 이용하여 출력 프레임을 획득하는 단계로서, 여기서 출력 프레임은 상기 제2 프레임의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>77. 제61항 내지 제74항 중 어느 한 항의 방법을 수행하도록 구성된 데이터 처리 시스템.</claim></claimInfo><claimInfo><claim>78. 제75항 또는 제76항의 방법을 수행하도록 구성된 데이터 처리 장치.</claim></claimInfo><claimInfo><claim>79. 컴퓨터가 해당 프로그램을 실행할 때 제75항 또는 제76항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>80. 컴퓨터에 의해 실행되었을 때 제75항 또는 제76항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 판독가능 저장매체.</claim></claimInfo><claimInfo><claim>81. 손실 비디오의 인코딩, 전송 및 디코딩을 위한 방법에 있어서,제1 컴퓨터 시스템에서 제1 프레임 및 제2 프레임을 수신하는 단계;상기 제1 프레임과 제2 프레임 사이의 제1 플로우를 결정하는 단계;상기 제1 프레임과 제2 프레임을 기반으로 한 제2 플로우를 결정하는 단계;제1 학습 신경망을 사용해 상기 제1 플로우 및 제2 플로우를 기반으로 한 입력을 인코딩하여 잠재 표현을 생성하는 단계;상기 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;상기 잠재 표현을 제2 학습 신경망을 사용해 디코딩하여 출력 플로우를 생성하는 단계; 및상기 출력 플로우를 이용하여 출력 프레임을 획득하되, 여기서 출력 프레임은 상기 제2 프레임의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>82. 제81항에 있어서,제1 플로우를 적용해 제1 프레임을 변환하여 워핑된 프레임을 획득하는 단계를 더 포함하며;여기서 제2 플로우는 상기 워핑된 프레임과 제2 프레임 사이에서 결정되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>83. 제82항에 있어서,제1 플로우 및 제2 플로우를 기반으로 한 입력은 제2 플로우인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>84. 제82항 또는 제83항에 있어서,출력 프레임을 획득하는 단계에, 제1 프레임에 해당하는 프레임에 출력 플로우를 적용하여 중간 프레임을 획득하는 단계가 포함되는 방법.</claim></claimInfo><claimInfo><claim>85. 제84항에 있어서,출력 프레임을 획득하는 단계에, 상기 중간 프레임에 제1 플로우를 적용하여 출력 프레임을 획득하는 단계가 포함되는 방법.</claim></claimInfo><claimInfo><claim>86. 제81항에 있어서,제2 플로우는 포인트별 플로우이며, 제1 플로우 및 제2 플로우를 기반으로 한 입력은 제2 플로우에서 제1 플로우를 뺀 값인 방법.</claim></claimInfo><claimInfo><claim>87. 제86항에 있어서,출력 프레임을 획득하는 단계에, 상기 출력 플로우에 제1 플로우를 더함으로써 재구성된 플로우를 획득하는 단계가 포함되는 방법.</claim></claimInfo><claimInfo><claim>88. 제87항에 있어서,출력 프레임을 획득하는 단계에, 제1 프레임에 해당하는 프레임에 상기 재구성된 플로우를 적용하여 출력 프레임을 획득하는 단계가 포함되는 방법.</claim></claimInfo><claimInfo><claim>89. 제82항 또는 제83항에 있어서,제1 플로우 및 제2 플로우를 기반으로 한 입력은 제1 플로우 및 제2 플로우인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>90. 제89항에 있어서,출력 프레임을 획득하는 단계에, 제1 프레임에 해당하는 프레임에 출력 플로우를 적용하여 출력 프레임을 획득하는 단계가 포함되는 방법.</claim></claimInfo><claimInfo><claim>91. 제81항 내지 제90항 중 어느 한 항에 있어서,제1 프레임 및 제2 프레임 각각을 그에 대응되는 복수의 서브 프레임으로 분할하는 단계를 더 포함하며; 각각의 대응 서브 프레임에 대해 제1 플로우가 결정되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>92. 제81항 내지 제91항 중 어느 한 항에 있어서,상기 제1 플로우는 선형 함수인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>93. 제92항에 있어서,상기 제1 플로우가 다음 중 하나의 방법으로 결정되는 방법:제1 프레임과 제2 프레임을 입력으로 수신하여 제1 플로우를 정의하는 행렬의 파라미터 하나 이상을 출력하는 신경망, 선형 최소제곱법(linear least squares fit), 푸리에-멜린 변환(Fourier-Mellin transformation) 또는 제1 프레임과 제2 프레임 간의 광학적 오차(photometric error)를 최소화하는 최적화 방법.</claim></claimInfo><claimInfo><claim>94. 제81항 내지 제91항 중 어느 한 항에 있어서,상기 제1 플로우가 비선형 함수인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>95. 제94항에 있어서,상기 제1 플로우가 프레임 변환을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>96. 제94항 또는 제95항에 있어서,비선형 함수는 학습된 신경망인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>97. 제81항 내지 제96항 중 어느 한 항에 있어서,상기 제1 플로우가 글로벌 플로우인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>98. 제81항 내지 제97항 중 어느 한 항에 있어서,상기 제2 플로우가 로컬 플로우인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>99. 손실 비디오의 인코딩, 전송 및 디코딩에 사용하기 위한 하나 이상의 네트워크를 학습시키는 방법으로서,제1 컴퓨터 시스템에서 제1 프레임 및 제2 프레임을 수신하는 단계;상기 제1 프레임과 제2 프레임 사이의 제1 플로우를 결정하는 단계;상기 제1 프레임과 제2 프레임을 기반으로 한 제2 플로우를 결정하는 단계;제1 신경망을 사용해 상기 제1 플로우 및 제2 플로우에 기반한 입력을 인코딩하여 잠재 표현을 생성하는 단계;상기 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;상기 잠재 표현을 제2 학습 신경망을 사용해 디코딩하여 출력 플로우를 생성하는 단계; 상기 출력 플로우를 이용하여 출력 프레임을 획득하는 단계로서, 여기서 출력 프레임은 상기 제2 프레임의 근사값인 단계;상기 출력 프레임과 제2 프레임 간의 차이에 기반한 함수를 평가하는 단계;상기 평가된 함수에 따라 제1 신경망 및 제2 신경망의 파라미터를 업데이트하는 단계; 및제1 입력 프레임 세트를 이용하여 상기 단계를 반복 수행함으로써, 제1 학습 신경망 및 제2 학습 신경망을 생성하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>100. 손실 비디오의 인코딩 및 전송을 위한 방법으로서,제1 컴퓨터 시스템에서 제1 프레임 및 제2 프레임을 수신하는 단계;상기 제1 프레임과 제2 프레임 사이의 제1 플로우를 결정하는 단계;상기 제1 프레임과 제2 프레임을 기반으로 한 제2 플로우를 결정하는 단계;제1 학습 신경망을 사용해 상기 제1 플로우 및 제2 플로우에 기반한 입력을 인코딩하여 잠재 표현을 생성하는 단계; 및상기 잠재 표현을 전송하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>101. 손실 비디오의 수신 및 디코딩을 위한 방법으로서,제100항의 방법에 따라 전송된 상기 잠재 표현을 제2 컴퓨터 시스템에서 수신하는 단계; 제2 학습 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 플로우를 생성하는 단계; 및 상기 출력 플로우를 사용하여 출력 프레임을 획득하는 단계로서, 여기서 출력 프레임은 제2 프레임의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>102. 제81항 내지 제99항 중 어느 한 항의 방법을 수행하도록 구성된 데이터 처리 시스템.</claim></claimInfo><claimInfo><claim>103. 제100항 또는 제101항의 방법을 수행하도록 구성된 데이터 처리 장치.</claim></claimInfo><claimInfo><claim>104. 컴퓨터가 해당 프로그램을 실행할 때 제100항 또는 제101항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>105. 컴퓨터에 의해 실행되었을 때 제100항 또는 제101항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 판독가능 저장매체.</claim></claimInfo><claimInfo><claim>106. 손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩에 사용하기 위한 방법으로서,제1 컴퓨터 시스템에서 입력 이미지 또는 비디오를 수신하는 단계;제1 학습 신경망을 사용해 상기 입력 이미지 또는 비디오에 기반한 입력을 변환하여 복수의 파라미터 수정값을 생성하는 단계;제2 학습 신경망을 사용해 상기 입력 이미지 또는 비디오를 인코딩하여 잠재 표현을 생성하는 단계;상기 파라미터 수정값과 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;상기 복수의 파라미터 수정값을 사용해 제3 학습 신경망의 파라미터 중 적어도 하나 이상을 수정하는 단계; 및상기 제3 학습 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 이미지 또는 비디오를 생성하되, 여기서 출력 이미지 또는 비디오는 상기 입력 이미지 또는 비디오의 근사값인 것을 특징으로 하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>107. 제106항에 있어서,입력 이미지 또는 비디오를 인코딩하기 이전에, 상기 복수의 파라미터 수정값을 사용하여 제2 학습 신경망의 파라미터 중 적어도 하나 이상을 수정하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>108. 제107항에 있어서,상기 복수의 파라미터 수정값이 제2 신경망에 대응하는 파라미터 수정값 및 제3 신경망에 대응하는 파라미터 수정값을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>109. 제106항 내지 제108항 중 어느 한 항에 있어서,제1 학습 신경망에 의해 변환된 입력 이미지 또는 비디오 기반의 입력은, 해당 입력 이미지 또는 비디오를 입력으로 수신하는 제4 학습 신경망의 출력인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>110. 제109항에 있어서,제4 학습 신경망이 글로벌 평균 풀링 계층을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>111. 제109항 내지 제110항 중 어느 한 항에 있어서,제4 학습 신경망이 생성하는 출력은 입력에 비해 다운샘플링된 형태인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>112. 제106항 내지 제111항 중 어느 한 항에 있어서,본 발명에 따른 제1 학습 신경망이 밀집 선형 변환(dense linear transformation), 희소 선형 변환(sparse linear transformation), 저순위 선형 변환(low-rank linear transformation) 및 다층 퍼셉트론 네트워크(multi-layer perceptron network) 중 적어도 하나 이상을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>113. 제106항 내지 제112항 중 어느 한 항에 있어서,제1 학습 신경망의 출력이 입력보다 더 많은 수의 요소를 가지는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>114. 제106항 내지 제113항 중 어느 한 항에 있어서,복수의 파라미터 수정값을 사용하여 제3 학습 신경망의 파라미터 하나 이상을 수정하는 단계에, 해당 파라미터 중 적어도 하나에 대해 복수의 파라미터 수정값 중 하나를 사용해 섭동하는 것이 포함되는 방법.</claim></claimInfo><claimInfo><claim>115. 제106항 내지 제114항 중 어느 한 항에 있어서,복수의 파라미터 수정값을 사용하여 제3 학습 신경망의 파라미터 하나 이상을 수정하는 단계에, 해당 파라미터 중 적어도 하나를 복수의 파라미터 수정값 중 하나로 교체하는 것이 포함되는 방법.</claim></claimInfo><claimInfo><claim>116. 제106항 내지 제114항 중 어느 한 항에 있어서,더 학습된 신경망을 사용해 입력 이미지 또는 비디오를 기반으로 한 입력을 변환하여 복수의 파라미터 수정값을 생성하는 단계를 더 포함하며;여기서 제1 학습 신경망은 복수의 파라미터 수정값의 제1 하위 집합을 출력하고, 상기 더 학습된 신경망은 복수의 파라미터 수정값의 제2 하위 집합을 출력하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>117. 제106항 내지 제116항 중 어느 한 항에 있어서,입력 이미지 또는 비디오가 입력 비디오인 경우, 해당 입력 비디오를 기반으로 한 입력은 해당 입력 비디오 중 적어도 두 프레임 이상을 기반으로 하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>118. 제117항에 있어서,제4 학습 신경망은 상기 입력 비디오 중 적어도 두 프레임 이상에 걸친 컨볼루션을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>119. 제117항에 있어서,상기 입력 비디오를 기반으로 한 입력은 해당 입력 비디오 중 적어도 두 프레임 이상을; 평균화, 순환 신경망, 컨볼루션 신경망 및 트랜스포머 신경망 방식 중 적어도 하나 이상의 방식으로 결합하여 획득하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>120. 제109항 내지 제116항 중 어느 한 항에 있어서,입력 이미지 또는 비디오가 입력 비디오인 경우, 해당 입력 비디오 중 적어도 두 프레임에 대해 제4 학습 신경망의 개별 출력이 획득되고, 각 출력은 적어도 두 프레임 중 하나의 프레임에 대응하며;입력 비디오를 기반으로 한 입력은 제4 학습 신경망의 개별 출력에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>121. 제119항에 있어서,상기 입력 비디오를 기반으로 한 입력은 제4 학습 신경망의 개별 출력을:평균화, 순환 신경망, 컨볼루션 신경망, 트랜스포머 신경망 방식 중 하나 이상의 방식으로 결합하여 획득하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>122. 제106항 내지 제115항 중 어느 한 항에 있어서,입력 이미지 또는 비디오가 입력 비디오인 경우, 해당 입력 비디오 중 적어도 두 프레임 이상에 대해 개별적인 복수의 파라미터 수정값이 획득되고;각각의 개별적인 복수의 파라미터 수정값은 제2 컴퓨터 시스템으로 전송되기 전에 결합되며;상기 결합된 파라미터 수정값은, 상기 입력 비디오 중 적어도 두 프레임 이상을 디코딩하기 전에 제3 신경망을 수정하는 데 사용되는 복수의 파라미터 수정값인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>123. 제122항에 있어서,상기 개별적인 복수의 파라미터 수정값은: 평균화, 순환 신경망, 컨볼루션 신경망 또는 트랜스포머 신경망 방식 중 하나 이상의 방식을 사용해 결합되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>124. 제106항 내지 제123항 중 어느 한 항에 있어서,제1 컴퓨터 시스템에서 상기 파라미터 수정값과 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하는 단계;및제2 컴퓨터 시스템에서 상기 비트스트림을 엔트로피 디코딩하여 파라미터 수정값과 잠재 표현을 복원하는 단계;를 더 포함하며,여기서 상기 파라미터 수정값은 제1 확률 분포를 사용하여 엔트로피 인코딩 및 디코딩되고,상기 잠재 표현은 제2 확률 분포를 사용하여 엔트로피 인코딩 및 디코딩되며,상기 제1 확률 분포는 제2 확률 분포와 서로 다른 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>125. 제124항에 있어서,상기 제1 확률 분포가 라플라스 분포 또는 스파이크 앤 슬랩 분포인 방법.</claim></claimInfo><claimInfo><claim>126. 제124항 또는 제125항 중 어느 한 항에 있어서,복수의 파라미터 수정값을 제5 학습 신경망을 사용해 인코딩하여 하이퍼 잠재 표현을 생성하는 단계;상기 하이퍼 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계; 및상기 하이퍼 잠재 표현을 제6 학습 신경망을 사용하여 디코딩하고, 엔트로피 디코딩 과정에서 제6 학습 신경망의 출력을 사용하여 파라미터 수정값을 복원하는 단계;를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>127. 제124항에 있어서,상기 제1 확률 분포는 복수의 확률 분포를 포함하며;상기 복수의 확률 분포 각각은 제3 학습 신경망의 파라미터 그룹에 대응하는 파라미터 수정값 그룹에 대응하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>128. 손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩에 사용하기 위한 하나 이상의 네트워크를 학습시키는 방법으로서,제1 컴퓨터 시스템에서 입력 이미지 또는 비디오를 수신하는 단계;제1 신경망을 사용해 상기 입력 이미지 또는 비디오 기반의 입력을 변환하여 복수의 파라미터 수정값을 생성하는 단계;제2 신경망을 사용해 상기 입력 이미지 또는 비디오를 인코딩하여 잠재 표현을 생성하는 단계;상기 파라미터 수정값과 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계;복수의 파라미터 수정값을 이용하여 제3 신경망의 하나 이상의 파라미터를 수정하고, 제3 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 이미지 또는 비디오를 생성하되, 여기서 출력 이미지 또는 비디오는 상기 입력 이미지 또는 비디오의 근사값인 단계;출력 이미지 또는 비디오와 입력 이미지 또는 비디오 간의 차이에 기반하여 함수를 평가하는 단계;상기 평가된 함수에 기반하여 제1 신경망의 파라미터를 업데이트하는 단계; 및제1 입력 이미지 또는 비디오 세트에 대해 상기 단계를 반복 수행함으로써 제1 학습 신경망을 생성하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>129. 제128항에 있어서,상기 함수는 추가적으로 상기 복수의 파라미터 수정값의 비율에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>130. 제128항 또는 제129항에 있어서,제1 컴퓨터 시스템에서 상기 파라미터 수정값과 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하는 단계; 및제2 컴퓨터 시스템에서 상기 비트스트림을 엔트로피 디코딩하여 파라미터 수정값과 잠재 표현을 복원하는 단계; 를 더 포함하며,여기서 상기 파라미터 수정값은 제1 확률 분포를 사용하여 엔트로피 인코딩 및 디코딩되고,상기 제1 확률 분포의 파라미터들은 평가된 함수를 기반으로 추가적으로 업데이트되어 학습된 파라미터를 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>131. 제128항 내지 제130항 중 어느 한 항에 있어서,제2 신경망 및 제3 신경망은 학습된 신경망인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>132. 제128항 내지 제130항 중 어느 한 항에 있어서,상기 제2 신경망과 제3 신경망의 파라미터는, 평가된 함수에 기반하여 추가적으로 업데이트되어, 각각 제2 학습 신경망 및 제3 학습 신경망을 생성하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>133. 손실 이미지 또는 비디오의 인코딩 및 전송을 위한 방법으로서,제1 컴퓨터 시스템에서 입력 이미지 또는 비디오를 수신하는 단계;제1 학습 신경망을 사용해 상기 입력 이미지 또는 비디오 기반의 입력을 변환하여 복수의 파라미터 수정값을 생성하는 단계;제2 학습 신경망을 사용해 상기 입력 이미지 또는 비디오를 인코딩하여 잠재 표현을 생성하는 단계;상기 파라미터 수정값과 잠재 표현을 전송하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>134. 손실 이미지 또는 비디오 수신 및 디코딩을 위한 방법으로서,제133항의 방법에 따라 전송된 파라미터 수정값 및 잠재 표현을 제2 컴퓨터 시스템에서 수신하는 단계; 및상기 복수의 파라미터 수정값을 사용하여 제3 학습 신경망의 파라미터 하나 이상을 수정하고, 제3 학습 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 이미지 또는 비디오를 생성하는 단계로서, 여기서 출력 이미지 또는 비디오는 입력 이미지 또는 비디오의 근사값인 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>135. 제106항 내지 제132항 중 어느 한 항의 방법을 수행하도록 구성된 데이터 처리 시스템.</claim></claimInfo><claimInfo><claim>136. 제133항 또는 제134항의 방법을 수행하도록 구성된 데이터 처리 장치.</claim></claimInfo><claimInfo><claim>137. 컴퓨터가 해당 프로그램을 실행할 때 제133항 또는 제134항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>138. 컴퓨터에 의해 실행되었을 때 제133항 또는 제134항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 판독가능 저장매체.</claim></claimInfo><claimInfo><claim>139. 손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩에 사용하기 위한 하나 이상의 신경망을 학습시키는 방법으로서,제1 컴퓨터 시스템에서 입력 이미지를 수신하는 단계;제1 신경망을 사용해 상기 입력 이미지를 인코딩하여 잠재 표현을 생성하는 단계;제2 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 이미지를 생성하며, 여기서 출력 이미지는 상기 입력 이미지의 근사인 단계;상기 출력 이미지와 입력 이미지 간의 차이를 기반으로 함수를 평가하는 단계;상기 평가된 함수를 기반으로 제1 신경망 및 제2 신경망의 파라미터를 업데이트하는 단계; 및제1 입력 이미지 세트에 대해 상기 단계를 반복 수행함으로써 제1 학습 신경망 및 제2 학습 신경망을 생성하는 단계; 를 포함하며,여기서 출력 이미지와 입력 이미지 간의 차이는 판별기로 작동하는 신경망의 출력을 기준으로 결정되고;판별기 역할을 하는 신경망의 파라미터도 상기 평가된 함수에 기반하여 추가로 업데이트되며;판별기로 작동하는 신경망의 파라미터는 제1 학습률로 업데이트되고;여기서 판별기 신경망의 파라미터가 적어도 한 번 이상 업데이트된 이후, 상기 제1 학습률이 업데이트되며;해당 학습률의 업데이트는 판별기 신경망의 출력 오차에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>140. 제139항에 있어서,상기 오차는 판별기로 작동하는 신경망의 출력과 목표값 간의 차이에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>141. 제140항에 있어서,상기 목표값은 미리 결정되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>142. 제140항에 있어서,상기 목표값은 학습 단계의 반복 수행 중 적어도 한 번 이상 수정되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>143. 제139항 내지 제142항 중 어느 한 항에 있어서,상기 제1 학습률의 업데이트는 오차를 포함한 계산에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>144. 제139항 내지 제143항 중 어느 한 항에 있어서,상기 제1 학습률의 업데이트는 오차의 적분값을 포함한 계산에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>145. 제139항 내지 제144항 중 어느 한 항에 있어서,상기 제1 학습률의 업데이트는 오차의 미분값을 포함한 계산에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>146. 제141항 내지 제145항 중 어느 한 항에 있어서,오차, 오차의 적분 또는 오차의 미분 중 적어도 하나의 기여도는 미리 정해진 값에 의해 가중 조정되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>147. 제139항 내지 제144항 중 어느 한 항에 있어서,제1 학습률의 업데이트는 추가적으로 학습 단계 중 적어도 하나 이상의 이전 반복에서의 제1 학습률 업데이트에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>148. 제147항에 있어서,제1 학습률에 대한 업데이트는 판별기로 작용하는 신경망의 출력 오차에 기반하여 제1 학습률의 현재 수정값을 결정하는 단계를 포함하며;제1 학습률에 대한 업데이트는, 제1 학습률의 현재 수정값과 학습 단계 중 적어도 하나 이상의 이전 반복에서의 제1 학습률 업데이트의 평균에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>149. 제148항에 있어서,상기 평균이 지수 이동 평균인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>150. 제139항 내지 147항 중 어느 한 항에 있어서,제1 학습률에 대한 업데이트는, 판별기로 작용하는 신경망의 출력 오차를 입력으로 수신하는 경사 하강법 알고리즘의 출력에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>151. 제150항에 있어서,경사 하강법 알고리즘이: SGD, Adagrand, RMSProp 또는 Adam 중 하나인 방법.</claim></claimInfo><claimInfo><claim>152. 손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩에 사용하기 위한 하나 이상의 신경망을 학습시키는 방법으로서,제1 컴퓨터 시스템에서 입력 이미지를 수신하는 단계; 제1 신경망을 사용해 상기 입력 이미지를 인코딩하여 잠재 표현을 생성하는 단계; 제2 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 이미지를 생성하며, 여기서 출력 이미지는 상기 입력 이미지의 근사인 단계; 상기 출력 이미지와 입력 이미지 간의 차이 및 잠재 표현의 비율에 기반한 함수를 평가하는 단계;상기 평가된 함수를 기반으로 제1 신경망 및 제2 신경망의 파라미터를 업데이트하는 단계; 및제1 입력 이미지 세트에 대해 상기 단계를 반복 수행함으로써 제1 학습 신경망 및 제2 학습 신경망을 생성하는 단계;를 포함하며,여기서 출력 이미지와 입력 이미지 사이의 차이 및 잠재 표현의 비율 중 적어도 하나가 함수에 기여하는 정도는 제1 파라미터에 의해 스케일 조정되고; 제1 신경망 및 제2 신경망의 파라미터가 적어도 한 번 이상 업데이트된 후, 제1 파라미터가 업데이트되며; 제1 파라미터의 업데이트는 출력 이미지와 입력 이미지 간의 차이 및 잠재 표현의 비율 중 적어도 하나에 대한 오차에 기반하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>153. 손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩 방법으로서, 제1 컴퓨터 시스템에서 입력 이미지를 수신하는 단계; 제1 학습된 신경망을 사용해 상기 입력 이미지를 인코딩하여 잠재 표현을 생성하는 단계;상기 잠재 표현을 제2 컴퓨터 시스템으로 전송하는 단계; 및제2 학습된 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 이미지를 생성하는 단계로서, 여기서 출력 이미지는 상기 입력 이미지의 근사인 단계;를 포함하며,여기서 제1 학습된 신경망 및 제2 학습된 신경망 중 적어도 하나는 제139항 내지 제152항 중 어느 한 항의 방법에 따라 학습된 신경망인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>154. 손실 이미지 또는 비디오의 인코딩 및 전송 방법으로서, 제139 컴퓨터 시스템에서 입력 이미지를 수신하는 단계; 제1 훈련된 신경망을 사용해 상기 입력 이미지를 인코딩하여 잠재 표현을 생성하는 단계; 및 상기 잠재 표현을 전송하는 단계;를 포함하며, 상기 제1 훈련된 신경망은 제1항 내지 제152항 중 어느 한 항의 방법에 따라 훈련되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>155. 손실 이미지 또는 비디오의 수신 및 디코딩 방법으로서, 제152항의 방법에 따라 전송된 잠재 표현을 제2 컴퓨터 시스템에서 수신하는 단계; 및 제2 훈련된 신경망을 사용해 상기 잠재 표현을 디코딩하여 출력 이미지를 생성하되, 여기서 출력 이미지는 상기 입력 이미지의 근사인 단계;를 포함하며, 상기 제2 훈련된 신경망은 제139항 내지 제152항 중 어느 한 항의 방법에 따라 훈련된 신경망임을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>156. 제139항 내지 제153항 중 어느 한 항의 방법을 수행하도록 구성된 데이터 처리 시스템.</claim></claimInfo><claimInfo><claim>157. 제154항 또는 제155항의 방법을 수행하도록 구성된 데이터 처리 장치.</claim></claimInfo><claimInfo><claim>158. 컴퓨터가 해당 프로그램을 실행할 때 제154항 또는 제155항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>159. 컴퓨터에 의해 실행되었을 때 제154항 또는 제155항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 판독가능 저장매체.</claim></claimInfo><claimInfo><claim>160. 손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩 방법으로서,제1 컴퓨터 시스템에서 입력 이미지를 수신하는 단계;상기 입력 이미지를 제1 학습 신경망을 사용해 인코딩하여 잠재 표현을 생성하는 단계로서, 상기 잠재 표현은 적어도 하나 이상의 확률 파라미터로 설명되는 확률 분포를 갖는 단계;상기 잠재 표현을 복수의 서브 잠재 표현(sub-latent representation)으로 분할하는 단계로서, 각각의 서브 잠재 표현은 적어도 하나 이상의 서브 확률 파라미터로 설명되는 서브 확률 분포를 갖는 단계;상기 복수의 서브 잠재 표현을 적어도 하나 이상의 서브 확률 파라미터를 이용해 엔트로피 인코딩하여 비트스트림을 생성하는 단계;상기 비트스트림을 제2 컴퓨터 시스템으로 전송하는 단계;상기 비트스트림을 복수의 서브 확률 파라미터를 이용해 엔트로피 디코딩하여 복수의 서브 잠재 표현을 복원하고, 복수의 서브 잠재 표현을 결합하여 원래의 잠재 표현을 복원하는 단계; 및상기 잠재 표현을 제2 학습 신경망을 사용해 디코딩하여 출력 이미지를 생성하는 단계로서, 상기 출력 이미지는 입력 이미지의 근사값인 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>161. 제160항에 있어서,복수의 서브 잠재 표현 중 적어도 하나는 다른 서브 잠재 표현과 요소(element)의 수가 다른 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>162. 제160항 또는 제161항에 있어서,각 서브 잠재 표현의 요소는, 각 서브 잠재 표현의 엔트로피 인코딩이 동일한 계산 복잡도를 갖도록 그 수가 선택되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>163. 제160항 내지 제162항에 있어서,상기 잠재 표현을 복수의 서브 잠재 표현으로 분할한 후, 각 서브 잠재 표현에 태그를 추가하는 단계로서, 상기 태그는 잠재 표현 내에서 해당 서브 잠재 표현의 위치를 나타내는 단계; 및상기 태그를 사용해 복수의 서브 잠재 표현을 결합하여 원래의 잠재 표현을 복원하는 단계;를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>164. 제160항 내지 제163항에 있어서,상기 비트스트림이 복수의 서브 비트스트림을 포함하며, 각각의 서브 비트스트림은 복수의 서브 잠재 표현 중 하나에 대응하는 것을 특징으로 하는 방법으로, 추가적으로 각 서브 비트스트림에 태그를 추가하되, 상기 태그가 각 서브 비트스트림의 시작 위치를 나타내는 단계를 더 포함하며;여기서 비트스트림을 제2 컴퓨터 시스템에서 수신한 후, 상기 시작 위치를 나타내는 태그를 사용하여 비트스트림을 분할하고 각 서브 비트스트림을 복원하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>165. 제160항 내지 제164항 중 어느 한 항에 있어서,복수의 서브 잠재 표현 중 적어도 두 개 이상은 병렬로 인코딩되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>166. 제160항 내지 제165항 중 어느 한 항에 있어서,복수의 서브 잠재 표현 중 적어도 두 개 이상은 비트스트림의 디코딩을 통해 병렬로 복원되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>167. 제160항 내지 제166항 중 어느 한 항에 있어서,상기한 복수의 서브 잠재 표현을 적어도 하나 이상의 서브 확률 파라미터를 사용해 엔트로피 인코딩하여 비트스트림을 생성하는 단계는: 복수의 서브 잠재 표현 각각을 엔트로피 인코딩하여 복수의 상태(state) 및 복수의 서브 비트스트림을 획득하는 단계; 상기 복수의 상태를 엔트로피 인코딩하는 단계; 및 복수의 서브 비트스트림과 엔트로피 인코딩된 복수의 상태를 결합하여 비트스트림을 생성하는 단계를 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>168. 제160항 내지 제167항 중 어느 한 항에 있어서,제3 학습된 신경망을 사용해 잠재 표현을 인코딩하여 하이퍼 잠재 표현을 생성하는 단계;상기 하이퍼 잠재 표현을 엔트로피 인코딩하고, 해당 엔트로피 인코딩된 하이퍼 잠재 표현을 비트스트림에 포함시키는 단계;제2 컴퓨터 시스템에서, 상기 엔트로피 인코딩된 하이퍼 잠재 표현을 엔트로피 디코딩하여 하이퍼 잠재 표현을 복원하는 단계; 및상기 하이퍼 잠재 표현을 제4 학습된 신경망을 사용해 디코딩하는 단계로서, 상기 제4 학습된 신경망의 출력은 출력 이미지를 획득하는 데 사용되는 단계;를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>169. 제168항에 있어서,상기한 하이퍼 잠재 표현을 엔트로피 인코딩하는 단계는: 하이퍼 잠재 표현을 복수의 서브 하이퍼 잠재 표현으로 분할하는 단계로서, 각각의 서브 하이퍼 잠재 표현은 적어도 하나 이상의 서브 확률 파라미터로 기술되는 서브 확률 분포를 갖는 단계; 및 복수의 서브 하이퍼 잠재 표현을 적어도 하나 이상의 서브 확률 파라미터를 사용하여 엔트로피 인코딩하는 단계;를 포함하며,하이퍼 잠재 표현의 엔트로피 인코딩된 데이터를 디코딩하는 단계는: 적어도 하나 이상의 서브 확률 파라미터를 사용해 엔트로피 인코딩된 하이퍼 잠재 표현을 엔트로피 디코딩하여 복수의 서브 하이퍼 잠재 표현을 복원하고, 상기 복수의 서브 하이퍼 잠재 표현을 결합하여 하이퍼 잠재 표현을 복원하는 단계를 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>170. 제160항 내지 제169항 중 어느 한 항에 있어서,상기한 적어도 하나 이상의 서브 확률 파라미터를 사용해 복수의 서브 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하는 단계는: 복수의 서브 잠재 표현 각각을 엔트로피 인코딩하여 복수의 상태(state) 정보 및 복수의 서브 비트스트림을 획득하는 단계; 상기 복수의 상태를 엔트로피 디코딩하여 상태 잠재를 획득하는 단계; 상기 상태 잠재를 엔트로피 인코딩하여 상태 비트스트림을 획득하는 단계; 및 상기 복수의 서브 비트스트림과 상기 상태 비트스트림을 결합하여 비트스트림을 획득하는 단계;를 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>171. 제170항에 있어서,상태 잠재 표현을 엔트로피 인코딩하여 상태 비트스트림을 생성하기 이전에 상태 잠재 표현에 대해 손실 또는 무손실 압축 프로세스를 수행하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>172. 제169항 내지 제171항 중 어느 한 항에 있어서,상기한 적어도 하나 이상의 서브 확률 파라미터를 사용해 복수의 서브 하이퍼 잠재 표현을 엔트로피 인코딩하는 단계는: 복수의 서브 하이퍼 잠재 표현 각각을 엔트로피 인코딩하여 복수의 하이퍼 상태 및 복수의 서브 하이퍼 비트스트림을 획득하는 단계; 상기 복수의 하이퍼 상태를 엔트로피 디코딩하여 하이퍼 상태 잠재를 획득하는 단계; 상기 하이퍼 상태 잠재를 엔트로피 인코딩하여 하이퍼 상태 비트스트림을 획득하는 단계; 및상기 복수의 서브 하이퍼 비트스트림과 상기 하이퍼 상태 비트스트림을 결합하여 비트스트림을 획득하는 단계;를 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>173. 제172항에 있어서,하이퍼 상태 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하기 이전에 하이퍼 상태 잠재 표현에 대해 손실 또는 무손실 압축 프로세스를 수행하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>174. 제169항에 있어서,상기한 적어도 하나 이상의 서브 확률 파라미터를 사용해 복수의 서브 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하는 단계는: 복수의 서브 잠재 표현 각각을 엔트로피 인코딩하여 복수의 상태 및 복수의 서브 비트스트림을 획득하는 단계; 및 상기 복수의 상태를 엔트로피 인코딩하하여 상태 잠재 표현을 획득하는 단계; 를 포함하며,상기한 적어도 하나 이상의 서브 확률 파라미터를 사용해 복수의 서브 하이퍼 잠재 표현을 엔트로피 인코딩하는 단계는: 복수의 서브 하이퍼 잠재 표현 각각을 엔트로피 인코딩하여 복수의 하이퍼 상태 및 복수의 서브 하이퍼 비트스트림을 획득하는 단계;및 상기 복수의 하이퍼 상태를 엔트로피 디코딩하여 하이퍼 상태 잠재 표현을 획득하는 단계; 를 포함하며,추가적으로, 상태 잠재 표현과 하이퍼 상태 잠재 표현을 결합하여 결합된 상태 잠재 표현을 생성하는 단계; 상기 결합된 상태 잠재 표현을 엔트로피 인코딩하여 결합된 상태 비트스트림을 획득하는 단계; 및복수의 서브 비트스트림과 상기 결합된 상태 비트스트림을 결합하여 비트스트림을 획득하는 단계;를 더 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>175. 제174항에 있어서,결합된 상태 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하기 이전에 결합된 상태 잠재 표현에 대해 손실 또는 무손실 압축 프로세스를 수행하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>176. 제1항 내지 제175항 중 어느 한 항에 있어서,제1 컴퓨터 시스템에서 추가 입력 이미지를 수신하는 단계로서, 해당 입력 이미지 및 추가 입력 이미지는 입력 비디오의 프레임인 단계;제1 학습된 신경망을 사용해 상기 추가 입력 이미지를 인코딩하여 추가 잠재 표현을 생성하는 단계로서, 상기 추가 잠재 표현은 적어도 하나 이상의 확률 파라미터로 기술되는 확률 분포를 갖는 단계;상기 추가 잠재 표현을 복수의 추가 서브 잠재 표현으로 분할하는 단계로서, 각각의 추가 서브 잠재 표현은 적어도 하나 이상의 서브 확률 파라미터로 기술되는 서브 확률 분포를 갖는 단계;상기 복수의 추가 서브 잠재 표현을 적어도 하나 이상의 서브 확률 파라미터를 사용해 엔트로피 인코딩하여 추가 비트스트림을 생성하는 단계;상기 추가 비트스트림을 제2 컴퓨터 시스템으로 전송하는 단계;상기 추가 비트스트림을 적어도 하나 이상의 서브 확률 파라미터를 사용해 엔트로피 디코딩하여 복수의 추가 서브 잠재 표현을 복원하고, 이를 결합하여 추가 잠재 표현을 복원하는 단계; 및상기 추가 잠재 표현을 제2 학습된 신경망을 사용해 디코딩하여 추가 출력 이미지를 생성하는 단계로서, 상기 추가 출력 이미지는 상기 추가 입력 이미지의 근사값인 단계;를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>177. 제170항에 종속될 때 제176항에 있어서,상기한 적어도 하나 이상의 서브 확률 파라미터를 사용해 복수의 추가 서브 잠재 표현을 엔트로피 인코딩하여 추가 비트스트림을 생성하는 단계는: 상기 복수의 추가 서브 잠재 표현 각각을 엔트로피 인코딩하여 복수의 추가 상태 및 복수의 추가 서브 비트스트림을 획득하는 단계;상기 복수의 추가 상태를 엔트로피 디코딩하여 추가 상태 잠재 표현을 획득하는 단계;상태 잠재 표현과 상기 추가 상태 잠재 표현을 결합하여 결합된 상태 잠재 표현을 획득하는 단계;상기 결합된 상태 잠재 표현을 엔트로피 인코딩하여 결합된 상태 비트스트림을 획득하는 단계; 및복수의 서브 비트스트림, 상기 복수의 추가 서브 비트스트림 및 상기 결합된 상태 비트스트림을 결합하여 비트스트림을 획득하는 단계;를 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>178. 비트스트림의 인코딩, 전송 및 디코딩을 위한 방법으로서,제1 컴퓨터 시스템에서 입력을 수신하는 단계로서, 상기 입력은 적어도 하나 이상의 확률 파라미터로 기술되는 확률 분포를 갖는 단계;상기 입력을 복수의 서브 입력으로 분할하는 단계로서, 각각의 서브 입력은 적어도 하나 이상의 서브 확률 파라미터로 기술되는 서브 확률 분포를 갖는 단계;상기 복수의 서브 입력을 적어도 하나 이상의 서브 확률 파라미터를 사용해 엔트로피 인코딩하여 비트스트림을 생성하는 단계;상기 비트스트림을 제2 컴퓨터 시스템으로 전송하는 단계; 및상기 비트스트림을 적어도 하나 이상의 서브 확률 파라미터를 사용해 엔트로피 디코딩하여 복수의 서브 입력을 복원하고, 이들을 결합하여 입력을 복원하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>179. 손실 이미지 또는 비디오의 인코딩 및 전송을 위한 방법으로서,제1 컴퓨터 시스템에서 입력 이미지를 수신하는 단계;상기 입력 이미지를 제1 학습 신경망을 사용해 인코딩하여 잠재 표현을 생성하는 단계로서, 상기 잠재 표현은 적어도 하나 이상의 확률 파라미터로 기술되는 확률 분포를 갖는 단계;상기 잠재 표현을 복수의 서브 잠재 표현으로 분할하는 단계로서, 각각의 서브 잠재 표현은 적어도 하나 이상의 서브 확률 파라미터로 기술되는 서브 확률 분포를 갖는 단계;상기 복수의 서브 잠재 표현을 적어도 하나 이상의 서브 확률 파라미터를 이용해 엔트로피 인코딩하여 비트스트림을 생성하는 단계; 및상기 비트스트림을 제2 컴퓨터 시스템으로 전송하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>180. 손실 이미지 또는 비디오 수신 및 디코딩을 위한 방법으로서, 제179항의 방법에 따라 전송된 비트스트림을 제2 컴퓨터 시스템에서 수신하는 단계; 적어도 하나 이상의 서브 확률 파라미터를 사용해 상기 비트스트림을 엔트로피 디코딩하여 복수의 서브 잠재 표현을 복원한 다음 이들을 결합하여 잠재 표현을 복원하는 단계; 및상기 잠재 표현을 제2 학습된 신경망을 사용해 디코딩하여 출력 이미지를 생성하는 단계로서, 상기 출력 이미지는 입력 이미지의 근사값인 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>181. 제160항 내지 제178항 중 어느 한 항의 방법을 수행하도록 구성된 데이터 처리 시스템.</claim></claimInfo><claimInfo><claim>182. 제160항 내지 제178항의 방법을 수행하도록 구성된 데이터 처리 장치.</claim></claimInfo><claimInfo><claim>183. 컴퓨터가 해당 프로그램을 실행할 때 제160항 내지 제178항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>184. 컴퓨터에 의해 실행되었을 때 제160항 내지 제178항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 판독가능 저장매체.</claim></claimInfo><claimInfo><claim>185. 손실 이미지 또는 비디오의 인코딩, 전송 및 디코딩을 위한 방법으로서,제1 컴퓨터 시스템에서 입력 이미지를 수신하는 단계;상기 입력 이미지를 제1 학습된 신경망을 사용해 인코딩하여 잠재 표현을 생성하는 단계로서, 상기 잠재 표현은 적어도 하나 이상의 확률 파라미터로 기술되는 확률 분포를 갖는 단계;잠재 표현 또는 적어도 하나 이상의 확률 파라미터 중 적어도 하나에 기반하여 마스크를 생성하는 단계;상기 마스크를 잠재 표현에 적용하여 마스킹된 잠재 표현을 생성하는 단계;적어도 하나 이상의 확률 파라미터를 사용해 상기 마스킹된 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하는 단계;상기 비트스트림을 제2 컴퓨터 시스템으로 전송하는 단계;적어도 하나 이상의 확률 파라미터 및 마스크를 사용해 상기 비트스트림을 엔트로피 디코딩하여 잠재 표현을 복원하는 단계; 및상기 잠재 표현을 제2 학습된 신경망을 사용해 디코딩하여 출력 이미지를 생성하는 단계로서, 상기 출력 이미지는 입력 이미지의 근사값인 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>186. 제185항에 있어서,잠재 표현을 복수의 파티션(partition)으로 분할하는 단계; 상기 복수의 파티션과 관련하여 적어도 하나 이상의 통계값을 추정하는 단계; 및상기 추정된 적어도 하나 이상의 통계값을 사용하여 마스크를 생성하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>187. 제186항에 있어서,상기 통계값은 평균값을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>188. 제186항 또는 제187항에 있어서,상기 마스크 생성 단계는, 상기한 적어도 하나 이상의 통계값에 임계값을 적용하여 상기 복수의 파티션 중 적어도 하나의 파티션에 대한 마스크 값을 생성하는 단계를 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>189. 제185항 내지 제188항 중 어느 한 항에 있어서,마스크를 적용하는 단계는 상기 잠재 표현의 요소 하나 이상을 0으로 설정하는 단계를 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>190. 제185항 내지 제189항 중 어느 한 항에 있어서,적어도 하나 이상의 확률 파라미터를 사용하여 적어도 하나 이상의 잠재 표현 요소에 대한 값을 추정하는 단계; 및 상기 추정된 값을 사용하여 상기 하나 이상의 요소에 대한 마스크 값을 생성하는 단계; 를 포함하는 방법.</claim></claimInfo><claimInfo><claim>191. 제185항 내지 제190항 중 어느 한 항에 있어서,상기 마스킹된 잠재 표현의 엔트로피 인코딩에, 마스킹된 잠재 표현 중 마스킹된 요소들을 건너뛰는 단계가 포함되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>192. 제185항 내지 제191항 중 어느 한 항에 있어서,상기 비트스트림의 엔트로피 디코딩에, 마스킹된 잠재 표현 중 마스킹된 요소에 대응하는 요소를 건너뛰는 단계가 포함되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>193. 제192항에 있어서,요소 건너뛰기 단계에, 버퍼를 0으로 초기화하는 단계와 마스킹된 잠재 표현 중 마스킹된 영역의 크기에 따라 지정된 개수의 0을 건너뛰는 단계가 포함되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>194. 제193항에 있어서,요소 건너뛰기 단계에, 마스킹된 잠재 표현 중 마스킹된 영역의 크기에 따라 지정된 개수의 0을 버퍼에 삽입하는 단계가 포함되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>195. 제185항 내지 제194항 중 어느 한 항에 있어서,비트스트림에서 마스크를 복원하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>196. 제185항 내지 제195항 중 어느 한 항에 있어서,엔트로피 디코딩에, 적어도 하나 이상의 확률 파라미터를 사용하여 적어도 하나 이상의 잠재 표현 요소에 대한 값을 추정하는 단계; 및 상기 추정된 값을 사용하여 상기 하나 이상의 요소에 대한 마스크 값을 생성하는 단계가 포함되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>197. 손실 이미지 또는 비디오의 인코딩 및 전송을 위한 방법으로서,제1 컴퓨터 시스템에서 입력 이미지를 수신하는 단계;상기 입력 이미지를 제1 학습된 신경망을 사용해 인코딩하여 잠재 표현을 생성하는 단계로서, 상기 잠재 표현은 적어도 하나 이상의 확률 파라미터로 기술되는 확률 분포를 갖는 단계;상기 잠재 표현 또는 적어도 하나 이상의 확률 파라미터 중 적어도 하나에 기반하여 마스크를 생성하는 단계;상기 마스크를 잠재 표현에 적용하여 마스킹된 잠재 표현을 생성하는 단계;적어도 하나 이상의 확률 파라미터를 사용해 상기 마스킹된 잠재 표현을 엔트로피 인코딩하여 비트스트림을 생성하는 단계; 및상기 비트스트림을 제2 컴퓨터 시스템으로 전송하는 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>198. 손실 이미지 또는 비디오의 수신 및 디코딩을 위한 방법으로서,적어도 하나의 엔트로피 파라미터를 사용하여 입력 이미지와 관련된 마스킹된 잠재 표현을 엔트로피 인코딩함으로써 생성된 비트스트림을 제2 컴퓨터 시스템에서 수신하는 단계;상기 비트스트림을 적어도 하나의 확률 파라미터 및 마스크를 사용해 엔트로피 디코딩하여 잠재 표현을 복원하는 단계; 및상기 잠재 표현을 제2 학습된 신경망을 사용해 디코딩하여 출력 이미지를 생성하는 단계로서, 상기 출력 이미지는 입력 이미지의 근사값인 단계;를 포함하는 방법.</claim></claimInfo><claimInfo><claim>199. 제185항 내지 제198항 중 어느 한 항의 방법을 수행하도록 구성된 데이터 처리 시스템.</claim></claimInfo><claimInfo><claim>200. 제185항 내지 제198항의 방법을 수행하도록 구성된 데이터 처리 장치.</claim></claimInfo><claimInfo><claim>201. 컴퓨터가 해당 프로그램을 실행할 때 제185항 내지 제198항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>202. 컴퓨터에 의해 실행되었을 때 제185항 내지 제198항의 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 판독가능 저장매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>영국 이* *엘엔 런던 커머셜 로드 **-** 딥 랜더 앳 테크스페이스</address><code>520230266064</code><country>영국</country><engName>Deep Render Ltd.</engName><name>딥 랜더 엘티디.</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>우크라이나</country><engName>KOSHKINA, Vira</engName><name>코시키나, 비라</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>스웨덴</country><engName>XU, Jan</engName><name>쉬, 얀</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>독일</country><engName>ETMANN, Christian</engName><name>에트만, 크리스티안</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>독일</country><engName>FINLAY, Christopher</engName><name>핀레이, 크리스토퍼</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>영국</country><engName>ZAFAR, Arsalan</engName><name>자파르, 아르살란</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>캐나다</country><engName>ABBASI, Bilal</engName><name>압바시, 비랄</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>슬로바키아</country><engName>CIZEL, Sebastjan</engName><name>치젤, 세바스티안</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>불가리아</country><engName>CHERGANSKI, Aleksander</engName><name>체르간스키, 알렉산데르</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>영국</country><engName>ALAWIYE, Hamza</engName><name>알라위예, 함자</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>아일랜드</country><engName>DEES, Aaron</engName><name>디스, 아론</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>아일랜드</country><engName>O' ROURKE, Ciaran</engName><name>오' 루어크, 키어런</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>스웨덴</country><engName>LYTCHIÉR, Alexander</engName><name>리치에르, 알렉산더</name></inventorInfo><inventorInfo><address>영국 이* *엘엔 런던 커머셜 로드 ...</address><code> </code><country>독일</country><engName>BESENBRUCH, Christian</engName><name>베젠브루흐, 크리스티안</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>영국</priorityApplicationCountry><priorityApplicationDate>2023.02.19</priorityApplicationDate><priorityApplicationNumber>2302350.0</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>영국</priorityApplicationCountry><priorityApplicationDate>2023.03.22</priorityApplicationDate><priorityApplicationNumber>2304148.6</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>영국</priorityApplicationCountry><priorityApplicationDate>2023.11.30</priorityApplicationDate><priorityApplicationNumber>2318291.8</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2025.09.04</receiptDate><receiptNumber>1-1-2025-1020346-30</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2025.09.23</receiptDate><receiptNumber>1-5-2025-0162266-31</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257029625.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c937e31a9d38214751603f3d70225fd341730420038e7fa975b409bd6d182c2eed6904585949a41c4f7411cf38a6c2cacd2b0be640f0ecd171a</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf247b5c78e370b4b94586a638aaef274fa60974fb23a90ff3395b5a17f335343154500fac89e20b81847548dacbaa16f751c210ae0f9003cd</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>