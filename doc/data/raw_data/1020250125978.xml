<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:28:29.2829</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2025.09.04</applicationDate><applicationFlag>특허</applicationFlag><applicationNumber>10-2025-0125978</applicationNumber><claimCount>18</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>전자 장치 및 이의 제어 방법</inventionTitle><inventionTitleEng>Electronic device and Method for controlling th electronic  device thereof</inventionTitleEng><openDate>2025.09.12</openDate><openNumber>10-2025-0135159</openNumber><originalApplicationDate>2021.01.26</originalApplicationDate><originalApplicationKind>국내출원/분할</originalApplicationKind><originalApplicationNumber>10-2021-0010600</originalApplicationNumber><originalExaminationRequestDate>2025.10.02</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>H04N 23/951</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>H04N 23/63</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>H04N 23/55</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/194</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/10</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo><familyApplicationNumber>1020210010600</familyApplicationNumber></familyInfo></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 전자 장치가 개시된다. 전자 장치는 복수의 렌즈를 포함하는 카메라 및 프로세서를 포함하고, 프로세서는 카메라의 제1 렌즈를 이용하여 제1 영상을 획득하고, 제1 렌즈와 상이한 화각의 영상을 획득할 수 있는 카메라의 제2 렌즈를 이용하여 제2 영상을 획득하며, 적어도 하나의 신경망 모델을 이용하여 제2 영상에 대한 정보 및 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보를 획득하고, 제2 영상에 대한 정보 및 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보를 바탕으로 제2 영상에 대응되는 필터 셋을 식별하고, 식별된 필터 셋을 바탕으로 제1 영상을 보정한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 전자 장치에 있어서,복수의 렌즈를 포함하는 카메라; 및프로세서;를 포함하고,상기 프로세서는,상기 카메라의 제1 렌즈를 이용하여 제1 영상을 획득하고,상기 제1 렌즈와 상이한 화각의 영상을 획득할 수 있는 상기 카메라의 제2 렌즈를 이용하여 제2 영상을 획득하며,적어도 하나의 신경망 모델을 이용하여 상기 제2 영상에 대한 정보 및 상기 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보를 획득하고,상기 제2 영상에 대한 정보 및 상기 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보를 바탕으로 상기 제2 영상에 대응되는 필터 셋을 식별하고,상기 식별된 필터 셋을 바탕으로 상기 제1 영상을 보정하는, 전자 장치.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 제2 영상에 대한 정보는,상기 제2 영상에 대응되는 뎁스 맵 정보 및 상기 제2 영상에 대응되는 saliency 정보를 포함하며,상기 프로세서는,제1 신경망 모델에 상기 제2 영상을 입력하여 상기 뎁스 맵 정보를 획득하고, 제2 신경망 모델에 상기 제2 영상을 입력하여 상기 saliency 정보를 획득하는 전자 장치.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보는,상기 적어도 하나의 오브젝트의 유형 정보, 상기 적어도 하나의 오브젝트의 3차원 위치 정보, 상기 적어도 하나의 오브젝트가 위치하는 영역에 대한 정보 및 상기 적어도 하나의 오브젝트의 자세 정보를 포함하며,상기 프로세서는,제3 신경망 모델에 상기 제2 영상을 입력하여 상기 제2 영상 내에 포함된 상기 적어도 하나의 오브젝트와 배경을 분할한 분할 정보(segmentation information) 및 상기 적어도 하나의 오브젝트의 유형 정보를 획득하고,상기 분할 정보 및 상기 뎁스 맵 정보를 바탕으로 상기 적어도 하나의 오브젝트의 3차원 위치 정보 및 상기 적어도 하나의 오브젝트가 위치하는 영역에 대한 정보를 획득하고,제4 신경망 모델에 상기 분할 정보에 포함된 적어도 하나의 오브젝트에 대한 정보를 입력하여 상기 적어도 하나의 오브젝트의 자세 정보를 획득하는 전자 장치.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 프로세서는,제5 신경망 모델에 상기 오브젝트의 3차원 위치 정보 및 상기 오브젝트가 위치하는 영역에 대한 정보를 입력하여 상기 적어도 하나의 오브젝트 사이의 관계 정보를 획득하는 전자 장치.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 프로세서는, 제6 신경망 모델에 상기 관계 정보, 상기 적어도 하나의 오브젝트의 자세 정보, 상기 제2 영상에 대한 포커스 정보 및 상기 saliency 정보를 입력하여 보정된 saliency 정보에 대응되는 열지도 정보를 획득하는 전자 장치. </claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 프로세서는,상기 제2 영상, 상기 획득된 열지도 정보 및 상기 적어도 하나의 오브젝트의 유형 정보를 바탕으로 상기 제2 영상의 화면 유형을 식별하고,상기 제2 영상의 화면 유형에 대응하는 필터 셋을 식별하는 전자 장치.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 메모리는,복수의 화면 유형에 대응되는 복수의 필터 셋을 포함하며,상기 프로세서는,상기 영상 정보 및 상기 오브젝트 정보에 기초하여 상기 제2 영상의 화면 유형을 식별하고,상기 복수의 필터 셋 중 상기 제2 영상의 화면 유형에 대응되는 필터 셋을 식별하는 전자 장치.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 보정된 제1 영상을 라이브 뷰로 제공하는 디스플레이;를 더 포함하고,상기 프로세서는,상기 영상 정보 및 상기 오브젝트 정보에 기초하여 상기 제2 영상의 화면 유형을 식별하고,상기 보정된 제1 영상을 포함하는 라이브 뷰 상에 상기 화면 유형에 대한 정보를 함께 제공하도록 상기 디스플레이를 제어하는 전자 장치.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 제2 렌즈는 상기 제1 렌즈보다 넓은 화각의 영상을 획득할 수 있는 전자 장치.</claim></claimInfo><claimInfo><claim>10. 복수의 렌즈를 포함하는 카메라를 포함하는 전자 장치의 제어 방법에 있어서,상기 카메라의 제1 렌즈를 이용하여 제1 영상을 획득하는 단계;상기 제1 렌즈와 상이한 화각의 영상을 획득할 수 있는 상기 카메라의 제2 렌즈를 이용하여 제2 영상을 획득하는 단계;적어도 하나의 신경망 모델을 이용하여 상기 제2 영상에 대한 정보 및 상기 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보를 획득하는 단계;상기 제2 영상에 대한 정보 및 상기 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보를 바탕으로 상기 제2 영상에 대응되는 필터 셋을 식별하는 단계; 및상기 식별된 필터 셋을 바탕으로 상기 제1 영상을 보정하는 단계;를 포함하는, 제어 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 제2 영상에 대한 정보는,상기 제2 영상에 대응되는 뎁스 맵 정보 및 상기 제2 영상에 대응되는 saliency 정보를 포함하며,상기 적어도 하나의 오브젝트에 대한 정보를 획득하는 단계는,제1 신경망 모델에 상기 제2 영상을 입력하여 상기 뎁스 맵 정보를 획득하고, 제2 신경망 모델에 상기 제2 영상을 입력하여 상기 saliency 정보를 획득하는 제어 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 제2 영상에 포함된 적어도 하나의 오브젝트에 대한 정보는,상기 적어도 하나의 오브젝트의 유형 정보, 상기 적어도 하나의 오브젝트의 3차원 위치 정보, 상기 적어도 하나의 오브젝트가 위치하는 영역에 대한 정보 및 상기 적어도 하나의 오브젝트의 자세 정보를 포함하며,상기 적어도 하나의 오브젝트에 대한 정보를 획득하는 단계는,제3 신경망 모델에 상기 제2 영상을 입력하여 상기 제2 영상 내에 포함된 상기 적어도 하나의 오브젝트와 배경을 분할한 분할 정보(segmentation information) 및 상기 적어도 하나의 오브젝트의 유형 정보를 획득하고,상기 분할 정보 및 상기 뎁스 맵 정보를 바탕으로 상기 적어도 하나의 오브젝트의 3차원 위치 정보 및 상기 적어도 하나의 오브젝트가 위치하는 영역에 대한 정보를 획득하고,제4 신경망 모델에 상기 분할 정보에 포함된 적어도 하나의 오브젝트에 대한 정보를 입력하여 상기 적어도 하나의 오브젝트의 자세 정보를 획득하는 제어 방법.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서,상기 필터 셋을 식별하는 단계는,제5 신경망 모델에 상기 오브젝트의 3차원 위치 정보 및 상기 오브젝트가 위치하는 영역에 대한 정보를 입력하여 상기 적어도 하나의 오브젝트 사이 관계 정보를 획득하는 제어 방법.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 필터 셋을 식별하는 단계는,제6 신경망 모델에 상기 관계 정보, 상기 적어도 하나의 오브젝트의 자세 정보, 상기 제2 영상에 대한 포커스 정보 및 상기 saliency 정보를 입력하여 보정된 saliency 정보에 대응되는 열지도 정보를 획득하는 제어 방법.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 필터 셋을 식별하는 단계는,상기 제2 영상, 상기 획득된 열지도 정보 및 상기 적어도 하나의 오브젝트의 유형 정보를 바탕으로 상기 제2 영상의 화면 유형을 식별하고,상기 제2 영상의 화면 유형에 대응하는 상기 필터 셋을 식별하는 제어 방법.</claim></claimInfo><claimInfo><claim>16. 제10항에 있어서,상기 전자 장치는,복수의 화면 유형에 대응되는 복수의 필터 셋을 저장하며,상기 필터 셋을 식별하는 단계는,상기 영상 정보 및 상기 오브젝트 정보에 기초하여 상기 제2 영상의 화면 유형을 식별하고,상기 복수의 필터 셋 중 상기 제2 영상의 화면 유형에 대응되는 필터 셋을 식별하는 제어 방법.</claim></claimInfo><claimInfo><claim>17. 제10항에 있어서,상기 보정된 제1 영상을 라이브 뷰로 제공하는 단계; 및상기 보정된 제1 영상을 포함하는 라이브 뷰 상에 상기 화면 유형에 대한 정보를 함께 제공하는 단계;를 포함하는 제어 방법.</claim></claimInfo><claimInfo><claim>18. 제10항에 있어서,상기 제2 렌즈는 상기 제1 렌즈보다 넓은 화각의 영상을 획득할 수 있는 제어 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country>대한민국</country><engName>OH, Seongwoo</engName><name>오성우</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country>대한민국</country><engName>YOON, Yeonggyo</engName><name>윤영교</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country>대한민국</country><engName>HWANG, Jinyoung</engName><name>황진영</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울시 서초구 강남대로 *** 신덕빌딩 *층(나우특허법률사무소)</address><code>919980005433</code><country>대한민국</country><engName>Jeong Hong Sik</engName><name>정홍식</name></agentInfo><agentInfo><address>서울시 서초구 강남대로 *** 신덕빌딩 *층(나우특허법률사무소)</address><code>920050001107</code><country>대한민국</country><engName>KIM TAEHUN</engName><name>김태헌</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Divisional Application] Patent Application</documentEngName><documentName>[분할출원]특허출원서</documentName><receiptDate>2025.09.04</receiptDate><receiptNumber>1-1-2025-1019820-46</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.10.02</receiptDate><receiptNumber>1-1-2025-1127583-75</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020250125978.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c939bc80d05dbf27d03bbd3325a7d9c7ef5c3e33077dd1fa47467d6f1059cb408d5f66d56070e8306b6fbfed696cfe271adb1469fcb1882101c</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf1024b3b7123a61e9549872eed1afdcaedaa7603760fc150724b7c6b0d8e2c0aace0cf8564d2cf6110226041f753f610b0441d2e4e6b8a391</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>