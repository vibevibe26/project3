<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:53:43.5343</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2020.01.18</applicationDate><applicationFlag>특허</applicationFlag><applicationNumber>10-2025-7032031</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>포토리얼리스틱 실시간 인물 애니메이션을 위한 시스템 및 방법</inventionTitle><inventionTitleEng>Systems and methods for photorealistic real-time portrait  animation</inventionTitleEng><openDate>2025.10.20</openDate><openNumber>10-2025-0150661</openNumber><originalApplicationDate>2020.01.18</originalApplicationDate><originalApplicationKind>국제출원/분할</originalApplicationKind><originalApplicationNumber>10-2023-7042816</originalApplicationNumber><originalExaminationRequestDate>2025.09.24</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.09.24</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 13/40</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo><familyApplicationNumber>1020237042816</familyApplicationNumber></familyInfo></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 포토리얼리스틱 실시간 인물 애니메이션(photorealistic real-time portrait animation)을 위한 시스템 및 방법이 제공된다. 예시적인 방법은 적어도 하나의 입력 프레임이 있는 시나리오 비디오를 수신하는 단계를 포함한다. 상기 입력 프레임은 제1 얼굴을 포함한다. 상기 방법은 제2 얼굴이 있는 타겟 이미지를 수신하는 단계를 더 포함한다. 상기 방법은 상기 적어도 하나의 입력 프레임 및 타겟 이미지에 기초하여, 2D 변형 (two-dimensional (2D) deformations)을 결정하는 단계를 더 포함하며, 상기 2D 변형은, 상기 제2 얼굴에 적용(apply)된 경우, 적어도 상기 제1 얼굴의 머리 방향(head orientation) 및 얼굴 표정(facial expression)을 모방(imitate)하도록 상기 제2 얼굴을 수정(modify)한다. 상기 방법은 출력 비디오(output vide)의 적어도 하나의 출력 프레임(output frame)을 획득(obtain)하기 위해 상기 2D 변형을 상기 타겟 이미지에 적용하는 단계를 더 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2020.07.23</internationOpenDate><internationOpenNumber>WO2020150687</internationOpenNumber><internationalApplicationDate>2020.01.18</internationalApplicationDate><internationalApplicationNumber>PCT/US2020/014220</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 인물 애니메이션(portrait animation)을 위한 방법에 있어서,컴퓨팅 장치에 의하여, 제1 머리의 움직임에 대한 정보를 포함하는 시나리오 데이터를 수신하는 단계;상기 컴퓨팅 장치에 의하여, 제2 머리 및 배경을 포함하는 타겟 이미지를 수신하는 단계;상기 컴퓨팅 장치에 의하여, 상기 타겟 이미지 및 상기 제1 머리의 상기 움직임에 대한 상기 정보에 기초하여, 상기 타겟 이미지의 상기 제2 머리의 2차원(2D) 변형(deformation)을 결정하는 단계;상기 컴퓨팅 장치에 의하여, 출력 비디오의 적어도 하나의 출력 프레임을 획득하도록 상기 타겟 이미지에 대해 상기 2D 변형을 적용하는 단계 - 상기 적어도 하나의 출력 프레임은 상기 제1 머리의 상기 움직임에 따라 변위된(displaced) 상기 제2 머리를 포함함 -; 및상기 컴퓨팅 장치에 의하여, 배경 예측 뉴럴 네트워크를 이용하여 상기 변위된 제2 머리와 상기 배경 사이의 갭에 상기 배경의 부분을 채우는 단계를 포함하는,방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 배경 예측 뉴럴 네트워크는, 합성 데이터세트(synthetic dataset)에 기초하여 트레이닝되고, 상기 합성 데이터세트는 하나 또는 그 이상의 배경 이미지 앞에 위치하는 하나 또는 그 이상의 사람의 이미지를 포함하는,방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 배경 예측 뉴럴 네트워크는, 상기 하나 또는 그 이상의 배경 이미지의 부분(part)을 예측하도록 트레이닝되고, 상기 부분은 상기 하나 또는 그 이상의 사람의 상기 이미지에 의해 덮여(covered) 있는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 배경 예측 뉴럴 네트워크는 학습 스킴(learning scheme)을 사용하여 트레이닝되고, 상기 학습 스킴은:상기 배경 예측 뉴럴 네트워크의 출력 및 타겟 배경 이미지에 기초하여 트레이닝 손실(loss)을 계산하도록 설계된 손실 계산기;상기 배경 예측 뉴럴 네트워크의 상기 출력에 기초하여 생성기 손실을 생성하도록 설계된 디스크리미네이터(discriminator);상기 타겟 배경 이미지에 기초하여 예측 값을 계산하도록 설계된 제2 디스크리미네이터; 및상기 생성기 손실 및 상기 예측 값에 기초하여 디스크리미네이터 손실을 계산하도록 설계된 차이 블록을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 배경 예측 뉴럴 네트워크는:상기 제1 디스크리미네이터 네트워크의 제1 가중치 및 상기 제2 디스크리미네이터 네트워크의 제2 가중치를 변하지 않도록 유지하고, 역방향 전파(backward propagation)을 위해 상기 트레이닝 손실 및 상기 발생기 손실을 이용하는 것을 포함하는 제1 트레이닝 스텝;상기 제1 디스크리미네이터 네트워크의 제1 가중치 및 상기 제2 디스크리미네이터 네트워크의 제2 가중치를 변하지 않도록 유지하고, 상기 역방향 전파(backward propagation)을 위해 오직 상기 트레이닝 손실만을 이용하는 것을 포함하는 제2 트레이닝 스텝; 및상기 배경 예측 뉴럴 네트워크의 제3 가중치를 변하지 않도록 유지하고, 상기 역방향 전파(backward propagation)을 위해 상기 디스크리미네이터 손실을 이용하는 것을 포함하는 제3 트레이닝 스텝의 조합을 사용하여 트레이닝되는, 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 배경 예측 뉴럴 네트워크를 트레이닝하는 단계는,제1 미리 정해진 시간에 상기 제2 트레이닝 스텝을 수행하는 단계;제2 미리 정해진 시간에 상기 제3 트레이닝 스텝을 수행하는 단계; 및상기 제1 트레이닝 스텝을 수행하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 제1 미리 정해진 시간은 상기 제2 미리 정해진 시간보다 큰, 방법.</claim></claimInfo><claimInfo><claim>8. 제6항에 있어서,상기 제2 미리 정해진 시간에 상기 제3 트레이닝 스텝을 수행하는 단계 및 상기 제1 트레이닝 스텝을 수행하는 단계는 품질 기준(quality criterion)에 도달할 때까지 반복되는,방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 타겟 이미지의 은닉 영역(hidden region)을 생성하도록 추가 뉴럴 네트워크를 사용하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제9항에 있어서,상기 은닉 영역은, 상기 타겟 이미지의 입 영역 또는 눈 영역을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>11. 컴퓨팅 장치에 있어서,프로세서, 및명령어를 저장하는 메모리를 포함하고,상기 프로세서에 의해 상기 명령어가 실행되는 경우, 상기 컴퓨팅 장치가:제1 머리의 움직임에 대한 정보를 포함하는 시나리오 데이터를 수신하고;제2 머리 및 배경을 포함하는 타겟 이미지를 수신하고;상기 타겟 이미지 및 상기 제1 머리의 상기 움직임에 대한 상기 정보에 기초하여, 상기 타겟 이미지의 상기 제2 머리의 2차원(2D) 변형(deformation)을 결정하고;출력 비디오의 적어도 하나의 출력 프레임을 획득하도록 상기 타겟 이미지에 대해 상기 2D 변형을 적용하고 - 상기 적어도 하나의 출력 프레임은 상기 제1 머리의 상기 움직임에 따라 변위된(displaced) 상기 제2 머리를 포함함 -; 및경 예측 뉴럴 네트워크를 이용하여 상기 변위된 제2 머리와 상기 배경 사이의 갭에 상기 배경의 부분을 채우도록 구성되는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 배경 예측 뉴럴 네트워크는, 합성 데이터세트에 기초하여 트레이닝되고, 상기 합성 데이터세트는 하나 또는 그 이상의 배경 이미지 앞에 위치하는 하나 또는 그 이상의 사람의 이미지를 포함하는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서,상기 배경 예측 뉴럴 네트워크는, 상기 하나 또는 그 이상의 배경 이미지의 부분(part)을 예측하도록 트레이닝되고, 상기 부분은 상기 하나 또는 그 이상의 사람의 상기 이미지에 의해 덮여 있는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>14. 제11항에 있어서,상기 배경 예측 뉴럴 네트워크는 학습 스킴(learning scheme)을 사용하여 트레이닝되고, 상기 학습 스킴은:상기 배경 예측 뉴럴 네트워크의 출력 및 타겟 배경 이미지에 기초하여 트레이닝 손실을 계산하도록 설계된 손실 계산기;상기 배경 예측 뉴럴 네트워크의 상기 출력에 기초하여 생성기 손실을 생성하도록 설계된 디스크리미네이터(discriminator);상기 타겟 배경 이미지에 기초하여 예측 값을 계산하도록 설계된 제2 디스크리미네이터; 및상기 생성기 손실 및 상기 예측 값에 기초하여 디스크리미네이터 손실을 계산하도록 설계된 차이 블록을 포함하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 배경 예측 뉴럴 네트워크는:상기 제1 디스크리미네이터 네트워크의 제1 가중치 및 상기 제2 디스크리미네이터 네트워크의 제2 가중치를 변하지 않도록 유지하고, 역방향 전파(backward propagation)을 위해 상기 트레이닝 손실 및 상기 발생기 손실을 이용하는 것을 포함하는 제1 트레이닝 스텝;상기 제1 디스크리미네이터 네트워크의 제1 가중치 및 상기 제2 디스크리미네이터 네트워크의 제2 가중치를 변하지 않도록 유지하고, 상기 역방향 전파(backward propagation)을 위해 오직 상기 트레이닝 손실만을 이용하는 것을 포함하는 제2 트레이닝 스텝; 및상기 배경 예측 뉴럴 네트워크의 제3 가중치를 변하지 않도록 유지하고, 상기 역방향 전파(backward propagation)을 위해 상기 디스크리미네이터 손실을 이용하는 것을 포함하는 제3 트레이닝 스텝의 조합을 사용하여 트레이닝되는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 배경 예측 뉴럴 네트워크를 트레이닝하는 것은,제1 미리 정해진 시간에 상기 제2 트레이닝 스텝을 수행하는 것;제2 미리 정해진 시간에 상기 제3 트레이닝 스텝을 수행하는 것; 및상기 제1 트레이닝 스텝을 수행하는 것를 포함하는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 제1 미리 정해진 시간은 상기 제2 미리 정해진 시간보다 큰, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>18. 제16항에 있어서,상기 제2 미리 정해진 시간에 상기 제3 트레이닝 스텝을 수행하는 것 및 상기 제1 트레이닝 스텝을 수행하는 것은 품질 기준(quality criterion)에 도달할 때까지 반복되는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>19. 제11항에 있어서,상기 명령어는 상기 컴퓨팅 장치가:상기 타겟 이미지의 은닉 영역(hidden region)을 생성하도록 추가 뉴럴 네트워크를 사용하도록 더 구성되는, 컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>20. 비-일시적 컴퓨터-판독가능 저장 매체에 있어서,상기 비-일시적 컴퓨터-판독 가능 저장 매체는 명령어를 포함하고, 상기 명령어가 컴퓨팅 장치에 의해서 실행되는 경우, 상기 컴퓨팅 장치가:제1 머리의 움직임에 대한 정보를 포함하는 시나리오 데이터를 수신하고;제2 머리 및 배경을 포함하는 타겟 이미지를 수신하고;상기 타겟 이미지 및 상기 제1 머리의 상기 움직임에 대한 상기 정보에 기초하여, 상기 타겟 이미지의 상기 제2 머리의 2차원(2D) 변형(deformation)을 결정하고;출력 비디오의 적어도 하나의 출력 프레임을 획득하도록 상기 타겟 이미지에 대해 상기 2D 변형을 적용하고 - 상기 적어도 하나의 출력 프레임은 상기 제1 머리의 상기 움직임에 따라 변위된(displaced) 상기 제2 머리를 포함함 -; 및경 예측 뉴럴 네트워크를 이용하여 상기 변위된 제2 머리와 상기 배경 사이의 갭에 상기 배경의 부분을 채우도록 하는,비-일시적 컴퓨터-판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국, 캘리포니아 *****, 산타 모니카, ** 스트리트 ****</address><code>520200092045</code><country>미국</country><engName>Snap Inc.</engName><name>스냅 아이엔씨</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 캘리포니아 ***** 산...</address><code> </code><country>러시아</country><engName>KROKHALEV, Eugene</engName><name>크로크할레브, 유진</name></inventorInfo><inventorInfo><address>미국 캘리포니아 ***** 산...</address><code> </code><country>미국</country><engName>MASHRABOV, Aleksandr</engName><name>마쉬라보브, 알렉산더</name></inventorInfo><inventorInfo><address>미국 캘리포니아 ***** 산...</address><code> </code><country>러시아</country><engName>SAVCHENKOV, Pavel</engName><name>새브첸코브, 파벨</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2019.01.18</priorityApplicationDate><priorityApplicationNumber>16/251,472</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Divisional Application(International Application)] Patent Application</documentEngName><documentName>[분할출원(국제출원)]특허출원서</documentName><receiptDate>2025.09.24</receiptDate><receiptNumber>1-1-2025-1091999-76</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257032031.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=348aaf18c46825cf02d6c2de1c78338ebd6b9e5188b12e6e3dd807b411c835c5eaa8969a917153ad71daae83114924f7008137411ac9e0e679f3b3a37140ed0179e253c736867569</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfd79bea3195aba4d57a89e4b02c6d1a1f972cb3fcd723fa595e7541abd8463e1a20d090cf2b31e05b6a01a739402b6b45c4b1bfeff4f349b2</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>