<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:05:39.539</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.02.25</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0025387</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>기계 학습 모델의 트레이닝 방법 및 전자 장치</inventionTitle><inventionTitleEng>ELECTRONIC DEVICE AND TRAINING METHOD OF MACHINE  LEARNING MODEL</inventionTitleEng><openDate>2023.05.11</openDate><openNumber>10-2023-0065125</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.02.20</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/75</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/75</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/72</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0475</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/094</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/047</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 일 실시예에 따른 전자 장치는 트레이닝 샘플 영상으로부터 적어도 부분적으로 중첩된 영상 패치들을 추출하고, 추출된 영상 패치들로부터 판별기 모델의 일부 레이어를 이용하여 중첩 영역에 대응하는 부분 특징 맵들을 추출하며, 추출된 부분 특징 맵들에 기초하여 판별기 모델을 트레이닝시킬 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 전자 장치에 있어서,판별기 모델(discriminator model)을 포함하는 기계 학습 모델을 저장한 메모리; 및트레이닝 샘플 영상으로부터 제1 영상 패치 및 상기 제1 영상 패치와 적어도 부분적으로 중첩되는 제2 영상 패치를 추출하고, 상기 제1 영상 패치로부터, 상기 판별기 모델의 일부 레이어에 기초하여 제1 특징 맵을 추출하며, 상기 제2 영상 패치로부터 상기 일부 레이어에 기초하여 제2 특징 맵을 추출하고, 상기 추출된 제1 특징 맵으로부터 투사기 모델(projector model)에 기초하여 투사된 맵(projected map) 중 일부분에 대응하는 제1 부분 특징 맵을 추출하며, 상기 추출된 제2 특징 맵 중 일부분에 대응하는 제2 부분 특징 맵 및 상기 제1 부분 특징 맵에 기초하여 산출된 제1 목적 함수 값에 기초하여 상기 판별기 모델을 트레이닝시키는 프로세서를 포함하는 전자 장치.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 프로세서는,상기 제1 특징 맵이 투사된 맵에서 상기 제1 영상 패치와 상기 제2 영상 패치 간의 중첩 영역(overlapped region)에 대응하는 부분을 상기 제1 부분 특징 맵으로 결정하고,상기 추출된 제2 특징 맵 중 상기 중첩 영역에 대응하는 부분을 상기 제2 부분 특징 맵으로 결정하는,전자 장치.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 프로세서는,상기 제1 목적 함수 값에 기초하여 상기 판별기 모델의 적어도 일부 레이어의 파라미터 및 상기 투사기 모델의 파라미터를 업데이트하는,전자 장치.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 메모리는,상기 기계 학습 모델에서 상기 판별기 모델의 입력측에 연결되는 생성기 모델(generator model)을 더 저장하고,상기 프로세서는,상기 트레이닝 샘플 영상으로부터 상기 판별기 모델을 이용하여 산출된 타겟 판별 데이터(target discrimination data) 및 상기 생성기 모델을 이용하여 생성된 페이크 영상(fake image)으로부터 상기 판별기 모델에 기초하여 산출된 소스 판별 데이터(source discrimination data)에 기초하여 제2 목적 함수 값을 산출하는,전자 장치.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 프로세서는,상기 생성기 모델로부터 생성된 상기 페이크 영상으로부터 상기 판별기 모델을 이용하여 산출된 상기 소스 판별 데이터에 기초한 제3 목적 함수 값을 통해 상기 생성기 모델을 트레이닝시키는,전자 장치.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 프로세서는,상기 판별기 모델 및 상기 판별기 모델을 이용하여 트레이닝된 생성기 모델 중 적어도 하나에 기초하여 입력 영상에 대한 판별 결과를 출력하는,전자 장치.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 프로세서는,상기 판별기 모델로부터 픽셀 별 판별 확률 값을 가지는 판별 데이터를 획득하고,상기 판별 확률 값 및 상기 판별 데이터에서 상기 판별 확률 값에 기초하여 판별된 영역(discriminated region) 중 적어도 하나에 기초하여 상기 판별 결과를 결정하는,전자 장치.</claim></claimInfo><claimInfo><claim>8. 제6항에 있어서,상기 프로세서는,상기 기계 학습 모델에 입력된 제품 영상으로부터, 상기 판별기 모델 및 상기 판별기 모델을 이용하여 트레이닝된 생성기 모델 중 적어도 하나에 기초하여, 상기 제품 영상에 포함된 제품이 정상인지 또는 비정상인지 여부를 출력하는,전자 장치.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 프로세서는,판별 대상인 부분의 값이 나머지 부분의 값보다 높은 특징 맵을, 상기 판별기 모델의 일부 레이어에서, 출력하는,전자 장치.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 프로세서는,상기 판별기 모델의 업데이트가 요구되는 것에 기초하여, 새로 수집된 트레이닝 데이터 셋트의 각 영상으로부터 서로 부분적으로 중첩되는 새로운 영상 패치들을 추출하고,상기 추출된 새로운 영상 패치들에 기초하여 산출된 목적 함수 값을 이용하여 상기 판별기 모델의 파라미터를 업데이트하는,전자 장치.</claim></claimInfo><claimInfo><claim>11. 전자 장치에 의해 수행되는 기계 학습 모델의 트레이닝 방법에 있어서,트레이닝 샘플 영상으로부터 제1 영상 패치 및 상기 제1 영상 패치와 적어도 부분적으로 중첩되는 제2 영상 패치를 추출하는 단계;상기 제1 영상 패치로부터, 상기 기계 학습 모델에 포함된 판별기 모델(discriminator model)의 일부 레이어에 기초하여, 제1 특징 맵을 추출하는 단계:상기 제2 영상 패치로부터 상기 일부 레이어에 기초하여 제2 특징 맵을 추출하는 단계;상기 추출된 제1 특징 맵으로부터 투사기 모델(projector model)에 기초하여 투사된 맵(projected map) 중 일부분에 대응하는 제1 부분 특징 맵을 추출하는 단계; 및상기 추출된 제2 특징 맵 중 일부분에 대응하는 제2 부분 특징 맵 및 상기 제1 부분 특징 맵에 기초하여 산출된 제1 목적 함수 값에 기초하여 상기 판별기 모델을 트레이닝시키는 단계를 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 제1 부분 특징 맵을 추출하는 단계는,상기 제1 특징 맵이 투사된 맵에서 상기 제1 영상 패치와 상기 제2 영상 패치 간의 중첩 영역(overlapped region)에 대응하는 부분을 상기 제1 부분 특징 맵으로 결정하는 단계를 포함하고,상기 판별기 모델을 트레이닝시키는 단계는,상기 추출된 제2 특징 맵 중 상기 중첩 영역에 대응하는 부분을 상기 제2 부분 특징 맵으로 결정하는 단계를 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 판별기 모델을 트레이닝시키는 단계는,상기 제1 목적 함수 값에 기초하여 상기 판별기 모델의 적어도 일부 레이어의 파라미터 및 상기 투사기 모델의 파라미터를 업데이트하는 단계를 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>14. 제11항에 있어서,상기 판별기 모델을 트레이닝시키는 단계는,상기 트레이닝 샘플 영상으로부터 상기 판별기 모델을 이용하여 산출된 타겟 판별 데이터(target discrimination data) 및 상기 기계 학습 모델의 생성기 모델(generator model)을 이용하여 생성된 페이크 영상(fake image)로부터 상기 판별기 모델에 기초하여 산출된 소스 판별 데이터(source discrimination data)에 기초하여 제2 목적 함수 값을 산출하는 단계를 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 생성기 모델로부터 생성된 상기 페이크 영상으로부터 상기 판별기 모델을 이용하여 산출된 상기 소스 판별 데이터에 기초한 제3 목적 함수 값을 통해 상기 생성기 모델을 트레이닝시키는 단계를 더 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>16. 제11항에 있어서,상기 판별기 모델 및 상기 판별기 모델을 이용하여 트레이닝된 생성기 모델 중 적어도 하나에 기초하여 입력 영상에 대한 판별 결과를 출력하는 단계를 더 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 판별 결과를 출력하는 단계는,상기 판별기 모델로부터 픽셀 별 판별 확률 값을 가지는 판별 데이터를 획득하는 단계;상기 판별 확률 값 및 상기 판별 데이터에서 상기 판별 확률 값에 기초하여 판별된 영역(discriminated region)에 기초하여 상기 판별 결과를 결정하는 단계를 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>18. 제16항에 있어서,상기 판별 결과를 출력하는 단계는,상기 기계 학습 모델에 입력된 제품 영상으로부터, 상기 판별기 모델 및 상기 판별기 모델을 이용하여 트레이닝된 생성기 모델 중 적어도 하나에 기초하여, 상기 제품 영상에 포함된 제품이 정상인지 또는 비정상인지 여부를 출력하는 단계를 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>19. 제11항에 있어서,판별 대상인 부분의 값이 나머지 부분의 값보다 높은 특징 맵을, 상기 판별기 모델의 일부 레이어에서, 출력하는 단계를 더 포함하는 기계 학습 모델의 트레이닝 방법.</claim></claimInfo><claimInfo><claim>20. 제11항 내지 제19항 중 어느 한 항의 방법을 수행하기 위한 명령어를 포함하는 하나 이상의 컴퓨터 프로그램을 저장한 컴퓨터 판독 가능 기록 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code>420170474452</code><country>대한민국</country><engName>KO, MIN SU</engName><name>고민수</name></inventorInfo><inventorInfo><address>경기도 성남시 분당구...</address><code>420190080505</code><country>대한민국</country><engName>SUH, Sung Joo</engName><name>서성주</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code>420180459477</code><country>대한민국</country><engName>LEE, HUI JIN</engName><name>이희진</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code>420220122302</code><country>대한민국</country><engName>Cha, Eunju</engName><name>차은주</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2021.11.04</priorityApplicationDate><priorityApplicationNumber>1020210150686</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.02.25</receiptDate><receiptNumber>1-1-2022-0217990-40</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Amendment to Patent Application, etc.] Amendment</documentEngName><documentName>[출원서 등 보정]보정서</documentName><receiptDate>2022.02.28</receiptDate><receiptNumber>1-1-2022-0226444-44</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.02.20</receiptDate><receiptNumber>1-1-2025-0200833-18</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220025387.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93acac1d6f24fbdcd16b7157542124f86d1cb267fde7e3ec29e393561a130825a9022c1c9950e01e81aa41afec2173da944b047fb30bf6b959</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf258eab45f6a5cb59d3273b60fbc19dba3010cf72122a0d2f9dc98152b1f6f2a77cdbfa16e4851f54cee3bc7d259b91ab7efc6e6a66589283</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>