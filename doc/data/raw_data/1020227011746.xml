<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:48.4148</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2020.09.22</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-7011746</applicationNumber><claimCount>21</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>인지 시스템을 훈련 및 검증하기 위한 방법 및 시스템</inventionTitle><inventionTitleEng>METHODS AND SYSTEMS FOR TRAINING AND VALIDATING A PERCEPTION SYSTEM</inventionTitleEng><openDate>2022.07.05</openDate><openNumber>10-2022-0093312</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.08.04</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2022.04.08</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/58</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06F 18/24</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/088</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/25</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/764</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/774</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/776</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/778</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/80</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 참조 센서의 세트; 테스트 센서의 세트; 및 컴퓨팅 장치로서: 참조 센서의 세트로부터 제1 훈련 신호를 수신하고 테스트 센서의 세트로부터 제2 훈련 신호를 수신하고; 장면과 연관되는 참조 깊이 정보를 포함하는 참조 이미지를 획득하기 위하여 제1 훈련 신호를 처리하고; 테스트 센서의 세트로부터의 후속 테스트 신호를 추론된 깊이 정보를 포함하는 테스트 이미지로 변환하기 위하여 신경망을 훈련하기 위하여 제2 훈련 신호와 참조 이미지를 사용하도록 구성되는 컴퓨팅 장치를 포함하고, 참조 센서의 세트와 테스트 센서의 세트는 동시에 공통 장면에 노출되는 인지 시스템이 개시된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2021.03.25</internationOpenDate><internationOpenNumber>WO2021053680</internationOpenNumber><internationalApplicationDate>2020.09.22</internationalApplicationDate><internationalApplicationNumber>PCT/IL2020/051028</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 특정 기준을 만족하는 명시되지 않은 객체를 포함하는 이미지의 관심 영역을 식별하기 위하여 깊이 정보를 포함하는 이미지를 처리하는 단계;장면의 알려진 객체를 감지 및 분류하도록 훈련된 신경망을 사용하여 식별된 관심 영역을 처리하는 단계; 및신경망에 의하여 감지 및 분류된 객체의 객체 기술자 및 이미지 내의 위치의 표시를 출력하는 단계를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>2. 청구항 1에 있어서,기준은 명시되지 않은 객체가 도로에 존재하는 것을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>3. 청구항 1 또는 청구항 2에 있어서,기준은 명시되지 않은 객체가 기결정된 최소 높이보다 큰 높이를 가지는 것을 포함하여, 신경망에 의하여 감지 및 분류되는 객체 모두가 기결정된 최소 높이보다 큰 높이를 가지는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>4. 청구항 1 내지 청구항 3 중 어느 하나의 항에 있어서,신경망에 의하여 감지 및 분류되는 객체의 위치의 표시는 이미지 내의 경계 상자를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>5. 청구항 1 내지 청구항 4 중 어느 하나의 항에 있어서,신경망에 의하여 감지 및 분류되는 객체에 대한 객체 기술자는 객체 클래스를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>6. 청구항 5에 있어서,객체 클래스는 적어도 보행자 클래스와 차량 클래스를 포함하는 객체 클래스의 세트에 속하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>7. 청구항 1 내지 청구항 6 중 어느 하나의 항에 있어서,신경망에 의하여 감지 및 분류되는 객체에 대한 객체 기술자는 객체 클래스와 서브 클래스를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>8. 청구항 1 내지 청구항 7 중 어느 하나의 항에 있어서,신경망에 의하여 감지 및 분류되는 객체는 명시되지 않은 객체 중 전부를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>9. 청구항 1 내지 청구항 8 중 어느 하나의 항에 있어서,신경망에 의하여 분류되지 않은 명시되지 않은 객체의 표시를 출력하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>10. 청구항 1 내지 청구항 9 중 어느 하나의 항에 있어서,명시되지 않은 객체 중 하나 이상이 신경망에 의하여 감지 및 분류되지 않은 경우, 상기 하나 이상의 명시되지 않은 객체의 표시를 출력하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>11. 청구항 1 내지 청구항 10 중 어느 하나의 항에 있어서,명시되지 않은 객체는 신경망이 분류할 수 없는 객체를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>12. 청구항 1 내지 청구항 11 중 어느 하나의 항에 있어서,상기 이미지를 처리하는 단계는 감독되지 않는 처리를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>13. 청구항 1 내지 청구항 12 중 어느 하나의 항에 있어서,깊이 정보를 포함하는 이미지는 RGBDV 이미지인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>14. 청구항 13에 있어서,명시되지 않은 객체의 높이를 결정하기 위하여 픽셀당 높이 맵을 계산하는 단계를 더 포함하고 기준은 명시되지 않은 객체가 기결정된 최소 높이보다 큰 높이를 가지는 것을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>15. 청구항 1 내지 청구항 13 중 어느 하나의 항에 있어서,참조 이미지 및 참조 이미지 내의 객체와 객체의 클래스에 대한 참조 데이터의 세트를 사용함으로써 신경망을 훈련하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>16. 청구항 1 내지 청구항 15 중 어느 하나의 항에 있어서,신경망은 DNN(deep neural network)을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>17. 컴퓨터 판독 가능한 명령어를 포함하는 컴퓨터 판독 가능한 저장 매체로서, 컴퓨터 판독 가능한 명령어는 컴퓨팅 장치에 의하여 구현될 때, 컴퓨팅 장치로 하여금 청구항 1 내지 청구항 16 중 어느 하나의 항에 따른 방법을 수행하도록 야기하는 컴퓨터 판독 가능한 저장 매체.</claim></claimInfo><claimInfo><claim>18. 컴퓨터 판독 가능한 명령어를 저장하는 메모리;입력/출력; 및메모리 및 입력/출력에 연결되는 처리 엔티티를 포함하고,처리 엔티티는: 입력/출력을 통하여 깊이 정보를 포함하는 이미지를 수신하고; 특정 기준을 만족하는 명시되지 않은 객체를 포함하는 이미지의 관심 영역을 식별하기 위하여 이미지를 처리하고; 장면의 알려진 객체를 감지 및 분류하도록 훈련된 신경망을 사용하여 식별된 관심 영역을 처리하고; 신경망에 의하여 감지 및 분류된 객체의 객체 기술자 및 이미지 내의 위치의 표시를 입력/출력을 통하여 출력하기 위하여 컴퓨터 판독 가능한 명령어를 수행하도록 구성되는 인지 시스템.</claim></claimInfo><claimInfo><claim>19. 참조 센서의 세트로부터 제1 훈련 신호를 수신하고 테스트 센서의 세트로부터 제2 훈련 신호를 수신하는 단계;장면과 연관되는 참조 깊이 정보를 포함하는 참조 이미지를 획득하기 위하여 제1 훈련 신호를 처리하는 단계; 및테스트 센서의 세트로부터의 후속 테스트 신호를 추론된 깊이 정보를 포함하는 테스트 이미지로 변환하기 위하여 신경망을 훈련하기 위하여 제2 훈련 신호와 참조 이미지를 사용하는 단계를 포함하고,참조 센서의 세트와 테스트 센서의 세트는 동시에 공통 장면에 노출되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>20. 청구항 19에 있어서,신경망은 파라미터의 세트로 특징지어지고, 방법은 신경망을 훈련함으로써 파라미터의 세트를 획득하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>21. 청구항 20에 있어서,파라미터는 가중치 및/또는 편향값을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>22. 청구항 19 내지 청구항 21 중 어느 하나의 항에 있어서,테스트 이미지와 참조 이미지는 RGBDV 이미지인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>23. 청구항 19 내지 청구항 22 중 어느 하나의 항에 있어서,테스트 센서의 세트는 참조 센서의 세트와 다른 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>24. 청구항 19 내지 청구항 23 중 어느 하나의 항에 있어서,테스트 센서의 세트는 참조 센서의 세트의 서브셋인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>25. 청구항 19 내지 청구항 24 중 어느 하나의 항에 있어서,테스트 센서의 세트와 참조 센서의 세트는 다른 타입인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>26. 청구항 19 내지 청구항 25 중 어느 하나의 항에 있어서,테스트 센서는 참조 센서보다 전력을 덜 소비하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>27. 청구항 19 내지 청구항 26 중 어느 하나의 항에 있어서,참조 센서의 세트는 적어도 라이다 센서 및 비-라이다 센서를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>28. 청구항 27에 있어서,참조 이미지를 획득하기 위하여 제1 훈련 신호를 처리하는 단계는 라이다 센서 및 비-라이다 센서로부터의 제1 훈련 신호에 로우 데이터 퓨전을 수행하는 단계를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>29. 청구항 19 내지 청구항 28 중 어느 하나의 항에 있어서,참조 센서의 세트는 라이다 센서를 포함하고 테스트 센서의 세트는 레이더 센서를 포함하고 라이다 센서를 포함하지 않는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>30. 청구항 19 내지 청구항 29 중 어느 하나의 항에 있어서,참조 센서의 세트는 적어도 제1 라이다 센서를 포함하고 테스트 센서의 세트는 적어도 제2 라이다 센서를 포함하고, 제1 라이다 센서는 제2 라이다 센서보다 높은 해상도, 큰 범위 또는 넓은 시야를 가지는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>31. 청구항 19 내지 청구항 30 중 어느 하나의 항에 있어서,테스트 센서는 차량에 장착되기 위한 것인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>32. 청구항 19 내지 청구항 31 중 어느 하나의 항에 있어서,장면에 대한 테스트 센서의 추정된 위치를 결정하는 단계를 더 포함하고, 신경망은 상기 추정된 위치를 고려하도록 구성되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>33. 청구항 19 내지 청구항 32 중 어느 하나의 항에 있어서,참조 이미지는 상기 장면과 연관되는 참조 속력 정보를 더 포함하고 테스트 이미지는 추론된 속력 정보를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>34. 테스트 센서의 세트로부터의 테스트 신호를 추론된 깊이 정보를 포함하는 테스트 이미지로 변환하기 위하여 신경망을 사용하는 단계를 포함하고, 신경망은 청구항 19 내지 청구항 33 중 어느 하나의 항에 정의된 컴퓨터 구현 방법에 따라 훈련된 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>35. 청구항 34에 있어서,신경망은 제1 신경망이고, 방법은:특정 기준을 만족하는 명시되지 않은 객체를 포함하는 테스트 이미지 중 주어진 하나의 관심 영역을 식별하기 위하여 테스트 이미지 중 주어진 하나를 처리하는 단계;알려진 객체를 감지 및 분류하도록 훈련된 제2 신경망을 사용하여 식별된 관심 영역을 처리하는 단계; 및제2 신경망에 의하여 감지 및 분류된 객체의 객체 기술자 및 테스트 이미지 중 주어진 하나 내의 위치의 표시를 출력하는 단계를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>36. 청구항 35에 있어서,장면은 도로를 포함하고 기준은 명시되지 않은 객체가 도로에 존재하는 것을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>37. 청구항 36에 있어서,기준은 명시되지 않은 객체가 기결정된 최소 높이보다 큰 높이를 가지는 것을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>38. 청구항 35 내지 청구항 37 중 어느 하나의 항에 있어서,제2 신경망에 의하여 감지 및 분류되는 객체의 위치의 표시는 이미지 내의 경계 상자를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>39. 청구항 35 내지 청구항 38 중 어느 하나의 항에 있어서,제2 신경망에 의하여 감지 및 분류되는 객체에 대한 객체 기술자는 객체 클래스를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>40. 청구항 39에 있어서,객체 클래스는 적어도 보행자 클래스와 차량 클래스를 포함하는 객체 클래스의 세트에 속하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>41. 청구항 35 내지 청구항 40 중 어느 하나의 항에 있어서,제2 신경망에 의하여 감지 및 분류되는 객체에 대한 객체 기술자는 객체 클래스와 서브 클래스를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>42. 청구항 35 내지 청구항 41 중 어느 하나의 항에 있어서,제2 신경망에 의하여 감지 및 분류되는 객체는 명시되지 않은 객체 중 전부를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>43. 청구항 35 내지 청구항 42 중 어느 하나의 항에 있어서,신경망에 의하여 분류되지 않은 명시되지 않은 객체의 표시를 출력하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>44. 청구항 35 내지 청구항 43 중 어느 하나의 항에 있어서,명시되지 않은 객체 중 하나 이상이 제2 신경망에 의하여 감지 및 분류되지 않은 경우, 상기 하나 이상의 명시되지 않은 객체의 표시를 출력하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>45. 청구항 35 내지 청구항 44 중 어느 하나의 항에 있어서,명시되지 않은 객체는 제2 신경망이 분류할 수 없는 객체를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>46. 청구항 35 내지 청구항 45 중 어느 하나의 항에 있어서,상기 이미지를 처리하는 단계는 감독되지 않는 처리를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>47. 청구항 35 내지 청구항 46 중 어느 하나의 항에 있어서,깊이 정보를 포함하는 이미지는 RGBDV 이미지인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>48. 청구항 47에 있어서,명시되지 않은 객체의 높이를 결정하기 위하여 픽셀당 높이 맵을 계산하는 단계를 더 포함하고 기준은 명시되지 않은 객체가 기결정된 최소 높이보다 큰 높이를 가지는 것을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>49. 청구항 35 내지 청구항 48 중 어느 하나의 항에 있어서,참조 이미지 및 참조 이미지 내의 객체와 객체의 클래스에 대한 참조 데이터의 세트를 사용함으로써 제2 신경망을 훈련하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>50. 청구항 35 내지 청구항 49 중 어느 하나의 항에 있어서,상기 기준을 만족하는 명시되지 않은 객체를 포함하는 참조 이미지의 관심 영역을 식별하기 위하여 참조 이미지 중 주어진 하나를 처리하는 단계; 및참조 이미지의 관심 영역 및 참조 이미지의 관심 영역의 객체와 객체의 클래스에 대한 참조 데이터를 사용하여 제2 신경망을 훈련하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>51. 청구항 50에 있어서,제2 신경망은 파라미터의 세트로 특징지어지고, 방법은 상기 제2 신경망을 훈련하는 단계에 의하여 파라미터의 세트를 획득하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>52. 청구항 35 내지 청구항 51 중 적어도 하나에 있어서,신경망은 테스트 신호를 테스트 이미지로 실시간으로 변환하도록 구성되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>53. 청구항 35 내지 청구항 52 중 어느 하나의 항에 있어서,신경망은 DNN(deep neural network)을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>54. 컴퓨터 판독 가능한 명령어를 포함하는 컴퓨터 판독 가능한 저장 매체로서, 컴퓨터 판독 가능한 명령어는 컴퓨팅 장치에 의하여 구현될 때, 컴퓨팅 장치로 하여금 청구항 35 내지 청구항 53 중 어느 하나의 항에 따른 방법을 수행하도록 야기하는 컴퓨터 판독 가능한 저장 매체.</claim></claimInfo><claimInfo><claim>55. 참조 센서의 세트;테스트 센서의 세트; 및컴퓨팅 장치로서: 참조 센서의 세트로부터 제1 훈련 신호를 수신하고 테스트 센서의 세트로부터 제2 훈련 신호를 수신하고; 장면과 연관되는 참조 깊이 정보를 포함하는 참조 이미지를 획득하기 위하여 제1 훈련 신호를 처리하고; 및 테스트 센서의 세트로부터의 후속 테스트 신호를 추론된 깊이 정보를 포함하는 테스트 이미지로 변환하기 위하여 신경망을 훈련하기 위하여 제2 훈련 신호와 참조 이미지를 사용하도록 구성되는 컴퓨팅 장치를 포함하고,참조 센서의 세트와 테스트 센서의 세트는 동시에 공통 장면에 노출되는 인지 시스템.</claim></claimInfo><claimInfo><claim>56. 테스트 센서의 세트로부터 획득한 실시간 신호를 깊이 정보를 포함하는 이미지로 변환하기 위하여 신경망을 사용하는 단계;실시간 신호 및/또는 이미지의 특징적인 특성을 계산하는 단계;테스트 특징적인 특성을 참조 특징적인 특성과 비교하는데 기반하여 신경망의 성능을 검증하는 단계; 및상기 검증의 결과를 나타내는 신호를 출력하는 단계를 포함하고,신경망은 신경망 파라미터의 세트에 의하여 특징지어지고,특징적인 특성은 테스트 특징적인 특성이고,참조 특징적인 특성은 신경망 파라미터의 세트와 연관되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>57. 청구항 56에 있어서,신경망 파라미터의 세트에 기반하여 메모리로부터 참조 특징적인 특성을 획득하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>58. 청구항 57에 있어서,파라미터는 가중치 및/또는 편향값을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>59. 청구항 56 내지 청구항 58 중 어느 하나의 항에 있어서,신경망을 훈련함으로써 신경망 파라미터의 세트를 획득하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>60. 청구항 56 내지 청구항 59 중 어느 하나의 항에 있어서,신경망 파라미터의 세트를 도출하기 위하여 신경망을 훈련하기 위하여 장면의 훈련 신호의 세트와 참조 이미지의 세트를 사용하는 단계를 더 포함하고, 참조 이미지는 장면과 연관된 참조 깊이 정보를 포함하고, 훈련 신호는 테스트 센서가 상기 장면에 노출되면 테스트 센서의 세트로부터 수신되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>61. 청구항 60에 있어서,참조 이미지는 상기 장면에 노출되는 참조 신호의 세트에 데이터 퓨전을 수행함으로써 획득되는 RGBDV 이미지인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>62. 청구항 60에 있어서,테스트 특징적인 특성은 실시간 신호의 특징적인 특성을 포함하고, 참조 특징적인 특성은 상기 훈련 신호로부터 도출되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>63. 청구항 62에 있어서,특징적인 특성은 훈련 신호의 통계적 특성을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>64. 청구항 63에 있어서,훈련 신호의 통계적 특성은 평균, 분산, 표준 편차 및 최대값 중 적어도 하나를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>65. 청구항 60에 있어서,테스트 특징적인 특성은 출력 이미지의 특징적인 특성을 포함하고, 참조 특징적인 특성은 상기 참조 이미지로부터 도출되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>66. 청구항 65에 있어서,특징적인 특성은 참조 이미지의 통계적 특성을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>67. 청구항 66에 있어서,참조 이미지의 통계적 특성은 평균, 분산, 표준 편차 및 최대값 중 적어도 하나를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>68. 청구항 56 내지 청구항 67 중 어느 하나의 항에 있어서,특정 기준을 만족하는 명시되지 않은 객체를 포함하는 이미지의 관심 영역을 식별하는 단계;장면의 알려진 객체를 감지 및 분류하도록 훈련된 제2 신경망을 사용하여 식별된 관심 영역을 처리하는 단계; 및제2 신경망에 의하여 감지 및 분류된 객체의 객체 기술자 및 이미지 내의 위치의 표시를 출력하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>69. 청구항 68에 있어서,이미지는 테스트 이미지이고, 방법은:상기 기준을 만족하는 명시되지 않은 객체를 포함하는 참조 이미지의 관심 영역을 식별하기 위하여 참조 이미지의 세트를 처리하는 단계; 및참조 이미지의 관심 영역 및 참조 이미지의 관심 영역의 객체와 객체의 클래스에 대한 참조 데이터를 사용하여 제2 신경망을 훈련하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>70. 청구항 69에 있어서,테스트 이미지의 특징적인 특성을 계산하는 단계;테스트 이미지의 특징적인 특성을 참조 이미지의 특징적인 특성과 비교하는데 기반하여 신경망의 성능을 검증하는 단계; 및상기 검증의 결과를 나타내는 신호를 출력하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>71. 청구항 56 내지 청구항 70 중 어느 하나의 항에 있어서,신경망은 DNN(deep neural network)을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>72. 컴퓨터 판독 가능한 명령어를 포함하는 컴퓨터 판독 가능한 저장 매체로서, 컴퓨터 판독 가능한 명령어는 컴퓨팅 장치에 의하여 구현될 때, 컴퓨팅 장치로 하여금 청구항 56 내지 청구항 71 중 어느 하나의 항에 따른 방법을 수행하도록 야기하는 컴퓨터 판독 가능한 저장 매체.</claim></claimInfo><claimInfo><claim>73. 테스트 센서의 세트; 및컴퓨팅 장치로서: 테스트 센서의 세트로부터 획득한 실시간 신호를 깊이 정보를 포함하는 이미지로 변환하기 위하여 신경망을 사용하고; 실시간 신호 및/또는 이미지의 특징적인 특성을 계산하고; 테스트 특징적인 특성을 참조 특징적인 특성과 비교하는데 기반하여 신경망의 성능을 검증하고; 및 상기 검증의 결과를 나타내는 신호를 출력하도록 구성되는 컴퓨팅 장치를 포함하고,신경망은 신경망 파라미터의 세트에 의하여 특징지어지고,특징적인 특성은 테스트 특징적인 특성이고,참조 특징적인 특성은 신경망 파라미터의 세트와 연관되는 인지 시스템.</claim></claimInfo><claimInfo><claim>74. 제1 센서의 세트로부터 획득한 신호를 깊이 정보를 포함하는 제1 이미지로 변환하는 단계;특정 기준을 만족하는 명시되지 않은 객체를 포함하는 관심 영역을 식별하기 위하여 제1 이미지를 처리하는 단계;제1 이미지의 관심 영역의 알려진 객체의 제1 감지 및 분류를 수행하는 단계;제2 센서의 세트로부터 획득한 신호를 깊이 정보를 포함하는 제2 이미지로 변환하기 위하여 신경망을 사용하는 단계;상기 기준을 만족하는 명시되지 않은 객체를 포함하는 관심 영역을 식별하기 위하여 제2 이미지를 처리하는 단계;제2 이미지의 관심 영역의 알려진 객체의 제2 감지 및 분류를 수행하는 단계; 및제1 객체 감지 및 분류와 제2 객체 감지 및 분류의 결과에 기반하여 신경망의 성능을 검증하는 단계를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>75. 청구항 74에 있어서,제1 감지 및 분류를 겪은 객체의 제1 객체 기술자를 출력하는 단계; 및제2 감지 및 분류를 겪은 객체의 제2 객체 기술자를 출력하는 단계를 더 포함하고,신경망의 성능을 검증하는 단계는 제1 및 제2 객체 기술자를 비교하는 단계를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>76. 청구항 75에 있어서,제1 및 제2 객체 기술자 각각은 객체 클래스를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>77. 청구항 76에 있어서,객체 클래스는 적어도 보행자 클래스와 차량 클래스를 포함하는 객체 클래스의 세트에 속하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>78. 청구항 75에 있어서,제1 및 제2 객체 기술자 각각은 객체 클래스와 서브 클래스를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>79. 청구항 74 내지 청구항 78 중 어느 하나의 항에 있어서,제1 감지 및 분류를 겪은 객체의 제1 위치 표시를 출력하는 단계; 및제2 감지 및 분류를 겪은 객체의 제2 위치 표시를 출력하는 단계를 더 포함하고,신경망의 성능을 검증하는 단계는 제1 및 제2 위치 표시자를 비교하는 단계를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>80. 청구항 79에 있어서,제1 및 제2 위치 표시자는 경계 상자를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>81. 청구항 74 내지 청구항 80 중 어느 하나의 항에 있어서,제1 감지 및 분류와 제2 감지 및 분류는 각 신경망에 의하여 수행되는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>82. 청구항 74 내지 청구항 81 중 어느 하나의 항에 있어서,제1 센서는 적어도 라이다 센서 및 적어도 비-라이다 센서를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>83. 청구항 82에 있어서,제1 센서의 세트로부터의 신호를 제1 이미지로 변환하는 단계는 라이다 센서 및 비-라이다 센서로부터의 신호에 로우 데이터 퓨전을 수행하는 단계를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>84. 청구항 73 내지 청구항 83 중 어느 하나의 항에 있어서,기준은 명시되지 않은 객체가 도로에 존재하는 것을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>85. 청구항 74 내지 청구항 84 중 어느 하나의 항에 있어서,기준은 명시되지 않은 객체가 기결정된 최소 높이보다 큰 높이를 가지는 것을 포함하여, 제1 감지 및 분류 또는 제2 감지 및 분류를 겪은 객체 모두가 기결정된 최소 높이보다 큰 높이를 가지는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>86. 청구항 74 내지 청구항 85 중 어느 하나의 항에 있어서,제1 감지 및 분류를 겪은 객체는 제1 이미지의 관심 영역의 명시되지 않은 객체 중 전부를 포함하고 제2 감지 및 분류를 겪은 객체는 제2 이미지의 관심 영역의 명시되지 않은 객체 중 전부를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>87. 청구항 74 내지 청구항 86 중 어느 하나의 항에 있어서,제1 감지 및 분류 또는 제2 감지 및 분류를 겪지 않은 적어도 하나의 명시되지 않은 객체의 표시를 출력하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>88. 청구항 74 내지 청구항 87 중 어느 하나의 항에 있어서,상기 제1 이미지를 처리하는 단계 및 상기 제2 이미지를 처리하는 단계는 감독되지 않는 처리를 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>89. 청구항 74 내지 청구항 88 중 어느 하나의 항에 있어서,제1 이미지와 제2 이미지는 RGBDV 이미지인 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>90. 청구항 89에 있어서,각 명시되지 않은 객체의 높이를 결정하기 위하여 픽셀당 높이 맵을 계산하는 단계를 더 포함하고 기준은 명시되지 않은 객체의 높이가 기결정된 최소 높이보다 큰 것을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>91. 청구항 74 내지 청구항 90 중 어느 하나의 항에 있어서,참조 이미지 및 참조 이미지 내의 객체와 객체의 클래스에 대한 참조 데이터의 세트를 사용함으로써 신경망을 훈련하는 단계를 더 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>92. 청구항 74 내지 청구항 91 중 어느 하나의 항에 있어서,신경망은 DNN(deep neural network)을 포함하는 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>93. 컴퓨터 판독 가능한 명령어를 포함하는 컴퓨터 판독 가능한 저장 매체로서, 컴퓨터 판독 가능한 명령어는 컴퓨팅 장치에 의하여 구현될 때, 컴퓨팅 장치로 하여금 청구항 74 내지 청구항 92 중 어느 하나의 항에 따른 방법을 수행하도록 야기하는 컴퓨터 판독 가능한 저장 매체.</claim></claimInfo><claimInfo><claim>94. 제1 센서의 세트;제2 센서의 세트;컴퓨팅 장치로서: 제1 센서의 세트로부터 획득한 신호를 깊이 정보를 포함하는 제1 이미지로 변환하고; 특정 기준을 만족하는 명시되지 않은 객체를 포함하는 관심 영역을 식별하기 위하여 제1 이미지를 처리하고; 제1 이미지의 관심 영역의 알려진 객체의 제1 감지 및 분류를 수행하고; 제2 센서의 세트로부터 획득한 신호를 깊이 정보를 포함하는 제2 이미지로 변환하기 위하여 신경망을 사용하고; 상기 기준을 만족하는 명시되지 않은 객체를 포함하는 관심 영역을 식별하기 위하여 제2 이미지를 처리하고; 제2 이미지의 관심 영역의 알려진 객체의 제2 감지 및 분류를 수행하고; 및 제1 객체 감지 및 분류와 제2 객체 감지 및 분류의 결과에 기반하여 신경망의 성능을 검증하도록 구성되는 인지 시스템.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>이스라엘 ******* 오르 예후다 요나탄 네타냐후 *</address><code>520220216761</code><country>이스라엘</country><engName>VAYAVISION SENSING LTD.</engName><name>바야비전 센싱 리미티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>이스라엘 ******...</address><code> </code><country> </country><engName>NEHMADI, Youval</engName><name>네흐마디 유발</name></inventorInfo><inventorInfo><address>이스라엘 ******* 오르예후다 요...</address><code> </code><country> </country><engName>BEN EZRA, Shahar</engName><name>벤 에즈라 사하르</name></inventorInfo><inventorInfo><address>이스라엘 *******...</address><code> </code><country> </country><engName>MANGAN, Shmuel</engName><name>망간 슈무엘</name></inventorInfo><inventorInfo><address>이스라엘 ****** ...</address><code> </code><country> </country><engName>WAGNER, Mark</engName><name>바그너 마크</name></inventorInfo><inventorInfo><address>이스라엘 *******...</address><code> </code><country> </country><engName>COHEN, Anna</engName><name>코헨 안나</name></inventorInfo><inventorInfo><address>이스라엘 ******* ...</address><code> </code><country> </country><engName>AVITAL, Itzik</engName><name>아비탈 잇직크</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 송파구 올림픽로 ** (잠실현대빌딩 *층)</address><code>920191002615</code><country>대한민국</country><engName>KBK &amp;amp; Associates</engName><name>특허법인(유한)케이비케이</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2019.09.22</priorityApplicationDate><priorityApplicationNumber>62/903,846</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2022.04.08</receiptDate><receiptNumber>1-1-2022-0377565-67</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Request for Amendment</documentEngName><documentName>보정요구서</documentName><receiptDate>2022.04.25</receiptDate><receiptNumber>1-5-2022-0062696-23</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Amendment to Patent Application, etc.] Amendment</documentEngName><documentName>[출원서 등 보정]보정서</documentName><receiptDate>2022.06.17</receiptDate><receiptNumber>1-1-2022-0634385-52</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2022.06.20</receiptDate><receiptNumber>1-5-2022-0091599-72</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2023.08.04</receiptDate><receiptNumber>1-1-2023-0861507-39</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2023.08.04</receiptDate><receiptNumber>1-1-2023-0861508-85</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020227011746.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93f3f59f2a90591463136989e33854cd3fc11d83ae506516c8517a3162df8bc733e440ce9ec73eb199ce6fc9ff30aead377960faad5bcebab7</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cffd3e840d629cd35d82166f557f59d04af98bab625103fbe0cf9ba24fc5be0d5ca5cd78919aee043bc8c50a0b7d82f87812e41c3ef354fc73</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>