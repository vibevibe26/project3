<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:08:05.85</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.12.21</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-7019657</applicationNumber><claimCount>30</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>이미지 재투영을 위한 시스템들 및 방법들</inventionTitle><inventionTitleEng>SYSTEMS AND METHODS FOR IMAGE REPROJECTION</inventionTitleEng><openDate>2024.08.29</openDate><openNumber>10-2024-0130687</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.06.12</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 15/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/77</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/64</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/74</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>H04N 13/111</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 이미징 시스템은 깊이 센서로부터 (환경에 대응하는) 깊이 데이터를 그리고 이미지 센서로부터 제1 이미지 데이터(환경의 묘사)를 수신한다. 이미징 시스템은, 깊이 데이터에 기초하여, 제1 이미지 데이터 내의 환경의 묘사의 관점의 변경에 대응하는 제1 모션 벡터들을 생성한다. 이미징 시스템은, 제1 모션 벡터들에 기초한 그리드 반전을 사용하여, 관점의 변경을 위해 제1 이미지 데이터 내의 환경의 묘사의 각자의 픽셀들만큼 이동되는 각자의 거리를 나타내는 제2 모션 벡터들을 생성한다. 이미징 시스템은 제2 모션 벡터들에 따라 제1 이미지 데이터를 수정함으로써 제2 이미지 데이터를 생성한다. 제2 이미지 데이터는 제1 이미지 데이터와는 상이한 관점으로부터의 환경의 제2 묘사를 포함한다. 일부 이미지 재투영 응용들(예컨대, 프레임 보간)이 깊이 데이터 없이 수행될 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2023.07.06</internationOpenDate><internationOpenNumber>WO2023129855</internationOpenNumber><internationalApplicationDate>2022.12.21</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/082189</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 이미지 프로세싱을 위한 장치로서,적어도 하나의 메모리; 및상기 적어도 하나의 메모리에 커플링된 적어도 하나의 프로세서를 포함하며, 상기 적어도 하나의 프로세서는,환경에 대응하는 깊이 정보를 포함하는 깊이 데이터를 수신하도록;이미지 센서에 의해 캡처되는 제1 이미지 데이터를 수신하도록 - 상기 제1 이미지 데이터는 상기 환경의 묘사를 포함함 -;적어도 상기 깊이 데이터에 기초하여, 상기 제1 이미지 데이터 내의 상기 환경의 묘사의 관점의 변경에 대응하는 제1 복수의 모션 벡터들을 생성하도록;상기 제1 복수의 모션 벡터들에 기초한 그리드 반전(grid inversion)을 사용하여, 상기 관점의 변경을 위해 상기 제1 이미지 데이터 내의 상기 환경의 묘사의 각자의 픽셀들만큼 이동되는 각자의 거리를 나타내는 제2 복수의 모션 벡터들을 생성하도록;적어도 부분적으로 상기 제2 복수의 모션 벡터들에 따라 상기 제1 이미지 데이터를 수정함으로써 제2 이미지 데이터를 생성하도록 - 상기 제2 이미지 데이터는 상기 제1 이미지 데이터와는 상이한 관점으로부터의 상기 환경의 제2 묘사를 포함함 -; 그리고상기 제2 이미지 데이터를 출력하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 제2 이미지 데이터는 제1 시간과 제3 시간 사이의 제2 시간에 상기 환경을 묘사하도록 구성되는 보간된 이미지를 포함하고, 상기 제1 이미지 데이터는 적어도 상기 제1 시간 또는 상기 제3 시간 중 하나에서 상기 환경을 묘사하는 적어도 하나의 이미지를 포함하는, 장치.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서, 상기 제1 이미지 데이터는 시차 이동(parallax movement)을 포함하는 복수의 비디오 데이터의 프레임들을 포함하고, 상기 제2 이미지 데이터는 상기 시차 이동을 감소시킨 상기 복수의 비디오 데이터의 프레임들의 안정화된 변형을 포함하는, 장치.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 제1 이미지 데이터는 사람이 제1 각도로부터 상기 이미지 센서를 보는 것을 포함하고, 상기 제2 이미지 데이터는 상기 사람이 상기 제1 각도와는 구별되는 제2 각도로부터 상기 이미지 센서를 보는 것을 포함하는, 장치.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서, 관점의 변경은 각도에 따른 그리고 축을 중심으로 하는 관점의 회전을 포함하는, 장치.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 관점의 변경은 방향 및 거리에 따른 관점의 병진을 포함하는, 장치.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서, 관점의 변경은 변환(transformation)을 포함하는, 장치.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서, 상기 관점의 변경은 상기 제1 이미지 데이터 내의 상기 환경의 묘사의 오리지널 관점과 상기 환경 내의 객체의 포지션 사이의 축을 따르는 이동을 포함하고, 상기 객체의 적어도 일부분은 상기 제1 이미지 데이터에 묘사되는, 장치.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서, 상기 적어도 하나의 프로세서는,상기 제2 복수의 모션 벡터들 내의 하나 이상의 갭(gap)들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 갭들을 식별하도록; 그리고상기 제2 이미지 데이터를 출력하기 전에 보간을 사용하여 적어도 부분적으로 상기 제2 이미지 데이터 내의 상기 하나 이상의 갭들을 채움으로써 상기 제2 이미지 데이터를 수정하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서, 상기 적어도 하나의 프로세서는,상기 제2 복수의 모션 벡터들 내의 하나 이상의 갭들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 폐색부(occlusion) 영역들을 식별하도록; 그리고상기 제2 이미지 데이터를 출력하기 전에 인페인팅(inpainting)을 사용하여 적어도 부분적으로 상기 제2 이미지 데이터 내의 상기 하나 이상의 갭들을 채움으로써 상기 제2 이미지 데이터를 수정하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>11. 제1항에 있어서, 상기 적어도 하나의 프로세서는,상기 제2 복수의 모션 벡터들 내의 하나 이상의 갭들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 폐색부 영역들을 식별하도록; 그리고상기 제2 이미지 데이터를 출력하기 전에 하나 이상의 트레이닝된 머신 러닝 모델(trained machine learning model)들을 사용하는 인페이팅을 사용하여 적어도 부분적으로 상기 제2 이미지 데이터 내의 상기 하나 이상의 갭들을 채움으로써 상기 제2 이미지 데이터를 수정하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>12. 제1항에 있어서, 상기 적어도 하나의 프로세서는,상기 제2 복수의 모션 벡터들 내의 상기 제1 이미지 데이터로부터의 하나 이상의 충돌 값(conflicting value)들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 충돌들을 식별하도록; 그리고상기 제2 복수의 모션 벡터들과 연관된 이동 데이터에 기초하여 상기 제1 이미지 데이터로부터의 상기 하나 이상의 충돌 값들 중 하나를 선택하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>13. 제1항에 있어서, 상기 깊이 정보는 제1 관점으로부터의 환경의 3차원 표현을 포함하는, 장치.</claim></claimInfo><claimInfo><claim>14. 제1항에 있어서, 상기 깊이 데이터는 적어도 하나의 깊이 센서로부터 수신되는, 장치.</claim></claimInfo><claimInfo><claim>15. 제1항에 있어서,디스플레이를 추가로 포함하고, 상기 제2 이미지 데이터를 출력하기 위해, 상기 적어도 하나의 프로세서는 적어도 상기 디스플레이를 사용하여 상기 제2 이미지 데이터를 디스플레이하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>16. 제1항에 있어서,통신 인터페이스를 추가로 포함하고, 상기 제2 이미지 데이터를 출력하기 위해, 상기 적어도 하나의 프로세서는 적어도 상기 통신 인터페이스를 사용하여 적어도 수신자 디바이스로 적어도 상기 제2 이미지 데이터를 전송하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>17. 제1항에 있어서, 상기 장치는 머리 장착형 디스플레이(head-mounted display, HMD), 모바일 핸드셋, 또는 무선 통신 디바이스 중 적어도 하나를 포함하는, 장치.</claim></claimInfo><claimInfo><claim>18. 이미지 프로세싱을 위한 방법으로서,환경에 대응하는 깊이 정보를 포함하는 깊이 데이터를 수신하는 단계;이미지 센서에 의해 캡처되는 제1 이미지 데이터를 수신하는 단계 - 상기 제1 이미지 데이터는 상기 환경의 묘사를 포함함 -;적어도 상기 깊이 데이터에 기초하여, 상기 제1 이미지 데이터 내의 상기 환경의 묘사의 관점의 변경에 대응하는 제1 복수의 모션 벡터들을 생성하는 단계;상기 제1 복수의 모션 벡터들에 기초한 그리드 반전을 사용하여, 상기 관점의 변경을 위해 상기 제1 이미지 데이터 내의 상기 환경의 묘사의 각자의 픽셀들만큼 이동되는 각자의 거리를 나타내는 제2 복수의 모션 벡터들을 생성하는 단계;적어도 부분적으로 상기 제2 복수의 모션 벡터들에 따라 상기 제1 이미지 데이터를 수정함으로써 제2 이미지 데이터를 생성하는 단계 - 상기 제2 이미지 데이터는 상기 제1 이미지 데이터와는 상이한 관점으로부터의 상기 환경의 제2 묘사를 포함함 -; 및상기 제2 이미지 데이터를 출력하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서, 상기 제2 이미지 데이터는 제1 시간과 제3 시간 사이의 제2 시간에 상기 환경을 묘사하도록 구성되는 보간된 이미지를 포함하고, 상기 제1 이미지 데이터는 적어도 상기 제1 시간 또는 상기 제3 시간 중 하나에서 상기 환경을 묘사하는 적어도 하나의 이미지를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>20. 제18항에 있어서, 상기 제1 이미지 데이터는 시차 이동을 포함하는 복수의 비디오 데이터의 프레임들을 포함하고, 상기 제2 이미지 데이터는 상기 시차 이동을 감소시킨 상기 복수의 비디오 데이터의 프레임들의 안정화된 변형을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>21. 제18항에 있어서, 상기 제1 이미지 데이터는 사람이 제1 각도로부터 상기 이미지 센서를 보는 것을 포함하고, 상기 제2 이미지 데이터는 상기 사람이 상기 제1 각도와는 구별되는 제2 각도로부터 상기 이미지 센서를 보는 것을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>22. 제18항에 있어서, 관점의 변경은 각도에 따른 그리고 축을 중심으로 하는 관점의 회전을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>23. 제18항에 있어서, 관점의 변경은 방향 및 거리에 따른 관점의 병진을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 제18항에 있어서, 관점의 변경은 변환을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>25. 제18항에 있어서, 상기 관점의 변경은 상기 제1 이미지 데이터 내의 상기 환경의 묘사의 오리지널 관점과 상기 환경 내의 객체의 포지션 사이의 축을 따르는 이동을 포함하고, 상기 객체의 적어도 일부분은 상기 제1 이미지 데이터에 묘사되는, 방법.</claim></claimInfo><claimInfo><claim>26. 제18항에 있어서,상기 제2 복수의 모션 벡터들 내의 하나 이상의 갭들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 갭들을 식별하는 단계; 및상기 제2 이미지 데이터를 출력하기 전에 보간을 사용하여 적어도 부분적으로 상기 제2 이미지 데이터 내의 상기 하나 이상의 갭들을 채움으로써 상기 제2 이미지 데이터를 수정하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>27. 제18항에 있어서,상기 제2 복수의 모션 벡터들 내의 하나 이상의 갭들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 폐색부 영역들을 식별하는 단계; 및상기 제2 이미지 데이터를 출력하기 전에 인페인팅을 사용하여 적어도 부분적으로 상기 제2 이미지 데이터 내의 상기 하나 이상의 갭들을 채움으로써 상기 제2 이미지 데이터를 수정하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>28. 제18항에 있어서,상기 제2 복수의 모션 벡터들 내의 하나 이상의 갭들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 폐색부 영역들을 식별하는 단계; 및상기 제2 이미지 데이터를 출력하기 전에 하나 이상의 트레이닝된 머신 러닝 모델들을 사용하는 인페인팅을 사용하여 적어도 부분적으로 상기 제2 이미지 데이터 내의 상기 하나 이상의 갭들을 채움으로써 상기 제2 이미지 데이터를 수정하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>29. 제18항에 있어서,상기 제2 복수의 모션 벡터들 내의 상기 제1 이미지 데이터로부터의 하나 이상의 충돌 값들에 기초하여 상기 제2 이미지 데이터에서 하나 이상의 충돌들을 식별하는 단계; 및상기 제2 복수의 모션 벡터들과 연관된 이동 데이터에 기초하여 상기 제1 이미지 데이터로부터의 상기 하나 이상의 충돌 값들 중 하나를 선택하는 단계를 추가로 포함하는, 방법.</claim></claimInfo><claimInfo><claim>30. 제18항에 있어서, 상기 제2 이미지 데이터를 출력하는 단계는 적어도 디스플레이를 사용하여 상기 제2 이미지 데이터가 디스플레이되게 하는 단계를 포함하는, 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 *****-**** 캘리포니아주 샌 디에고 모어하우스 드라이브 ****</address><code>519980804600</code><country>미국</country><engName>Qualcomm Incorporated</engName><name>퀄컴 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>ZOBEL, PIA</engName><name>조벨 피아</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>SCHWARTZ, YUVAL</engName><name>슈왈츠 유발</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>ZADIK, TAL</engName><name>자딕 탈</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>MARTSIANO, ITSCHAK</engName><name>마르치아노 이차크</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>HARDOON, ROEE</engName><name>하르둔 로에</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>TZUR, MEIR</engName><name>추르 메이르</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>GAIZMAN, RON</engName><name>가이즈만 론</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>PASTERNAK, YEHUDA</engName><name>파스터낙 예후다</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 강남대로 **길 **(역삼동, 케이피빌딩)</address><code>920011000013</code><country>대한민국</country><engName>Koreana Patent Firm</engName><name>특허법인코리아나</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.12.31</priorityApplicationDate><priorityApplicationNumber>63/266,316</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.09.09</priorityApplicationDate><priorityApplicationNumber>17/931,063</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2024.06.12</receiptDate><receiptNumber>1-1-2024-0634954-01</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2024.08.01</receiptDate><receiptNumber>1-5-2024-0125935-21</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247019657.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93d26470aab544a6c6ad1690a98faa19ad134fb4f4f1ccfa39489e8ae041e7ef5d2de6b991642b1388635109b84d287eb0da1d3964fd7f9036</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfa484949845b9ab8c0fb0b947cff687fea0bd502f575f94c272ce09c9bd228e98d52c21ea0b256f6a9efaf3b54bf5f63b1eeb7f3080fb9037</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>