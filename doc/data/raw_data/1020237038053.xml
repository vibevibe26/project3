<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:02:13.213</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.03.08</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7038053</applicationNumber><claimCount>30</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>사용자 장비(UE)의 포지션의 패시브한 결정</inventionTitle><inventionTitleEng>PASSIVELY DETERMINING A POSITION OF A USER EQUIPMENT</inventionTitleEng><openDate>2024.01.12</openDate><openNumber>10-2024-0005730</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.02.17</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.11.03</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2021.01.01)</ipcDate><ipcNumber>H04M 1/72454</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G01C 21/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 1/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 1/3231</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G06F 3/0346</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06F 3/0488</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 일부 양상들에서, 사용자 장비(UE)는 관성 측정 유닛을 사용하여 UE의 배향을 결정하고, 주변 광 센서들을 사용하여 UE의 주변 광 상태를 결정한다. UE는 기계 학습 모듈을 사용하여 그리고 배향 및 주변 광 상태에 기초하여, UE의 포지션을 결정한다. 포지션이 신체-상 포지션을 포함하는 경우, UE는, 기계 학습 모듈 및 UE의 터치스크린에 의해 수신된 터치 데이터를 사용하여, 포지션이 손-안 포지션을 포함하는지 여부를 결정한다. 포지션이 손-안 포지션을 포함하는 경우, UE는, 기계 학습 모듈을 사용하여 그리고 배향 및 터치 데이터에 기초하여, 그립 모드를 결정한다. 포지션이 신체-외 포지션을 포함하는 경우, UE는, 관성 측정 유닛 또는 주변 광 센서들 중 적어도 하나와 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.11.17</internationOpenDate><internationOpenNumber>WO2022241341</internationOpenNumber><internationalApplicationDate>2022.03.08</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/071026</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 사용자 장비(UE)의 포지션을 패시브하게(passively) 결정하기 위한 방법으로서,상기 사용자 장비의 관성 측정 유닛을 사용하여, 상기 사용자 장비의 배향(orientation)을 결정하는 단계;상기 사용자 장비의 주변 광 센서를 사용하여, 상기 사용자 장비와 연관된 주변 광 상태를 결정하는 단계;상기 사용자 장비의 기계 학습 모듈을 사용하여, 상기 배향 및 상기 주변 광 상태에 기초하여, 상기 사용자 장비의 포지션을 결정하는 단계;상기 포지션이 신체-상(on-body) 포지션을 포함함을 결정하는 것에 기초하여, 상기 기계 학습 모듈을 사용하여 그리고 상기 사용자 장비의 터치스크린에 의해 수신된 터치 데이터에 기초하여, 상기 포지션이 손-안(in-hand) 포지션을 포함하는지 여부를 결정하는 단계;상기 포지션이 손-안 포지션을 포함함을 결정하는 것에 기초하여, 상기 기계 학습 모듈을 사용하여 그리고 상기 배향 및 상기 터치 데이터에 적어도 부분적으로 기초하여, 상기 사용자 장비와 연관된 그립 모드를 결정하는 단계; 및상기 포지션이 신체-외(off-body) 포지션을 포함함을 결정하는 것에 기초하여, 상기 관성 측정 유닛 또는 상기 주변 광 센서 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하는 단계를 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 관성 측정 유닛 또는 상기 주변 광 센서 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하는 단계는, 상기 관성 측정 유닛으로부터 진동 데이터를 수신하는 단계; 필터링된 진동 데이터를 생성하기 위해 상기 진동 데이터를 필터링하는 단계; 상기 기계 학습 모듈을 사용하여, 상기 필터링된 진동 데이터가 사용자 움직임을 표시함을 결정하는 것에 기초하여, 상기 사용자 존재를 결정하는 단계; 및 상기 기계 학습 모듈을 사용하여, 상기 필터링된 진동 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 사용자 부재를 결정하는 단계를 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>3. 제1 항에 있어서,상기 관성 측정 유닛 또는 상기 주변 광 센서 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하는 단계는, 상기 주변 광 센서로부터 주변 광 데이터를 수신하는 단계; 상기 기계 학습 모듈을 사용하여, 상기 주변 광 데이터가 사용자 움직임을 표시함을 결정하는 것에 기초하여, 상기 사용자 존재를 결정하는 단계; 및 상기 기계 학습 모듈을 사용하여, 상기 주변 광 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 사용자 부재를 결정하는 단계를 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>4. 제1 항에 있어서,상기 포지션이 상기 손-안 포지션을 포함함을 결정하는 것에 기초하여, 상기 그립 모드와 연관된 그립 포인트들에 근접한 상기 사용자 장비의 하나 이상의 송신 안테나들을 튜닝하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>5. 제1 항에 있어서,상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여, 사용자 존재 및 사용자 부재를 다른 디바이스들에 통신하는 단계를 더 포함하며, 상기 다른 디바이스들은 상기 사용자 존재 및 상기 사용자 부재를 결정할 수 없는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>6. 제1 항에 있어서,상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 부재를 결정하는 것에 기초하여, 상기 사용자 장비와 연관된 통지들을 디스에이블(disable)하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>7. 제6 항에 있어서,상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 존재를 결정하는 것에 기초하여, 상기 사용자 장비와 연관된 통지들을 인에이블(enable)하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>8. 제1 항에 있어서,상기 포지션이 상기 신체-상 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 그립 모드를 결정하는 것에 기초하여, 상기 그립 모드의 그립 포인트들에 기초하여 상기 사용자 장비의 사용자 인터페이스(UI)의 그래픽 컴포넌트들을 배열하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>9. 제1 항에 있어서,상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 존재를 결정하는 것에 기초하여, 상기 사용자 장비의 카메라 센서를 선택하고, 얼굴 인식에 기초한 사용자 인증 프로세스를 개시하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>10. 제1 항에 있어서,상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 존재를 결정하는 것에 기초하여, 상기 사용자 장비의 마이크(microphone)를 선택하고, 음성 인식에 기초한 사용자 인증 프로세스를 개시하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>11. 제1 항에 있어서,중간(intervening) 손-안 포지션을 결정하지 않고 상기 포지션이 상기 신체-상 포지션으로부터 상기 신체-외 포지션으로 트랜지션(transition)됨을 결정하는 것에 기초하여, 상기 사용자 장비의 스피커를 사용하여, 청취 가능한 경보를 생성하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>12. 제1 항에 있어서,건강 경보 모드가 인에이블됨을 결정하는 것에 기초하여, 상기 사용자 장비의 현재 포지션 및 상기 현재 포지션과 연관된 현재 시간을 결정하는 단계;상기 사용자 장비와 연관된 포지션 이력에 상기 현재 포지션 및 상기 현재 시간을 저장하는 단계; 및상기 현재 포지션과 연관된 상기 현재 시간과 이전 포지션과 연관된 이전 시간 사이의 시간 길이가 시간 임계치를 만족시킴을 결정하는 것에 기초하여, 통지를 자동으로 생성하는 단계를 더 포함하는, 사용자 장비(UE)의 포지션을 패시브하게 결정하기 위한 방법.</claim></claimInfo><claimInfo><claim>13. 장치로서,메모리;통신 인터페이스; 및상기 메모리 및 상기 통신 인터페이스에 통신 가능하게 커플링된 적어도 하나의 프로세서를 포함하며,상기 적어도 하나의 프로세서는, 상기 장치의 관성 측정 유닛을 사용하여, 상기 장치의 배향을 결정하도록; 상기 장치의 주변 광 센서를 사용하여, 상기 장치와 연관된 주변 광 상태를 결정하도록; 상기 장치의 기계 학습 모듈을 사용하여 그리고 상기 배향 및 상기 주변 광 상태에 기초하여, 상기 장치의 포지션을 결정하도록; 상기 포지션이 신체-상 포지션을 포함함을 결정하는 것에 기초하여, 상기 기계 학습 모듈을 사용하여 그리고 상기 장치의 터치스크린에 의해 수신된 터치 데이터에 기초하여, 상기 포지션이 손-안 포지션을 포함하는지 여부를 결정하도록; 상기 포지션이 손-안 포지션을 포함함을 결정하는 것에 기초하여, 상기 기계 학습 모듈을 사용하여 그리고 상기 배향 및 상기 터치 데이터에 적어도 부분적으로 기초하여, 상기 장치와 연관된 그립 모드를 결정하도록; 그리고 상기 포지션이 신체-외 포지션을 포함함을 결정하는 것에 기초하여, 상기 관성 측정 유닛 또는 상기 주변 광 센서 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하도록 구성되는, 장치.</claim></claimInfo><claimInfo><claim>14. 제13 항에 있어서,상기 관성 측정 유닛 또는 상기 주변 광 센서 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하도록 구성되는 상기 적어도 하나의 프로세서는, 상기 통신 인터페이스를 통해, 상기 관성 측정 유닛으로부터 진동 데이터를 수신하도록; 필터링된 진동 데이터를 생성하기 위해 상기 진동 데이터를 필터링하도록; 상기 기계 학습 모듈을 사용하여, 상기 필터링된 진동 데이터가 사용자 움직임을 표시함을 결정하는 것에 기초하여, 상기 사용자 존재를 결정하도록; 그리고 상기 기계 학습 모듈을 사용하여, 상기 필터링된 진동 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 사용자 부재를 결정하도록 구성되는 상기 적어도 하나의 프로세서를 포함하는, 장치.</claim></claimInfo><claimInfo><claim>15. 제13 항에 있어서,상기 관성 측정 유닛 또는 상기 주변 광 센서 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하도록 구성되는 상기 적어도 하나의 프로세서는, 상기 통신 인터페이스를 통해, 상기 주변 광 센서로부터 주변 광 데이터를 수신하도록; 상기 기계 학습 모듈을 사용하여, 상기 주변 광 데이터가 사용자 움직임을 표시함을 결정하는 것에 기초하여, 상기 사용자 존재를 결정하도록; 그리고 상기 기계 학습 모듈을 사용하여, 상기 주변 광 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 사용자 부재를 결정하도록 구성되는 상기 적어도 하나의 프로세서를 포함하는, 장치.</claim></claimInfo><claimInfo><claim>16. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 상기 포지션이 상기 손-안 포지션을 포함함을 결정하는 것에 기초하여, 상기 그립 모드와 연관된 그립 포인트들에 근접한 상기 장치의 하나 이상의 송신 안테나들을 결정하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>17. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여, 사용자 존재 및 사용자 부재를 다른 디바이스들에 통신하도록 추가로 구성되며,상기 다른 디바이스들은 상기 사용자 존재 및 상기 사용자 부재를 결정할 수 없는, 장치.</claim></claimInfo><claimInfo><claim>18. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 부재를 결정하는 것에 기초하여, 상기 장치와 연관된 통지들을 디스에이블하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>19. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 존재를 결정하는 것에 기초하여, 상기 장치와 연관된 통지들을 인에이블하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>20. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 상기 포지션이 상기 신체-상 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 그립 모드를 결정하는 것에 기초하여, 상기 그립 모드의 그립 포인트들에 기초하여 상기 장치의 사용자 인터페이스(UI)의 그래픽 컴포넌트들을 배열하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>21. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 존재를 결정하는 것에 기초하여, 자동으로 상기 장치의 카메라 센서를 선택하고, 얼굴 인식에 기초한 사용자 인증 프로세스를 개시하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>22. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 상기 포지션이 상기 신체-외 포지션을 포함함을 결정하는 것에 기초하여 그리고 상기 사용자 존재를 결정하는 것에 기초하여, 상기 장치의 마이크를 선택하고, 음성 인식에 기초한 사용자 인증 프로세스를 개시하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>23. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 중간 손-안 포지션을 결정하지 않고 상기 포지션이 상기 신체-상 포지션으로부터 상기 신체-외 포지션으로 트랜지션됨을 결정하는 것에 기초하여, 상기 장치의 스피커를 사용하여, 청취 가능한 경보를 생성하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>24. 제13 항에 있어서,상기 적어도 하나의 프로세서는, 건강 경보 모드가 인에이블됨을 결정하는 것에 기초하여, 상기 장치의 현재 포지션 및 상기 현재 포지션과 연관된 현재 시간을 결정하도록; 상기 장치와 연관된 포지션 이력에 상기 현재 포지션 및 상기 현재 시간을 저장하도록; 그리고 상기 현재 포지션과 연관된 상기 현재 시간과 이전 포지션과 연관된 이전 시간 사이의 시간 길이가 시간 임계치를 만족시킴을 결정하는 것에 기초하여, 통지를 자동으로 생성하도록 추가로 구성되는, 장치.</claim></claimInfo><claimInfo><claim>25. 사용자 장비로서,상기 사용자 장비의 배향을 결정하기 위한 수단;상기 사용자 장비와 연관된 주변 광 상태를 결정하기 위한 수단;상기 사용자 장비의 기계 학습 모듈을 사용하여 그리고 상기 배향 및 상기 주변 광 상태에 기초하여, 상기 사용자 장비의 포지션을 결정하기 위한 수단;상기 기계 학습 모듈을 사용하여 그리고 상기 사용자 장비의 터치스크린에 의해 수신된 터치 데이터에 기초하여, 상기 포지션이 손-안 포지션을 포함하는지 여부를 결정하기 위한 수단;상기 기계 학습 모듈을 사용하여 그리고 상기 배향 및 상기 터치 데이터에 적어도 부분적으로 기초하여, 상기 사용자 장비와 연관된 그립 모드를 결정하기 위한 수단; 및상기 사용자 장비의 배향을 결정하기 위한 수단 및 상기 사용자 장비와 연관된 주변 광 상태를 결정하기 위한 수단 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하기 위한 수단을 포함하는, 사용자 장비.</claim></claimInfo><claimInfo><claim>26. 제25 항에 있어서,상기 사용자 존재 또는 사용자 부재를 결정하기 위한 수단은, 진동 데이터를 수신하기 위한 수단; 필터링된 진동 데이터를 생성하기 위해 상기 진동 데이터를 필터링하기 위한 수단; 상기 필터링된 진동 데이터가 사용자 움직임을 표시함을 결정하기 위한 수단; 상기 기계 학습 모듈을 사용하여, 상기 사용자 존재를 결정하기 위한 수단; 및 상기 필터링된 진동 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 기계 학습 모듈을 사용하여 상기 사용자 부재를 결정하기 위한 수단을 포함하는, 사용자 장비.</claim></claimInfo><claimInfo><claim>27. 제25 항에 있어서,상기 사용자 존재 또는 사용자 부재를 결정하기 위한 수단은, 주변 광 센서로부터 주변 광 데이터를 수신하기 위한 수단; 상기 주변 광 데이터가 사용자 움직임을 표시함을 결정하는 것에 기초하여, 상기 기계 학습 모듈을 사용하여 상기 사용자 존재를 결정하기 위한 수단; 및 상기 주변 광 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 기계 학습 모듈을 사용하여 상기 사용자 부재를 결정하기 위한 수단을 포함하는, 사용자 장비.</claim></claimInfo><claimInfo><claim>28. 컴퓨터 실행 가능 명령들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서,상기 명령들은, 사용자 장비에 의해 실행될 때, 상기 사용자 장비로 하여금, 상기 사용자 장비의 관성 측정 유닛을 사용하여, 상기 사용자 장비의 배향을 결정하게 하고; 상기 사용자 장비의 주변 광 센서를 사용하여, 상기 사용자 장비와 연관된 주변 광 상태를 결정하게 하고; 상기 사용자 장비의 기계 학습 모듈을 사용하여, 상기 배향 및 상기 주변 광 상태에 기초하여, 상기 사용자 장비의 포지션을 결정하게 하고; 상기 기계 학습 모듈을 사용하여 그리고 상기 사용자 장비의 터치스크린에 의해 수신된 터치 데이터에 기초하여, 상기 포지션이 손-안 포지션을 포함하는지 여부를 결정하게 하고; 상기 기계 학습 모듈을 사용하여 그리고 상기 배향 및 상기 터치 데이터에 적어도 부분적으로 기초하여, 상기 사용자 장비와 연관된 그립 모드를 결정하게 하고; 그리고 상기 관성 측정 유닛 또는 상기 주변 광 센서 중 적어도 하나와 상기 기계 학습 모듈을 사용하여, 사용자 존재 또는 사용자 부재를 결정하게 하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>29. 제28 항에 있어서,상기 명령들은, 상기 사용자 장비에 의해 추가로 실행될 때, 상기 사용자 장비로 하여금, 상기 관성 측정 유닛으로부터 진동 데이터를 수신하게 하고; 필터링된 진동 데이터를 생성하기 위해 상기 진동 데이터를 필터링하게 하고; 상기 기계 학습 모듈을 사용하여, 상기 필터링된 진동 데이터가 사용자 움직임을 표시함을 결정하는 것에 기초하여, 상기 사용자 존재를 결정하게 하고; 그리고 상기 기계 학습 모듈을 사용하여, 상기 필터링된 진동 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 사용자 부재를 결정하게 하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>30. 제28 항에 있어서,상기 명령들은, 상기 사용자 장비에 의해 추가로 실행될 때, 상기 사용자 장비로 하여금, 상기 주변 광 센서로부터 주변 광 데이터를 수신하게 하고; 상기 기계 학습 모듈을 사용하여, 상기 주변 광 데이터가 사용자 움직임을 표시함을 결정하는 것에 기초하여, 상기 사용자 존재를 결정하게 하고; 그리고 상기 기계 학습 모듈을 사용하여, 상기 주변 광 데이터가 사용자 움직임이 없음을 표시함을 결정하는 것에 기초하여, 상기 사용자 부재를 결정하게 하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 *****-**** 캘리포니아주 샌 디에고 모어하우스 드라이브 ****</address><code>519980798710</code><country>미국</country><engName>QUALCOMM INCORPORATED</engName><name>퀄컴 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 *****-**** 캘리포...</address><code> </code><country> </country><engName>TENG, Diyan</engName><name>텡, 디얀</name></inventorInfo><inventorInfo><address>미국 *****-**** 캘리포...</address><code> </code><country> </country><engName>SOMAN, Mehul</engName><name>소만, 메훌</name></inventorInfo><inventorInfo><address>미국 *****-**** 캘리포...</address><code> </code><country> </country><engName>TRIVEDI, Nisarg</engName><name>트리베디, 니사르그</name></inventorInfo><inventorInfo><address>미국 *****-**** 캘리포...</address><code> </code><country> </country><engName>KULKARNI, Rashmi</engName><name>쿨카니, 라쉬미</name></inventorInfo><inventorInfo><address>미국 *****-**** 캘리포...</address><code> </code><country> </country><engName>MCGLOIN, Justin</engName><name>맥글로인, 저스틴</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 중구 서소문로**(서소문동, 정안빌딩*층)</address><code>920121001826</code><country>대한민국</country><engName>NAM &amp; NAM</engName><name>특허법인 남앤남</name></agentInfo><agentInfo><address>서울 중구 서소문로 **, *층(서소문동)</address><code>920241000417</code><country>대한민국</country><engName>NAM IP GROUP</engName><name>특허법인(유)남아이피그룹</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.05.11</priorityApplicationDate><priorityApplicationNumber>17/317,416</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.11.03</receiptDate><receiptNumber>1-1-2023-1216209-11</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.12.12</receiptDate><receiptNumber>1-5-2023-0200744-66</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Appointment of Agent] Report on Agent (Representative)</documentEngName><documentName>[대리인선임]대리인(대표자)에 관한 신고서</documentName><receiptDate>2024.03.22</receiptDate><receiptNumber>1-1-2024-0326923-05</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.02.17</receiptDate><receiptNumber>1-1-2025-0178164-19</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237038053.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93a89b482ba757ab917b57c937fa74d4b00d46275054046516b0aa467c3b84eb8972facdb084ef0b341fabfbbc72e319623a7eb505b9c20f21</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf8dd0a54ed460deebf4dda3278abfd6ecf5f8fb7a6d4ea3e90faaf73b5e5325eb6ad9195a9029ed9e4443941d8b376f60a348807edfe32506</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>