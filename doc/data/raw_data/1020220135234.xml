<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:56:43.5643</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.10.19</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0135234</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>현실 공간에서 객체를 제어하는 증강 현실 서비스를 제공하는 증강 현실 디바이스 및 그 동작 방법</inventionTitle><inventionTitleEng>AN AUGMENTED REALITY DEVICE FOR PROVING AUGMENTED  REALITY SERVICE WHICH CONTROLS AN OBJECT IN THE REAL  WORLD SPACE AND A METHOD FOR OPERATING THE SAME</inventionTitleEng><openDate>2024.03.15</openDate><openNumber>10-2024-0035281</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 17/30</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 15/04</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/73</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 15/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/13</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/74</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 현실 공간에서 객체를 제어하는 증강 현실 서비스를 제공하는 증강 현실 디바이스 및 그 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 증강 현실 디바이스는 카메라를 이용하여 현실 공간을 촬영하여 획득한 공간 이미지로부터 벽 및 바닥을 포함하는 평면을 인식하고, 인식된 벽과 바닥을 확장하고, 공간 이미지를 이용하여 확장된 벽과 바닥 중 객체에 의해 가려진 영역에 대하여 3차원 인-페인팅을 수행함으로써 현실 공간에 관한 3차원 모델을 생성하고, 사용자 입력에 의해 선택된 객체를 상기 공간 이미지로부터 분할하는 2차원 세그멘테이션(2D segmentation)을 수행하며, 객체의 3차원 모델 또는 3차원 위치 정보에 기초하여 공간 이미지 상에서 객체를 현실 공간으로부터 분할하는 3차원 세그멘테이션(3D segmentation)을 수행할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 카메라(110);가속도 센서(accelerometer)(122) 및 자이로 센서(gyro sensor)(124)를 포함하는 IMU 센서(Inertial Measurement Unit)(120); 적어도 하나의 명령어들(instructions)를 저장하는 메모리(140); 및상기 적어도 하나의 명령어들을 실행하는 적어도 하나의 프로세서(130); 를 포함하고, 상기 적어도 하나의 프로세서(130)는, 상기 카메라(110)를 이용하여 현실 공간을 촬영하여 공간 이미지를 획득하고, 상기 획득된 공간 이미지로부터 벽 및 바닥을 포함하는 평면을 인식하고, 상기 인식된 평면에 따라 상기 벽과 바닥을 확장하고, 상기 공간 이미지를 이용하여 상기 확장된 벽과 바닥 중 객체에 의해 가려진 영역에 대하여 3차원 인-페인팅을 수행함으로써 현실 공간에 관한 3차원 모델을 생성하고, 사용자 입력에 의해 선택된 객체를 상기 공간 이미지로부터 분할하는 2차원 세그멘테이션(2D segmentation)을 수행하고, 상기 객체의 3차원 모델 또는 3차원 위치 정보에 기초하여 상기 공간 이미지 상에서 상기 객체를 현실 공간으로부터 분할하는 3차원 세그멘테이션(3D segmentation)을 수행하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 적어도 하나의 프로세서(130)는, 상기 생성된 3차원 모델을 현실 공간 내의 벽 및 바닥의 위치에 배치하고, 상기 3차원 세그멘테이션을 통해 상기 객체가 분할된 영역을 상기 3차원 모델을 이용하여 렌더링(rendering)하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>3. 제1 항 및 제2 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(130)는, 상기 인식된 벽 및 바닥을 포함하는 평면들 각각으로부터 평면 방정식을 도출하고, 상기 도출된 평면 방정식에 기초하여 상기 벽 및 바닥을 확장함으로써, 가상의 벽 및 가상의 바닥의 3차원 모델 형태를 획득하고, 상기 공간 이미지로부터 상기 벽 및 바닥과 현실 공간 내에 배치된 객체를 식별하고, 상기 벽 및 바닥 중 상기 식별된 객체에 의해 가려진 영역을 상기 공간 이미지의 정보를 이용하여 인-페인팅하고, 상기 가상의 벽 및 가상의 바닥의 3차원 모델 형태에 인-페인팅 이미지의 텍스쳐(texture)를 적용하여 상기 현실 공간에 관한 3차원 모델을 생성하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>4. 제3 항에 있어서,상기 적어도 하나의 프로세서(130)는,상기 확장된 벽 및 바닥이 만나는 교선을 추출하고, 상기 추출된 교선에 기초하여 상기 벽 및 상기 바닥의 평면들을 구분하고, 상기 구분된 평면들 각각의 버텍스(vertex) 좌표들을 획득하고, 상기 획득된 버텍스 좌표들에 기초하여 상기 가상의 벽 및 가상의 바닥의 3차원 모델 형태를 생성하는, 증강 현실 디바이스(100).  </claim></claimInfo><claimInfo><claim>5. 제3 항에 있어서,상기 적어도 하나의 프로세서(130)는,상기 공간 이미지로부터 획득된 현실 공간의 깊이 정보 및 컬러 정보 중 적어도 하나에 기초하여 상기 객체를 식별하고, 상기 인식된 벽 및 바닥과 상기 식별된 객체를 구분하는, 증강 현실 디바이스(100).</claim></claimInfo><claimInfo><claim>6. 제1 항 내지 제5 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(130)는, 상기 생성된 3차원 모델을 상기 메모리(140) 내의 저장 공간에 저장하는, 증강 현실 디바이스(100).  </claim></claimInfo><claimInfo><claim>7. 제1 항 내지 제6 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(130)는, 상기 선택된 객체에 대한 3차원 모델이 상기 메모리(140)에 기 저장되어 있는지 여부를 확인하고, 확인 결과 상기 선택된 객체에 관한 3차원 모델이 저장되어 있는 경우, 상기 기 저장된 3차원 모델의 방향을 조절함으로써 상기 공간 이미지 상의 2차원 세그멘테이션의 윤곽선(outlier)과 겹치도록 상기 3차원 모델을 배치하고,상기 배치된 3차원 모델로부터 상기 객체에 관한 3차원 위치 좌표값을 획득하고, 상기 획득된 3차원 위치 좌표값에 기초하여 상기 객체를 상기 공간 이미지로부터 분할하는 3차원 세그멘테이션을 수행하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>8. 제1 항 내지 제7 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(130)는,상기 선택된 객체에 대한 3차원 모델이 상기 메모리(140)에 저장되어 있는지 여부를 확인하고, 확인 결과 상기 선택된 객체에 관한 3차원 모델이 저장되어 있지 않은 경우, 상기 공간 이미지로부터 인식된 상기 객체의 경계선(edge), 특징점(feature point), 및 픽셀들의 3차원 위치 좌표값을 획득하고, 상기 획득된 객체의 경계선, 특징점, 및 픽셀들의 3차원 위치 좌표값 중 적어도 하나에 기초하여 3차원 버텍스 모델링(3D vertex modeling)을 수행하고,상기 3차원 버텍스 모델링을 통해 상기 객체를 상기 공간 이미지로부터 분할하는 3차원 세그멘테이션을 수행하는, 증강 현실 디바이스. </claim></claimInfo><claimInfo><claim>9. 제1 항 내지 제8 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(130)는, 상기 증강 현실 디바이스(100)의 오리엔테이션 또는 시야각(Field of View)이 변경되는 경우 상기 공간 이미지를 추가로 획득하고, 상기 추가로 획득된 공간 이미지에 대하여 2차원 세그멘테이션을 수행함으로써 상기 객체의 윤곽선을 추출하고, 상기 3차원 세그멘테이션을 통해 상기 객체가 분할된 영역의 2차원 윤곽선과 상기 추출된 윤곽선을 비교함으로써 유사도를 측정하고,상기 유사도를 기 설정된 임계치와 비교하여 상기 3차원 세그멘테이션을 추가로 수행할지 여부를 결정하는, 증강 현실 디바이스(100).  </claim></claimInfo><claimInfo><claim>10. 제9 항에 있어서,상기 적어도 하나의 프로세서(130)는,상기 유사도가 상기 임계치 미만인 경우, 상기 3차원 세그멘테이션을 추가로 수행하고, 추가로 수행된 3차원 세그멘테이션을 통해 세그멘테이션 결과를 업데이트(update)하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>11. 증강 현실 디바이스(100)가 현실 공간에서 객체를 제어하는 증강 현실 서비스를 제공하는 방법에 있어서, 카메라(110)를 이용하여 현실 공간을 촬영하여 획득된 공간 이미지로부터 벽 및 바닥을 포함하는 평면을 인식하는 단계(S410); 상기 인식된 평면에 따라 상기 벽과 바닥을 확장하고, 상기 공간 이미지를 이용하여 상기 확장된 벽과 바닥 중 객체에 의해 가려진 영역에 대하여 3차원 인-페인팅을 수행함으로써 현실 공간에 관한 3차원 모델을 생성하는 단계(S420);사용자 입력에 의해 선택된 객체를 상기 공간 이미지로부터 분할하는 2차원 세그멘테이션(2D segmentation)을 수행하는 단계(S430); 및상기 객체의 3차원 모델 또는 3차원 위치 정보에 기초하여 상기 공간 이미지 상에서 상기 객체를 현실 공간으로부터 분할하는 3차원 세그멘테이션(3D segmentation)을 수행하는 단계(S440);를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>12. 제11 항에 있어서,상기 생성된 3차원 모델을 현실 공간 내의 벽 및 바닥의 위치에 배치하는 단계(S1310); 및상기 3차원 세그멘테이션을 통해 상기 객체가 분할된 영역을 상기 3차원 모델을 이용하여 렌더링(rendering)하는 단계(S1320);를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>13. 제11 항 및 제12 항 중 어느 하나의 항에 있어서,상기 현실 공간에 관한 3차원 모델을 생성하는 단계(S420)는,상기 인식된 벽 및 바닥을 포함하는 평면들 각각으로부터 평면 방정식을 도출하는 단계(S510);상기 도출된 평면 방정식에 기초하여 상기 벽 및 바닥을 확장함으로써, 가상의 벽 및 가상의 바닥의 3차원 모델 형태를 획득하는 단계(S520); 상기 공간 이미지로부터 상기 벽 및 바닥과 현실 공간 내에 배치된 객체를 식별하는 단계(S530);상기 벽 및 바닥 중 상기 식별된 객체에 의해 가려진 영역을 상기 공간 이미지의 정보를 이용하여 인-페인팅하는 단계(S540); 및 상기 가상의 벽 및 가상의 바닥의 3차원 모델 형태에 인-페인팅 이미지의 텍스쳐(texture)를 적용하여 상기 현실 공간에 관한 3차원 모델을 생성하는 단계(S550);를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>14. 제13 항에 있어서,상기 가상의 벽 및 가상의 바닥의 3차원 모델 형태를 생성하는 단계(S520)는, 상기 확장된 벽 및 바닥이 만나는 교선을 추출하는 단계;상기 추출된 교선에 기초하여 상기 벽 및 상기 바닥의 평면들을 구분하는 단계;상기 구분된 평면들 각각의 버텍스(vertex) 좌표들을 획득하는 단계; 및상기 획득된 버텍스 좌표들에 기초하여 상기 가상의 벽 및 가상의 바닥의 3차원 모델 형태를 생성하는 단계; 를 포함하는 방법. </claim></claimInfo><claimInfo><claim>15. 제13 항에 있어서,상기 벽 및 바닥과 현실 공간 내에 배치된 객체를 식별하는 단계(S530)는, 상기 공간 이미지로부터 획득된 현실 공간의 깊이 정보 및 컬러 정보 중 적어도 하나에 기초하여 상기 객체를 식별하는 단계; 및 상기 인식된 벽 및 바닥과 상기 식별된 객체를 구분하는 단계;를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>16. 제11 항 내지 제15 항 중 어느 하나의 항에 있어서, 상기 3차원 세그멘테이션을 수행하는 단계(S440)는,상기 선택된 객체에 대한 3차원 모델이 기 저장되어 있는지 여부를 확인하는 단계(S1010); 확인 결과 상기 선택된 객체에 관한 3차원 모델이 저장되어 있는 경우, 상기 기 저장된 3차원 모델의 방향을 조절함으로써 상기 공간 이미지 상의 2차원 세그멘테이션의 윤곽선(outlier)과 겹치도록 상기 3차원 모델을 배치하는 단계(S1020); 상기 배치된 3차원 모델로부터 상기 객체에 관한 3차원 위치 좌표값을 획득하는 단계(S1030); 및상기 획득된 3차원 위치 좌표값에 기초하여 상기 객체를 상기 공간 이미지로부터 분할하는 3차원 세그멘테이션을 수행하는 단계(S1040);를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>17. 제11 항 내지 제16 항 중 어느 하나의 항에 있어서, 상기 3차원 세그멘테이션을 수행하는 단계는,상기 선택된 객체에 대한 3차원 모델이 기 저장되어 있는지 여부를 확인하는 단계(1010); 확인 결과 상기 선택된 객체에 관한 3차원 모델이 저장되어 있지 않은 경우, 상기 공간 이미지로부터 인식된 상기 객체의 경계선(edge), 특징점(feature point), 및 픽셀들의 3차원 위치 좌표값을 획득하는 단계(S1050); 상기 획득된 객체의 경계선, 특징점, 및 픽셀들의 3차원 위치 좌표값 중 적어도 하나에 기초하여 3차원 버텍스 모델링(3D vertex modeling)을 수행하는 단계(S1060); 및상기 3차원 버텍스 모델링을 통해 상기 객체를 상기 공간 이미지로부터 분할하는 3차원 세그멘테이션을 수행하는 단계(S1070);를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>18. 제11 항 내지 제17 항 중 어느 하나의 항에 있어서,상기 증강 현실 디바이스(100)의 오리엔테이션 또는 시야각(Field of View)이 변경되는 경우 상기 공간 이미지를 추가로 획득하고, 상기 추가로 획득된 공간 이미지에 대하여 2차원 세그멘테이션을 수행함으로써 상기 객체의 윤곽선을 추출하는 단계(S1420);상기 3차원 세그멘테이션을 통해 상기 객체가 분할된 영역의 2차원 윤곽선과 상기 추출된 윤곽선을 비교함으로써 유사도를 측정하는 단계(S1430); 및상기 유사도를 기 설정된 임계치와 비교하여 상기 3차원 세그멘테이션을 추가로 수행할지 여부를 결정하는 단계(S1440); 를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>19. 제18 항에 있어서,상기 유사도가 상기 임계치 미만인 경우, 상기 3차원 세그멘테이션을 추가로 수행하고, 추가로 수행된 3차원 세그멘테이션을 통해 세그멘테이션 결과를 업데이트(update)하는 단계(S1460);를 더 포함하고, 상기 세그멘테이션 결과를 업데이트하는 단계는 기 설정된 시간 간격에 따라 주기적으로 수행되는, 방법. </claim></claimInfo><claimInfo><claim>20. 제11 항 내지 제19 항 중 어느 하나의 항에 기재된 방법을 구현하기 위한 적어도 하나의 프로그램이 기록된 컴퓨터로 판독 가능한 기록 매체. </claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>LEE, Hyeong Geon</engName><name>이형건</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>HAN, Yong Gue</engName><name>한용규</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>SONG, Young Hoon</engName><name>송영훈</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.09.08</priorityApplicationDate><priorityApplicationNumber>1020220114487</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.10.19</receiptDate><receiptNumber>1-1-2022-1104084-17</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>접수중 (On receiving) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.10.20</receiptDate><receiptNumber>1-1-2025-1169144-18</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220135234.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9353750368cfa243cd384728bf8a13fbe66487f086c9c351aa4640a57aeeb197b1f61d58b9dfe8f59e69cdd85f4e5ceb2a5e16eeacd56f9e69</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfe6c0a37e5419bd01e5f27e308f5bedd12aead6118419b4f66983d44509893a2936e8096672df44d18e390879c5345c47828ebec3dc5e160f</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>