<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:51:22.5122</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.12.02</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-7037312</applicationNumber><claimCount>49</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>디지털 휴먼의 생성 방법, 모델의 트레이닝 방법, 장치, 기기 및 매체</inventionTitle><inventionTitleEng>METHOD AND APPARATUS FOR GENERATING DIGITAL PERSON, METHOD AND APPARATUS FOR TRAINING MODEL, AND DEVICE AND MEDIUM</inventionTitleEng><openDate>2024.11.26</openDate><openNumber>10-2024-0167091</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.11.08</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.11.08</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 13/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 13/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 16/953</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/30</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/774</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 발명은 디지털 휴먼의 생성 방법, 모델의 트레이닝 방법, 장치, 기기 및 매체를 제공하는 바, 인공 지능 분야에 관한 것으로, 구체적으로 자연 언어 처리, 딥러닝, 컴퓨터 비전, 이미지 처리, 증강 현실 및 가상 현실 등 기술 분야에 관한 것이며, 메타버스 등 장면에 응용될 수 있다. 실시 형태는 아래와 같은 바, 소재 콘텐츠를 획득하고; 사전 트레이닝된 장면 분할 모델에 기반하여, 소재 콘텐츠에서 복수의 장면을 결정하되, 여기서, 복수의 장면들 중 각 장면은 소재 콘텐츠 중 완전한 시맨틱 정보를 구비하는 하나의 콘텐츠 프래그먼트에 각각 대응되고; 복수의 장면들 중 각 장면에 대해, 대응되는 콘텐츠 프래그먼트에 기반하여, 상기 장면에 대응되는 타겟 콘텐츠를 결정하고; 대응되는 타겟 콘텐츠에 기반하여, 상기 장면의 장면 태그 정보를 결정하며; 장면 태그 정보에 기반하여, 상기 장면에 특정되는 디지털 휴먼을 구성한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2023.12.21</internationOpenDate><internationOpenNumber>WO2023240943</internationOpenNumber><internationalApplicationDate>2022.12.02</internationalApplicationDate><internationalApplicationNumber>PCT/CN2022/136340</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 디지털 휴먼의 생성 방법으로서,소재 콘텐츠를 획득하는 단계; 사전 트레이닝된 장면 분할 모델에 기반하여, 상기 소재 콘텐츠에서 복수의 장면을 결정하되, 상기 복수의 장면들 중 각 장면은 상기 소재 콘텐츠 중 완전한 시맨틱 정보를 구비하는 하나의 콘텐츠 프래그먼트에 각각 대응되는 단계; 및 상기 복수의 장면들 중 각 장면에 대해,  대응되는 콘텐츠 프래그먼트에 기반하여, 상기 장면에 대응되는 타겟 콘텐츠를 결정하는 단계;  상기 대응되는 타겟 콘텐츠에 기반하여, 상기 장면의 장면 태그 정보를 결정하는 단계; 및  상기 장면 태그 정보에 기반하여, 상기 장면에 특정되는 디지털 휴먼을 구성하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 소재 콘텐츠를 획득하는 단계는, 웹사이트에 기반하여, 상기 소재 콘텐츠를 획득하는 단계; 또는 검색 키워드에 기반하여, 상기 소재 콘텐츠를 획득하는 단계 중 적어도 하나에 기반하여 상기 소재 콘텐츠를 획득하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>3. 제1항 또는 제2항에 있어서,상기 소재 콘텐츠는 이미지 데이터 및 비디오 데이터 중 적어도 하나 및 텍스트 데이터를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>4. 제1항 내지 제3항 중 어느 한 항에 있어서, 사전 트레이닝된 장면 분할 모델에 기반하여, 상기 소재 콘텐츠에서 복수의 장면을 결정하는 단계는,상기 소재 콘텐츠에 대해 섹션 구조 분석 및 섹션 시맨틱 분할을 수행하여, 상기 소재 콘텐츠에서 복수의 서브 테마를 결정하고, 상기 복수의 서브 테마 사이의 구조 관계를 결정하는 단계; 및 상기 구조 관계에 기반하여, 상기 복수의 서브 테마를 상기 복수의 장면으로 분할하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 대응되는 콘텐츠 프래그먼트에 기반하여, 상기 장면에 대응되는 타겟 콘텐츠를 결정하는 단계는, 상기 장면과 바로 전 장면 사이의 구조 관계에 기반하여, 상기 장면을 위한 제1 콘텐츠를 생성하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>6. 제4항 또는 제5항에 있어서, 상기 복수의 장면들 중 각 장면에 대해, 대응되는 콘텐츠 프래그먼트에 기반하여, 상기 장면에 대응되는 타겟 콘텐츠를 결정하는 단계는,사전 트레이닝된 스타일 변환 모델에 기반하여, 상기 대응되는 콘텐츠 프래그먼트를 상기 대응되는 타겟 콘텐츠로 변환시키는 단계를 포함하되, 상기 스타일 변환 모델은 프롬프트 러닝에 기반하여 트레이닝되어 얻은 것인 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 대응되는 콘텐츠 프래그먼트에 기반하여, 상기 장면에 대응되는 타겟 콘텐츠를 결정하는 단계는,상기 대응되는 콘텐츠 프래그먼트에 대해 텍스트 다시 쓰기 및 텍스트 압축 중 적어도 하나의 처리를 수행하여, 상기 대응되는 콘텐츠 프래그먼트를 업데이트하는 단계; 및 변환된 상기 타겟 콘텐츠에 대해 텍스트 다시 쓰기 및 텍스트 압축 중 적어도 하나의 처리를 수행하여, 상기 대응되는 타겟 콘텐츠를 업데이트하는 단계 중 적어도 하나를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>8. 제1항 내지 제7항 중 어느 한 항에 있어서,상기 장면 태그 정보는 시맨틱 태그를 포함하고, 상기 복수의 장면들 중 각 장면에 대해, 상기 대응되는 타겟 콘텐츠에 기반하여, 상기 장면의 장면 태그 정보를 결정하는 단계는,상기 대응되는 타겟 콘텐츠에 대해 감성 분석을 수행하여, 상기 시맨틱 태그를 획득하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>9. 제8항에 있어서,상기 시맨틱 태그는 상기 대응되는 타겟 콘텐츠가 표현하는 긍정, 중립 또는 부정을 포함하는 감정을 마킹하기 위한 것인 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>10. 제8항 또는 제9항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 상기 태그 정보에 기반하여, 상기 장면에 특정되는 디지털 휴먼을 구성하는 단계는,상기 시맨틱 태그에 기반하여, 상기 디지털 휴먼의 복식, 표정 및 동작 중 적어도 하나를 구성하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 타겟 콘텐츠를 음성으로 변환하여 상기 디지털 휴먼이 재생하도록 하는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 상기 장면 태그 정보에 기반하여, 상기 장면에 특정되는 디지털 휴먼을 구성하는 단계는,상기 시맨틱 태그에 기반하여, 상기 디지털 휴먼 음성의 톤을 구성하는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>13. 제1항 내지 제12항 중 어느 한 항에 있어서,홀로그램 이미지의 형태로 상기 디지털 휴먼을 나타내는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>14. 제1항 내지 제12항 중 어느 한 항에 있어서,비디오의 형태로 상기 디지털 휴먼을 나타내는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 복수의 장면들 중 각 장면에 대해,  상기 소재 콘텐츠 및 상기 장면에 대응되는 타겟 콘텐츠에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하는 단계; 및  상기 비디오 소재와 상기 디지털 휴먼을 결합하는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 상기 소재 콘텐츠 및 상기 장면에 대응되는 타겟 콘텐츠에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하는 단계는,장면 키워드를 추출하는 단계; 및 상기 장면 키워드에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>17. 제15항 또는 제16항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 상기 소재 콘텐츠 및 상기 장면에 대응되는 타겟 콘텐츠에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하는 단계는,문장 수준 키워드를 추출하는 단계; 및 상기 문장 수준 키워드에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하는 단계를 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>18. 제17항에 있어서,상기 문장 수준 키워드에 기반하여, 서칭된 비디오 소재와 상기 타겟 콘텐츠를 정렬하는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>19. 제15항 내지 제18항 중 어느 한 항에 있어서,상기 비디오 소재에 특정 소재가 포함되는 것으로 결정된 것에 응답하여, 상기 비디오 소재에서 상기 특정 소재의 디스플레이 위치에 기반하여, 상기 디지털 휴먼의 동작을 결정하는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>20. 제14항 내지 제19항 중 어느 한 항에 있어서,상기 복수의 장면들 중 각 장면에 대해,  상기 장면에 대응되는 타겟 콘텐츠에서 키-값 형태의 정보를 추출하는 단계; 및  상기 키-값 형태의 정보에 기반하여, 상기 비디오를 위한 보조 소재를 생성하는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>21. 제15항 내지 제20항 중 어느 한 항에 있어서,상기 비디오 소재에 대응되는 장면에 소요되는 재생 시간의 점유율을 결정하는 단계; 및 상기 점유율에 기반하여, 상응한 장면에서 상기 디지털 휴먼이 트리거되는지 여부를 결정하는 단계를 더 포함하는 디지털 휴먼의 생성 방법. </claim></claimInfo><claimInfo><claim>22. 장면 분할 모델의 트레이닝 방법으로서,샘플 소재 콘텐츠 및 상기 샘플 소재 콘텐츠 중의 복수의 샘플 장면을 획득하는 단계; 기설정 장면 분할 모델에 기반하여, 상기 샘플 소재 콘텐츠에서 복수의 예측 장면을 결정하는 단계; 및 상기 복수의 샘플 장면 및 상기 복수의 예측 장면에 기반하여 상기 기설정 장면 분할 모델의 파라미터를 조정하여, 트레이닝된 장면 분할 모델을 얻는 단계를 포함하는 장면 분할 모델의 트레이닝 방법. </claim></claimInfo><claimInfo><claim>23. 제22항에 있어서,상기 기설정 장면 분할 모델은 섹션 시맨틱 분할 모델 및 섹션 구조 분석 모델을 포함하되, 기설정 장면 분할 모델에 기반하여, 상기 샘플 소재 콘텐츠에서 복수의 예측 장면을 결정하는 단계는,상기 섹션 시맨틱 분할 모델 및 상기 섹션 구조 분석 모델을 이용하여 상기 샘플 소재 콘텐츠를 처리하여, 상기 소재 콘텐츠 중 복수의 예측 서브 테마 및 상기 복수의 예측 서브 테마 사이의 예측 구조 관계를 결정하는 단계; 및 상기 예측 구조 관계에 기반하여, 상기 복수의 예측 서브 테마를 상기 복수의 예측 장면으로 분할하는 단계를 포함하는 장면 분할 모델의 트레이닝 방법. </claim></claimInfo><claimInfo><claim>24. 디지털 휴먼의 생성 장치로서,소재 콘텐츠를 획득하도록 구성되는 제1 획득 유닛; 사전 트레이닝된 장면 분할 모델에 기반하여, 상기 소재 콘텐츠에서 복수의 장면을 결정하도록 구성되되, 상기 복수의 장면들 중 각 장면은 상기 소재 콘텐츠 중 완전한 시맨틱 정보를 구비하는 하나의 콘텐츠 프래그먼트에 각각 대응되는 제1 결정 유닛; 상기 복수의 장면들 중 각 장면에 대해, 대응되는 콘텐츠 프래그먼트에 기반하여, 상기 장면에 대응되는 타겟 콘텐츠를 결정하도록 구성되는 제2 결정 유닛; 상기 대응되는 타겟 콘텐츠에 기반하여, 상기 장면의 장면 태그 정보를 결정하도록 구성되는 제3 결정 유닛; 및 상기 장면 태그 정보에 기반하여, 상기 장면에 특정되는 디지털 휴먼을 구성하도록 구성되는 디지털 휴먼 구성 유닛을 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>25. 제24항에 있어서,상기 제1 획득 유닛은 또한, 웹사이트에 기반하여, 상기 소재 콘텐츠를 획득하는 것; 또는 검색 키워드에 기반하여, 상기 소재 콘텐츠를 획득하는 것 중 적어도 하나의 방식에 기반하여, 상기 소재 콘텐츠를 획득하도록 구성되는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>26. 제24항 또는 제25항에 있어서,상기 소재 콘텐츠는 이미지 데이터 및 비디오 데이터 중 적어도 하나 및 텍스트 데이터를 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>27. 제24항 내지 제26항 중 어느 한 항에 있어서,상기 제1 결정 유닛은,상기 소재 콘텐츠에 대해 섹션 구조 분석 및 섹션 시맨틱 분할을 수행하여, 상기 소재 콘텐츠에서 복수의 서브 테마를 결정하고, 상기 복수의 서브 테마 사이의 구조 관계를 결정하도록 구성되는 제1 결정 서브 유닛; 및 상기 구조 관계에 기반하여, 상기 복수의 서브 테마를 상기 복수의 장면으로 분할하도록 구성되는 제1 분할 서브 유닛을 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>28. 제27항에 있어서, 상기 제2 결정 유닛은,상기 장면과 바로 전 장면 사이의 구조 관계에 기반하여, 상기 장면을 위한 제1 콘텐츠를 생성하도록 구성되는 생성 서브 유닛을 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>29. 제27항 또는 제28항에 있어서,상기 제2 결정 유닛은,사전 트레이닝된 스타일 변환 모델에 기반하여, 상기 대응되는 콘텐츠 프래그먼트를 상기 대응되는 타겟 콘텐츠로 변환시키도록 구성되는 변환 서브 유닛을 포함하고, 상기 스타일 변환 모델은 프롬프트 러닝에 기반하여 트레이닝되어 얻은 것인 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>30. 제29항에 있어서,상기 제2 결정 유닛은,상기 대응되는 콘텐츠 프래그먼트에 대해 텍스트 다시 쓰기 및 텍스트 압축 중 적어도 하나의 처리를 수행하여, 상기 대응되는 콘텐츠 프래그먼트를 업데이트하도록 구성되는 제1 업데이트 서브 유닛; 및 변환된 상기 타겟 콘텐츠에 대해 텍스트 다시 쓰기 및 텍스트 압축 중 적어도 하나의 처리를 수행하여, 상기 대응되는 타겟 콘텐츠를 업데이트하도록 구성되는 제2 업데이트 서브 유닛 중 적어도 하나를 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>31. 제24항 내지 제30항 중 어느 한 항에 있어서,상기 제3 결정 유닛은,상기 대응되는 타겟 콘텐츠에 대해 감성 분석을 수행하여, 상기 시맨틱 태그를 획득하도록 구성되는 감성 분석 서브 유닛을 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>32. 제31항에 있어서,상기 시맨틱 태그는 상기 대응되는 타겟 콘텐츠가 표현하는 긍정, 중립 또는 부정을 포함하는 감정을 마킹하기 위한 것인 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>33. 제31항 또는 제32항에 있어서,상기 디지털 휴먼 구성 유닛은,상기 시맨틱 태그에 기반하여, 상기 디지털 휴먼의 복식, 표정 및 동작 중 적어도 하나를 구성하도록 구성되는 제1 구성 서브 유닛을 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>34. 제33항에 있어서,상기 타겟 콘텐츠를 음성으로 변환하여 상기 디지털 휴먼이 재생하도록 구성되는 음성 변환 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>35. 제34항에 있어서,상기 디지털 휴먼 구성 유닛은,상기 시맨틱 태그에 기반하여, 상기 디지털 휴먼 음성의 톤을 구성하도록 구성되는 제2 구성 서브 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>36. 제24항 내지 제35항 중 어느 한 항에 있어서,홀로그램 이미지의 형태로 상기 디지털 휴먼을 나타내도록 구성되는 홀로그램 이미지 표현 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>37. 제24항 내지 제35항 중 어느 한 항에 있어서,비디오의 형태로 상기 디지털 휴먼을 나타내도록 구성되는 비디오 표현 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>38. 제37항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 상기 소재 콘텐츠 및 상기 장면에 대응되는 타겟 콘텐츠에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하도록 구성되는 서치 유닛; 및 상기 비디오 소재와 상기 디지털 휴먼을 결합하도록 구성되는 결합 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>39. 제38항에 있어서, 상기 서치 유닛은,장면 키워드를 추출하도록 구성되는 제1 추출 서브 유닛; 및 상기 장면 키워드에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하도록 구성되는 제1 서치 서브 유닛을 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>40. 제38항 또는 제39항에 있어서,상기 서치 유닛은,문장 수준 키워드를 추출하도록 구성되는 제2 추출 서브 유닛; 및 상기 문장 수준 키워드에 기반하여, 상기 장면과 관련되는 비디오 소재를 서칭하도록 구성되는 제2 서치 서브 유닛을 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>41. 제40항에 있어서,상기 문장 수준 키워드에 기반하여, 서칭된 비디오 소재와 상기 타겟 콘텐츠를 정렬하도록 구성되는 정렬 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>42. 제38항 내지 제41항 중 어느 한 항에 있어서,상기 비디오 소재에 특정 소재가 포함되는 것으로 결정된 것에 응답하여, 상기 비디오 소재에서 상기 특정 소재의 디스플레이 위치에 기반하여, 상기 디지털 휴먼의 동작을 결정하도록 구성되는 제4 결정 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>43. 제37항 내지 제42항 중 어느 한 항에 있어서,상기 복수의 장면들 중 각 장면에 대해, 상기 장면에 대응되는 타겟 콘텐츠에서 키-값 형태의 정보를 추출하도록 구성되는 추출 유닛; 및 상기 키-값 형태의 정보에 기반하여, 상기 비디오를 위한 보조 소재를 생성하도록 구성되는 생성 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>44. 제38항 내지 제43항 중 어느 한 항에 있어서,상기 비디오 소재에 대응되는 장면에 소요되는 재생 시간의 점유율을 결정하도록 구성되는 제5 결정 유닛; 및 상기 점유율에 기반하여, 상응한 장면에서 상기 디지털 휴먼이 트리거되는지 여부를 결정하도록 구성되는 제6 결정 유닛을 더 포함하는 디지털 휴먼의 생성 장치. </claim></claimInfo><claimInfo><claim>45. 장면 분할 모델의 트레이닝 장치로서,샘플 소재 콘텐츠 및 상기 샘플 소재 콘텐츠 중의 복수의 샘플 장면을 획득하도록 구성되는 제2 획득 유닛; 기설정 장면 분할 모델에 기반하여, 상기 샘플 소재 콘텐츠에서 복수의 예측 장면을 결정하도록 구성되는 제7 결정 유닛; 및 상기 복수의 샘플 장면 및 상기 복수의 예측 장면에 기반하여 상기 기설정 장면 분할 모델의 파라미터를 조정하여, 트레이닝된 장면 분할 모델을 얻도록 구성되는 트레이닝 유닛을 포함하는 장면 분할 모델의 트레이닝 장치. </claim></claimInfo><claimInfo><claim>46. 제45항에 있어서,상기 기설정 장면 분할 모델은 섹션 시맨틱 분할 모델 및 섹션 구조 분석 모델을 포함하되, 상기 제7 결정 유닛은,상기 섹션 시맨틱 분할 모델 및 상기 섹션 구조 분석 모델을 이용하여 상기 샘플 소재 콘텐츠를 처리하여, 상기 소재 콘텐츠 중 복수의 예측 서브 테마 및 상기 복수의 예측 서브 테마 사이의 예측 구조 관계를 결정하도록 구성되는 제2 결정 서브 유닛; 및 상기 예측 구조 관계에 기반하여, 상기 복수의 예측 서브 테마를 상기 복수의 예측 장면으로 분할하도록 구성되는 제2 분할 서브 유닛을 포함하는 장면 분할 모델의 트레이닝 장치. </claim></claimInfo><claimInfo><claim>47. 전자 기기로서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하되;상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되며, 상기 명령은 상기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서가 제1항 내지 제23항 중 어느 한 항에 따른 방법을 수행할 수 있도록 하는 전자 기기. </claim></claimInfo><claimInfo><claim>48. 컴퓨터 명령이 저장되는 비일시적 컴퓨터 판독 가능 저장 매체로서,상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제23항 중 어느 한 항에 따른 방법을 수행하도록 하는 비일시적 컴퓨터 판독 가능 저장 매체. </claim></claimInfo><claimInfo><claim>49. 컴퓨터 프로그램 제품으로서, 컴퓨터 프로그램을 포함하되, 상기 컴퓨터 프로그램이 프로세서에 의해 실행될 경우 제1항 내지 제23항 중 어느 한 항에 따른 방법을 구현하는 컴퓨터 프로그램 제품.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>중국 베이징 하이디안 디스트릭트 샹디 **번가 넘버 **, 바이두 캠퍼스 *층</address><code>520190701941</code><country>중국</country><engName>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</engName><name>베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니 리미티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>중국 ****** 베이징 하이뎬 디스트릭...</address><code> </code><country>중국</country><engName>WU, Tian</engName><name>우 톈</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 하이뎬 디스트릭...</address><code> </code><country>중국</country><engName>LI, Yanhong</engName><name>리 옌훙</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 하이뎬 디스트릭...</address><code> </code><country>중국</country><engName>XIAO, Xinyan</engName><name>샤오 신옌</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 하이뎬 디스트릭...</address><code> </code><country>중국</country><engName>LIU, Hao</engName><name>류 하오</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 하이뎬 디스트릭...</address><code> </code><country>중국</country><engName>LIU, Jiachen</engName><name>류 자천</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 하이뎬 디스트릭...</address><code> </code><country>중국</country><engName>SHE, Qiaoqiao</engName><name>서 차오차오</name></inventorInfo><inventorInfo><address>중국 ****** 베이징 하이뎬 디스트릭...</address><code> </code><country>중국</country><engName>LV, Yajuan</engName><name>엘브이 야쥐안</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920030004930</code><country>대한민국</country><engName>Lim KyuBin</engName><name>임규빈</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>중국</priorityApplicationCountry><priorityApplicationDate>2022.06.15</priorityApplicationDate><priorityApplicationNumber>202210681368.3</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2024.11.08</receiptDate><receiptNumber>1-1-2024-1228239-30</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2024.11.08</receiptDate><receiptNumber>1-1-2024-1230403-25</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2024.11.12</receiptDate><receiptNumber>1-5-2024-0183770-33</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247037312.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93e1495820a1bd63d9713daec52230998422373360a04c36179330a2d6b8d166aa436d0ad1f793e46c8235f08807f41fbc21457c5e05d8412c</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf2bd40d05a7bb6068e839e76b32f0a9a6867ccbcf2a637970108c6114054dd8c1fb4096203a4f9e156e5f679fcc9e0a2f10c3acb4533fdd50</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>