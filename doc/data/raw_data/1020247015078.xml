<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:06:48.648</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.11.11</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-7015078</applicationNumber><claimCount>30</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>델타 증류를 사용한 비디오 프로세싱</inventionTitle><inventionTitleEng>VIDEO PROCESSING USING DELTA DISTILLATION</inventionTitleEng><openDate>2024.07.18</openDate><openNumber>10-2024-0112267</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.10.23</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.05.03</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/096</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0464</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0495</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/77</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/40</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 개시내용의 소정 양태들은 인공 뉴럴 네트워크를 사용하여 비디오 콘텐츠를 프로세싱하기 위한 기법들 및 장치를 제공한다. 예시적인 방법은 대체적으로, 적어도 제1 프레임 및 제2 프레임을 포함하는 비디오 데이터 스트림을 수신하는 단계를 포함한다. 교사 뉴럴 네트워크를 사용하여 제1 프레임으로부터 제1 특징부들이 추출된다. 제1 프레임과 제2 프레임 사이의 차이가 결정된다. 학생 뉴럴 네트워크를 사용하여 적어도, 제1 프레임과 제2 프레임 사이의 차이로부터 제2 특징부들이 추출된다. 제1 특징부들 및 제2 특징부들의 합에 기초하여 제2 프레임에 대한 특징부 맵이 생성된다. 제2 특징부에 대한 생성된 특징부 맵에 기초하여 비디오 데이터 스트림의 적어도 제2 프레임에 대한 추론이 생성된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2023.05.19</internationOpenDate><internationOpenNumber>WO2023086911</internationOpenNumber><internationalApplicationDate>2022.11.11</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/079679</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 프로세서 구현 방법으로서,적어도 제1 프레임 및 제2 프레임을 포함하는 비디오 데이터 스트림을 수신하는 단계;교사 뉴럴 네트워크(teacher neural network)를 사용하여 상기 제1 프레임으로부터 제1 특징부들을 추출하는 단계;상기 제1 프레임과 상기 제2 프레임 사이의 차이를 결정하는 단계;학생 뉴럴 네트워크(student neural network)를 사용하여 적어도, 상기 제1 프레임과 상기 제2 프레임 사이의 상기 차이로부터 제2 특징부들을 추출하는 단계;상기 제1 특징부들 및 상기 제2 특징부들의 합에 기초하여 상기 제2 프레임에 대한 특징부 맵을 생성하는 단계; 및상기 제2 프레임에 대한 생성된 상기 특징부 맵에 기초하여 상기 비디오 데이터 스트림의 적어도 상기 제2 프레임에 대한 추론을 생성하는 단계를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 제1 프레임은 상기 비디오 데이터 스트림 내의 키 프레임을 포함하고, 상기 제2 프레임은 상기 비디오 데이터 스트림 내의 비-키 프레임을 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 비디오 데이터 스트림 내의 상기 제2 프레임과 제3 프레임 사이의 차이를 결정하는 단계;상기 학생 뉴럴 네트워크를 사용하여 적어도, 상기 제2 프레임과 상기 제3 프레임 사이의 상기 차이로부터 제3 특징부들을 추출하는 단계;상기 제2 특징부들 및 상기 제3 특징부들의 합에 기초하여 상기 제3 프레임에 대한 특징부 맵을 생성하는 단계; 및상기 제3 프레임에 대한 생성된 상기 특징부 맵에 기초하여 상기 비디오 데이터 스트림의 상기 제3 프레임에 대한 추론을 생성하는 단계를 추가로 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 교사 뉴럴 네트워크는 선형 네트워크를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서, 상기 학생 뉴럴 네트워크는 가중치들을 상기 교사 뉴럴 네트워크에서의 가중치들의 순위보다 더 낮은 순위로 분해하도록 구성되는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 상기 학생 뉴럴 네트워크는 하나 이상의 그룹 콘볼루션 계층들을 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서, 상기 교사 뉴럴 네트워크는 비선형 네트워크를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서, 상기 학생 뉴럴 네트워크는 상기 교사 뉴럴 네트워크에 대해 감소된 수의 채널들, 감소된 공간 해상도, 또는 감소된 양자화 중 하나 이상을 갖는 네트워크를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서, 상기 제2 특징부들은 상기 제1 프레임과 상기 제2 프레임 사이의 상기 차이와 조합하여 상기 제1 프레임으로부터 추가로 추출되는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서, 상기 학생 뉴럴 네트워크는 상기 제1 프레임과 상기 제2 프레임 사이의 특징부 맵의 실제 변화와 상기 제1 프레임과 상기 제2 프레임 사이의 상기 특징부 맵의 예측된 변화 사이의 차이에 기초하여 손실 함수를 최소화하도록 훈련된 뉴럴 네트워크를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서, 상기 손실 함수는 상기 학생 뉴럴 네트워크의 복잡도 측정치 및 복수의 후보 모델들에 걸친 카테고리 분포에 기초하여 정의된 비용 함수에 추가로 기초하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>12. 제1항에 있어서, 상기 추론을 생성하는 단계는 상기 비디오 데이터 스트림의 상기 제2 프레임 내의 하나 이상의 객체들을 식별하는 단계를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>13. 제1항에 있어서, 상기 추론을 생성하는 단계는 상기 비디오 데이터 스트림 내의 대상의 포즈 또는 예측된 모션 중 적어도 하나를 추정하는 단계를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>14. 제1항에 있어서, 상기 추론을 생성하는 단계는 상기 비디오 데이터 스트림을 상기 비디오 데이터 스트림에서 캡처된 상이한 대상들과 연관된 복수의 세그먼트들로 시맨틱으로 세그먼트화하는 단계를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>15. 제1항에 있어서, 상기 추론을 생성하는 단계는 상기 제2 프레임을 잠재 공간 내의 복수의 코드들로부터의 코드에 맵핑하는 단계를 포함하고, 상기 방법은 상기 제2 프레임이 맵핑되는 상기 잠재 공간 내의 상기 코드에 기초하여 상기 제2 프레임을 수정하는 단계를 추가로 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>16. 프로세서 구현 방법으로서,복수의 비디오 샘플들을 포함하는 훈련 데이터 세트를 수신하는 단계로서, 상기 복수의 비디오 샘플들의 각각의 비디오 샘플은 복수의 프레임들을 포함하는, 상기 훈련 데이터 세트를 수신하는 단계;상기 훈련 데이터 세트에 기초하여 교사 뉴럴 네트워크를 훈련시키는 단계;각각의 비디오 샘플 내의 연속적인 프레임들에 대한 특징부 맵들 사이의 예측된 차이들 및 각각의 비디오 샘플 내의 상기 연속적인 프레임들에 대한 특징부 맵들 사이의 실제 차이들에 기초하여 학생 뉴럴 네트워크를 훈련시키는 단계; 및상기 교사 뉴럴 네트워크 및 상기 학생 뉴럴 네트워크를 배치하는 단계를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 교사 뉴럴 네트워크 및 상기 학생 뉴럴 네트워크는 동일한 태스크 특정적 목적 함수를 최소화하도록 훈련되고,상기 태스크 특정적 목적 함수는 비디오 샘플 내의 연속적인 프레임들에 대해 생성된 특징부 맵들의 실제 변화와 예측된 변화 사이의 차이와 연관된 가중 델타 분포 손실 항 및 상기 학생 뉴럴 네트워크에 대한 복잡도 측정치와 연관된 가중 비용 항에 기초하여 정의된 함수를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>18. 제16항에 있어서, 상기 학생 뉴럴 네트워크를 훈련시키는 단계는 상기 훈련 데이터 세트 내의 비디오 샘플 내의 연속적인 프레임들에 대해 생성된 출력들 사이의 실제 차이와 상기 비디오 샘플 내의 상기 연속적인 프레임들에 대해 생성된 상기 출력들 사이의 예측된 차이 사이의 손실을 최소화하도록 상기 학생 뉴럴 네트워크를 훈련시키는 단계를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>19. 제16항에 있어서, 상기 학생 뉴럴 네트워크를 훈련시키는 단계는 상기 학생 뉴럴 네트워크에 대한 복잡도 측정치 및 복수의 후보 모델들에 걸친 카테고리 분포에 기초하여 정의된 비용 함수를 최소화하도록 상기 학생 뉴럴 네트워크를 훈련시키는 단계를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>20. 제16항에 있어서, 상기 교사 뉴럴 네트워크는 선형 네트워크를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>21. 제16항에 있어서, 상기 학생 뉴럴 네트워크는 가중치들을 상기 교사 뉴럴 네트워크의 가중치들의 순위보다 더 낮은 순위로 분해하도록 구성된 네트워크를 포함하는, 프로세서 구현 방법.</claim></claimInfo><claimInfo><claim>22. 프로세싱 시스템으로서,실행가능 명령들이 저장된 메모리; 및프로세서를 포함하고, 상기 프로세서는 상기 실행가능 명령들을 실행하여, 상기 프로세싱 시스템으로 하여금, 적어도 제1 프레임 및 제2 프레임을 포함하는 비디오 데이터 스트림을 수신하게 하고; 교사 뉴럴 네트워크를 사용하여 상기 제1 프레임으로부터 제1 특징부들을 추출하게 하고; 상기 제1 프레임과 상기 제2 프레임 사이의 차이를 결정하게 하고; 학생 뉴럴 네트워크를 사용하여 적어도, 상기 제1 프레임과 상기 제2 프레임 사이의 상기 차이로부터 제2 특징부들을 추출하게 하고; 상기 제1 특징부들 및 상기 제2 특징부들의 합에 기초하여 상기 제2 프레임에 대한 특징부 맵을 생성하게 하고; 그리고 상기 제2 프레임에 대한 생성된 상기 특징부 맵에 기초하여 상기 비디오 데이터 스트림의 적어도 상기 제2 프레임에 대한 추론을 생성하게 하도록 구성되는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>23. 제22항에 있어서, 상기 프로세서는 상기 프로세싱 시스템으로 하여금,상기 비디오 데이터 스트림 내의 상기 제2 프레임과 제3 프레임 사이의 차이를 결정하게 하고;상기 학생 뉴럴 네트워크를 사용하여 적어도, 상기 제2 프레임과 상기 제3 프레임 사이의 상기 차이로부터 제3 특징부들을 추출하게 하고;상기 제2 특징부들 및 상기 제3 특징부들의 합에 기초하여 상기 제3 프레임에 대한 특징부 맵을 생성하게 하고; 그리고상기 제3 프레임에 대한 생성된 상기 특징부 맵에 기초하여 상기 비디오 데이터 스트림의 상기 제3 프레임에 대한 추론을 생성하게 하도록 추가로 구성되는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>24. 제22항에 있어서, 상기 교사 뉴럴 네트워크는 선형 네트워크를 포함하는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>25. 제22항에 있어서, 상기 교사 뉴럴 네트워크는 비선형 네트워크를 포함하고, 상기 학생 뉴럴 네트워크는 상기 교사 뉴럴 네트워크에 대해 감소된 수의 채널들, 감소된 공간 해상도, 또는 감소된 양자화 중 하나 이상을 갖는 네트워크를 포함하는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>26. 제22항에 있어서, 상기 제2 특징부들은 상기 제1 프레임과 상기 제2 프레임 사이의 상기 차이와 조합하여 상기 제1 프레임으로부터 추가로 추출되는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>27. 제22항에 있어서, 상기 학생 뉴럴 네트워크는 상기 제1 프레임과 상기 제2 프레임 사이의 특징부 맵의 실제 변화와 상기 제1 프레임과 상기 제2 프레임 사이의 상기 특징부 맵의 예측된 변화 사이의 차이에 기초하여 손실 함수를 최소화하도록 훈련된 뉴럴 네트워크를 포함하는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>28. 제27항에 있어서, 상기 손실 함수는 상기 학생 뉴럴 네트워크의 복잡도 측정치 및 복수의 후보 모델들에 걸친 카테고리 분포에 기초하여 정의된 비용 함수에 추가로 기초하는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>29. 프로세싱 시스템으로서,실행가능 명령들이 저장된 메모리; 및프로세서를 포함하고, 상기 프로세서는 상기 실행가능 명령들을 실행하여, 상기 프로세싱 시스템으로 하여금, 복수의 비디오 샘플들을 포함하는 훈련 데이터 세트를 수신하게 하는 것으로서, 상기 복수의 비디오 샘플들의 각각의 비디오 샘플은 복수의 프레임들을 포함하는, 상기 훈련 데이터 세트를 수신하게 하고; 상기 훈련 데이터 세트에 기초하여 교사 뉴럴 네트워크를 훈련시키게 하고; 각각의 비디오 샘플 내의 연속적인 프레임들에 대한 특징부 맵들 사이의 예측된 차이들 및 각각의 비디오 샘플 내의 상기 연속적인 프레임들에 대한 특징부 맵들 사이의 실제 차이들에 기초하여 학생 뉴럴 네트워크를 훈련시키게 하고; 그리고 상기 교사 뉴럴 네트워크 및 상기 학생 뉴럴 네트워크를 배치하게 하도록 구성되는, 프로세싱 시스템.</claim></claimInfo><claimInfo><claim>30. 제29항에 있어서, 상기 학생 뉴럴 네트워크를 훈련시키기 위해, 상기 프로세서는, 상기 프로세싱 시스템으로 하여금, 상기 학생 뉴럴 네트워크에 대한 복잡도 측정치 및 복수의 후보 모델들에 걸친 카테고리 분포에 기초하여 정의된 비용 함수를 최소화하도록 상기 학생 뉴럴 네트워크를 훈련시키게 하도록 구성되는, 프로세싱 시스템.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 *****-**** 캘리포니아주 샌 디에고 모어하우스 드라이브 ****</address><code>519980804600</code><country>미국</country><engName>Qualcomm Incorporated</engName><name>퀄컴 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 *****-**** 캘리...</address><code> </code><country> </country><engName>HABIBIAN, AMIRHOSSEIN</engName><name>하비비안 아미르호세인</name></inventorInfo><inventorInfo><address>미국 *****-**** 캘리...</address><code> </code><country> </country><engName>ABATI, DAVIDE</engName><name>아바티 다비데</name></inventorInfo><inventorInfo><address>미국 *****-**** 캘리...</address><code> </code><country> </country><engName>BEN YAHIA, HAITAM</engName><name>벤 야히아 하이탐</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 강남대로 **길 **(역삼동, 케이피빌딩)</address><code>920011000013</code><country>대한민국</country><engName>Koreana Patent Firm</engName><name>특허법인코리아나</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.11.15</priorityApplicationDate><priorityApplicationNumber>63/264,072</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.11.10</priorityApplicationDate><priorityApplicationNumber>18/054,274</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2024.05.03</receiptDate><receiptNumber>1-1-2024-0487976-90</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2024.06.18</receiptDate><receiptNumber>1-5-2024-0099198-22</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.10.23</receiptDate><receiptNumber>1-1-2025-1182687-48</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사유예신청]결정 보류신청서·심사유예신청서</documentName><receiptDate>2025.10.23</receiptDate><receiptNumber>1-1-2025-1182688-94</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247015078.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9302a02d0175b4ee9903bdac689133f484de4935367cd6e506c299a5a45503bb01a009f1a559fed1161df9db5696a4eaf254dfd64969446080</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfd05e72d5bf722ddd65339b61d1512e912ca945b45f4d45fdfdb69ad3e9f5348d31110dcc16821af31a4bb9c89bcb3629064af2870a901568</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>