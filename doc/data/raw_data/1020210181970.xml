<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:53:30.5330</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2021.12.17</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2021-0181970</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>렌더링 방법 및 장치</inventionTitle><inventionTitleEng>RENDERING METHOD AND DEVICE</inventionTitleEng><openDate>2023.06.26</openDate><openNumber>10-2023-0092514</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.11.27</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 15/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 15/10</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 15/04</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 15/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 렌더링 방법 및 장치가 개시된다. 일 실시예에 따른 렌더링 방법은 입력 장면(scene)의 입력 요소들에 기초하여, 제1 렌더링을 수행하는 단계, 제1 렌더링 결과를 인공 신경망 기반의 생성 모델(generative model)에 입력하여 제2 렌더링을 수행하는 단계 및 제1 렌더링 결과 및 제2 렌더링 결과에 기초하여, 출력 영상을 생성하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 입력 장면(scene)의 입력 요소들에 기초하여, 제1 렌더링을 수행하는 단계;상기 제1 렌더링 결과를 인공 신경망 기반의 생성 모델(generative model)에 입력하여 제2 렌더링을 수행하는 단계; 및상기 제1 렌더링 결과 및 상기 제2 렌더링 결과에 기초하여, 출력 영상을 생성하는 단계를 포함하는 렌더링 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 입력 요소들은상기 입력 장면의 조명(light) 정보, 기하학적(geometry) 정보 및 텍스쳐(texture) 정보 중 적어도 하나를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 제2 렌더링을 수행하는 단계는상기 텍스쳐 정보를 세그먼트(segment) 별로 피쳐(feature) 임베딩(embedding)하는 단계;상기 피쳐 임베딩을 상기 생성 모델의 조건(condition)으로 추가하는 단계; 및상기 제1 렌더링 결과를 상기 조건이 추가된 생성 모델에 입력하여 상기 제2 렌더링을 수행하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 제1 렌더링을 수행하는 단계는렌더링 방정식(rendering equation)에 기초하여, 상기 제1 렌더링을 수행하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 제1 렌더링을 수행하는 단계는상기 입력 요소들에 기초하여, 직접 광(direct illumination) 렌더링을 수행하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 제2 렌더링을 수행하는 단계는상기 생성 모델에 기초하여, 간접 광(indirect illumination) 렌더링을 수행하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 제1 렌더링을 수행하는 단계는상기 입력 요소들에 기초하여, 직접 광 렌더링 및 미리 정해진 횟수 이하 분에 대한 간접 광 렌더링을 수행하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 제2 렌더링을 수행하는 단계는상기 생성 모델에 기초하여, 상기 미리 정해진 횟수 초과 분에 대한 간접 광 렌더링을 수행하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 출력 영상을 획득하는 단계는상기 제1 렌더링 결과와 상기 제2 렌더링 결과를 합하여, 상기 출력 영상을 생성하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 제2 렌더링을 수행하는 단계는어텐션 메커니즘(attention mechanism)을 적용하여 가중치를 계산하는 단계를 포함하는, 렌더링 방법.</claim></claimInfo><claimInfo><claim>11. 입력 장면(scene)의 입력 요소들에 기초하여 제1 렌더링 결과를 생성하는 단계;상기 제1 렌더링 결과를 생성기(generator)에 입력하여 제2 렌더링을 수행하는 단계;상기 제1 렌더링 결과 및 상기 제2 렌더링 결과에 기초하여 출력 영상을 생성하는 단계;상기 출력 영상과 상기 제1 렌더링 결과에 대응하는 정답 영상을 판별기(discriminator)에 입력하여, 상기 출력 영상과 상기 정답 영상 사이의 차이를 판별하는 단계; 및상기 판별기의 출력에 기초하여, 상기 출력 영상과 상기 정답 영상 사이의 차이가 최소가 되도록 상기 생성기를 학습하는 단계를 포함하는, 학습 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 판별기의 출력에 기초하여, 상기 출력 영상과 상기 정답 영상을 구분할 수 있도록 상기 판별기를 학습하는 단계를 더 포함하는, 학습 방법.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 정답 영상은상기 제1 렌더링 결과에 대응하는 풀 렌더링(full rendering) 영상 및 내추럴(natural) 영상 중 적어도 하나를 포함하는, 학습 방법.</claim></claimInfo><claimInfo><claim>14. 하드웨어와 결합되어 제1항 내지 제13항 중 어느 하나의 항의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>15. 기 학습된 인공 신경망 기반의 생성 모델을 저장하는 메모리; 및입력 장면(scene)의 입력 요소들에 기초하여 제1 렌더링을 수행하고, 상기 제1 렌더링 결과를 상기 생성 모델에 입력하여 제2 렌더링을 수행하고, 상기 제1 렌더링 결과 및 상기 제2 렌더링 결과에 기초하여 출력 영상을 생성하는 프로세서를 포함하는 전자 장치.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 입력 요소들은상기 입력 장면의 조명(light) 정보, 기하학적(geometry) 정보 및 텍스쳐(texture) 정보 중 적어도 하나를 포함하고,상기 프로세서는상기 텍스쳐 정보를 세그먼트(segment) 별로 피쳐(feature) 임베딩(embedding)하고, 상기 피쳐 임베딩을 상기 생성 모델의 조건(condition)으로 추가하고, 상기 제1 렌더링 결과를 상기 조건이 추가된 생성 모델에 입력하여 상기 제2 렌더링을 수행하는, 전자 장치.</claim></claimInfo><claimInfo><claim>17. 제15항에 있어서,상기 프로세서는상기 입력 요소들에 기초하여 직접 광(direct illumination) 렌더링을 수행하고, 상기 생성 모델에 기초하여 간접 광(indirect illumination) 렌더링을 수행하는, 전자 장치.</claim></claimInfo><claimInfo><claim>18. 제15항에 있어서,상기 프로세서는상기 입력 요소들에 기초하여 직접 광 렌더링 및 미리 정해진 횟수 이하 분에 대한 간접 광 렌더링을 수행하고, 상기 생성 모델에 기초하여 상기 미리 정해진 횟수 초과 분에 대한 간접 광 렌더링을 수행하는, 전자 장치.</claim></claimInfo><claimInfo><claim>19. 입력 장면(scene)의 입력 요소들에 기초하여 생성된 제1 렌더링 결과를 수신하여, 제2 렌더링을 수행하는 생성기(generator); 및상기 제1 렌더링 결과 및 상기 제2 렌더링 결과에 기초하여 생성된 출력 영상과 상기 제1 렌더링 결과에 대응하는 그라운드 트루스(ground truth) 영상 사이의 차이를 판별하는 판별기(discriminator)를 포함하는, 학습 장치.</claim></claimInfo><claimInfo><claim>20. 제19항에 있어서,상기 생성기는상기 판별기의 출력에 기초하여, 상기 출력 영상과 상기 그라운드 트루스 영상 사이의 차이가 최소가 되도록 학습되고,상기 판별기는상기 판별기의 출력에 기초하여, 상기 출력 영상과 상기 그라운드 트루스 영상을 구분할 수 있도록 학습되는, 학습 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code>420170495152</code><country>대한민국</country><engName>SON, Minjung</engName><name>손민정</name></inventorInfo><inventorInfo><address>서울특별시 송파구...</address><code>420170474303</code><country>대한민국</country><engName>CHANG, Hyun Sung</engName><name>장현성</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2021.12.17</receiptDate><receiptNumber>1-1-2021-1467463-31</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2024.11.27</receiptDate><receiptNumber>1-1-2024-1314414-75</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020210181970.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93676b5d52850a81e5d9ed445b3612e759f7e9ab6ec2ddfab7fe1c919fd6d50938ac04b3a881c913c03dcd9a17138e6e9f95626a7a48176e63</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfb1649e9f8e98929cee1f9a76480c4ce138af5e626d1ce1db2c0d70c4ac85db2be96479510bfac93e11d3493b4808cf229f1645ff20f25db5</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>