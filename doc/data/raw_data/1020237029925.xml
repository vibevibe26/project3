<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:31:28.3128</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.03.16</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7029925</applicationNumber><claimCount>24</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>점 대 점 객체 매칭 및 타겟팅을 위한 시스템 및 방법</inventionTitle><inventionTitleEng>SYSTEMS AND METHODS FOR POINT TO POINT OBJECT MATCHING AND TARGETING</inventionTitleEng><openDate>2023.11.20</openDate><openNumber>10-2023-0158477</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.03.12</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.09.01</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G01S 17/04</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G01S 17/894</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G01S 13/89</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G01S 17/87</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G01S 17/86</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G01S 13/87</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/24</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/56</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>A01B 79/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>A01M 21/04</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G01S 7/497</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G01S 7/41</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G01S 13/86</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G01S 13/931</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 명세서에서 객체들의 정확한 타겟팅을 위해 사용될 수 있는 방법들, 디바이스들, 모듈들, 및 시스템들이 개시된다. 점 대 점 타겟팅의 방법이 두 개 이상의 센서들을 갖는 시스템에 의해, 객체의 위치를 파악하고 두 개 이상의 센서들 사이의 핸드오프를 조정하여 구현될 수 있다. 이들 방법들, 디바이스들, 모듈들, 및 시스템들은 자동화된 작물 재배 또는 유지보수에 사용될 수 있다. 본 명세서에서 개시된 디바이스들은 잡초를 연소시키거나 조사할 수 있는 레이저 빔과 같은 빔으로 잡초(weed)의 위치를 파악하고, 식별하고, 자율적으로 타겟팅하도록 구성될 수 있다. 본 방법들, 디바이스들, 모듈들, 및 시스템들은 농작물 관리 또는 가정용 제초에 사용될 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.09.22</internationOpenDate><internationOpenNumber>WO2022197831</internationOpenNumber><internationalApplicationDate>2022.03.16</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/020592</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 객체를 타겟팅하는 방법으로서, 예측 표현(prediction representation)을 제공하는 단계; 상기 예측 표현에서 타겟팅될 객체를 식별하는 단계; 상기 예측 표현 내에서 상기 객체의 예측되는 위치를 결정하는 단계; 상기 예측되는 위치의 타겟팅 표현을 제공하는 단계; 상기 타겟팅 표현에서 상기 객체를 식별하는 단계; 및 상기 타겟팅 표현에 기초하여 상기 객체의 타겟 위치를 결정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 예측 센서로 상기 예측 표현을 수집하는 단계, 타겟팅 센서로 상기 타겟팅 표현을 수집하는 단계, 또는 둘 모두를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 예측되는 위치를 향해 상기 타겟팅 센서를 조준하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서, 상기 타겟 위치를 향해 상기 타겟팅 센서를 조준하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제2항에 있어서, 상기 타겟 위치를 향해 상기 타겟팅 센서를 조준하는 단계는 상기 타겟팅 센서의 제1 위치와 상기 타겟팅 센서의 제2 위치 사이의 오프셋을 결정하는 단계를 포함하는 것인, 방법.</claim></claimInfo><claimInfo><claim>6. 제2항에 있어서, 상기 타겟팅 센서는 상기 타겟팅 센서가 상기 타겟팅 센서의 상기 제2 위치에 위치될 때 상기 타겟 위치를 향해 조준되는 것인, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서, 상기 타겟 위치는 상기 예측되는 위치보다 상기 객체에 더 가까운 것인, 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서, 상기 타겟 위치를 향해 기구(implement)를 지향시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서, 상기 기구의 방향이 타겟팅 센서의 방향과 상관되는 것인, 방법.</claim></claimInfo><claimInfo><claim>10. 제8항에 있어서, 상기 타겟 위치를 향해 상기 기구를 지향시키는 단계는 상기 기구의 제1 위치와 상기 기구의 제2 위치 사이의 오프셋을 결정하는 단계를 포함하고, 상기 기구는 상기 기구가 상기 기구의 상기 제2 위치에 위치될 때 상기 타겟 위치를 향해 지향되는 것인, 방법.</claim></claimInfo><claimInfo><claim>11. 제8항에 있어서, 상기 기구를 사용하여 상기 객체를 조작하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서, 상기 객체를 조작하는 단계는 상기 객체에 전자기 방사선을 조사하는 단계, 상기 객체를 이동시키는 단계, 상기 객체에 분사하는 단계, 및 이들의 조합으로 이루어진 군으로부터 선택되는 것인, 방법.</claim></claimInfo><claimInfo><claim>13. 제2항에 있어서, 상기 예측 센서는 센서 유형, 센서 해상도, 배율, 시야, 컬러 밸런스, 컬러 감도, 및 포지셔닝으로 이루어진 군으로부터 선택되는 하나 이상의 파라미터에 있어서 상기 타겟팅 센서와 상이하되, 상기 포지셔닝은 상기 객체에 대한 각도, 거리, 또는 둘 모두를 포함하는 것인, 방법.</claim></claimInfo><claimInfo><claim>14. 제2항에 있어서, 상기 예측 센서 및 상기 타겟팅 센서는 차량에 결합되는 것인, 방법.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서, 상기 예측되는 위치는 상기 예측 표현이 수집된 시간과 상기 타겟팅 표현이 수집된 시간 사이의 상기 객체에 대한 상기 차량의 모션을 고려하는 것인, 방법.</claim></claimInfo><claimInfo><claim>16. 제14항에 있어서, 상기 타겟 위치는 상기 타겟팅 표현이 수집된 시간과 조작이 수행된 시간 사이의 상기 객체에 대한 상기 차량의 모션을 고려하는 것인, 방법.</claim></claimInfo><claimInfo><claim>17. 제1항에 있어서, 상기 객체는 표면(surface) 상에, 표면 위에, 또는 표면 아래에 위치되는 것인, 방법.</claim></claimInfo><claimInfo><claim>18. 제17항에 있어서, 상기 오프셋은 상기 표면의 깊이의 변동성을 고려하는 것인, 방법.</claim></claimInfo><claimInfo><claim>19. 제1항에 있어서, 상기 타겟 위치는 상기 객체로부터 50 mm 이하, 25 mm 이하, 10 mm 이하, 5 mm 이하, 3 mm 이하, 2 mm 이하, 또는 1 mm 이하인 것인, 방법.</claim></claimInfo><claimInfo><claim>20. 제2항에 있어서, 상기 예측 센서, 상기 타겟팅 센서, 또는 둘 모두는 카메라, LIDAR(light detection and ranging) 센서, 광검출기, 능동 픽셀 센서, 반도체 검출기, 초음파 센서, RADAR 검출기, 소나(sonar) 센서, 및 광다이오드 어레이로 이루어진 군으로부터 선택되는 것인, 방법.</claim></claimInfo><claimInfo><claim>21. 제1항에 있어서, 훈련된 기계 학습 모델을 사용하여: 상기 타겟팅 표현에서 상기 객체를 식별하는 단계; 상기 예측 표현에서 상기 객체를 식별하는 단계; 및 상기 타겟팅 표현에서의 상기 객체를 상기 예측 표현에서의 상기 객체에 매칭시키는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>22. 제1항에 있어서, 상기 객체는 잡초(weed), 식물(plant), 및 장애물로 이루어진 군으로부터 선택되는 것인, 방법.</claim></claimInfo><claimInfo><claim>23. 제2항에 있어서, 상기 예측되는 위치, 타겟 위치, 또는 둘 모두는 상기 예측 표현에서의 위치, 상기 타겟팅 센서의 위치, 상기 기구의 위치, 상기 예측 센서의 위치, 상기 표면 상의 상기 객체의 위치, 상기 차량의 위치, 또는 이들의 임의의 조합을 포함하는 것인, 방법.</claim></claimInfo><claimInfo><claim>24. 객체를 타겟팅하는 방법으로서, 예측 센서를 사용하여 예측 표현을 수집하는 단계; 상기 예측 표현에서 타겟팅될 객체를 식별하는 단계; 상기 예측 표현 내에서 상기 객체의 예측되는 위치를 결정하는 단계; 상기 예측되는 위치를 향해 타겟팅 센서를 조준하는 단계; 상기 타겟팅 센서를 사용하여 상기 예측되는 위치의 타겟팅 표현을 수집하는 단계; 상기 타겟팅 표현에서 상기 객체를 식별하는 단계; 상기 타겟팅 표현에 기초하여 상기 객체의 타겟 위치를 결정하는 단계; 상기 타겟 위치를 향해 상기 타겟팅 센서를 조준하는 단계; 상기 타겟 위치를 향해 기구를 지향시키는 단계; 및 상기 기구로 상기 객체를 조작하는 단계를 포함하는, 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 워싱턴 ***** 시애틀 오로라 애비뉴 노스 ***</address><code>520220214201</code><country>미국</country><engName>CARBON AUTONOMOUS ROBOTIC SYSTEMS INC.</engName><name>카본 오토노머스 로보틱 시스템즈 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 워싱턴 ***** 시애틀 오로라 애비뉴...</address><code> </code><country> </country><engName>SERGEEV, Alexander Igorevich</engName><name>세르게예프 알렉산더 이고레비치</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구  테헤란로  *** ,**층 (역삼동, 한국기술센터)</address><code>920171002811</code><country>대한민국</country><engName>Y.S.CHANG &amp; ASSOCIATES</engName><name>특허법인와이에스장</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.03.17</priorityApplicationDate><priorityApplicationNumber>63/162,285</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.01.14</priorityApplicationDate><priorityApplicationNumber>17/576,814</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.09.01</receiptDate><receiptNumber>1-1-2023-0968501-97</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.10.19</receiptDate><receiptNumber>1-5-2023-0165854-13</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.03.12</receiptDate><receiptNumber>1-1-2025-0279253-65</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237029925.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93a2d678329cfea7459f4b409ed5d921f5847d107e3d6fdc391dcda91bda9f47e0fcb8c456c5b7e5b58136668a3d7fa6f75972fd7b111df098</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfbc7079a1a6748d76e767b8d8c237844f721750e0c6781502cc9ebad826739728ce0876e1bf28f0b6b9407586b7c198ca88779f6e7ed690e8</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>