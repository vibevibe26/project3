<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:05:52.552</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.11.25</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0160397</applicationNumber><claimCount>8</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>강화학습 방법 및 시스템</inventionTitle><inventionTitleEng>Reinforcement learning method and system</inventionTitleEng><openDate>2024.06.03</openDate><openNumber>10-2024-0077958</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2022.11.25</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/092</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/063</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 기술은 강화학습 방법 및 시스템에 관한 것이다. 본 기술의 강화학습 방법은, 프로세서에 의해 구현되는 강화 학습 방법으로서, 행동 추출 모듈에서 환경의 상태에 대한 에이전트의 행동을 추출하고-상기 환경의 상태는 인코더 신경망을 통해 1차원 벡터로 압축됨-, 보상 예측 신경망 모듈에서 상기 상태와 상기 행동에 대한 보상을 예측하여 예측 보상을 출력하며, 상기 예측 보상이 N(N은 자연수)개를 만족할 때까지 반복해서 그 중 예측 보상이 가장 높은 행동을 선택하는 단계와, 상기 선택된 행동을 기반으로 에이전트와 환경이 상호작용을 한 후, 강화학습 모델 파라미터 최적화를 진행하는 단계를 포함한다. 본 기술은 에이전트의 탐험에 대한 효율을 극대화함으로써 강화학습 모델의 학습에 소요되는 시간을 단축할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 프로세서에 의해 구현되는 강화 학습 방법으로서, 행동 추출 모듈에서 환경의 상태에 대한 에이전트의 행동을 추출하고-상기 환경의 상태는 인코더 신경망을 통해 1차원 벡터로 압축됨-, 보상 예측 신경망 모듈에서 상기 상태와 상기 행동에 대한 보상을 예측하여 예측 보상을 출력하며, 상기 예측 보상이 N(N은 자연수)개를 만족할 때까지 반복해서 그 중 예측 보상이 가장 높은 행동을 선택하는 단계와, 상기 선택된 행동을 기반으로 에이전트와 환경이 상호작용을 한 후, 강화학습 모델 파라미터 최적화를 진행하는 단계를 포함하는, 강화 학습 방법. </claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 에이전트의 행동을 추출하는 것은 상기 1차원 벡터를 액터 신경망을 통해 행동으로 변환하여 추출하는 것인, 강화 학습 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서, 상기 보상 예측 신경망 모듈은 지도 학습 기법으로 학습되는, 강화 학습 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 최적화를 진행하는 단계는, 환경에서 에이전트에게 다음 상태와 보상을 제공하고, 상기 상태, 상기 행동, 상기 다음 상태 및 상기 보상에 관한 데이터는 리플레이 메모리에 저장되며, 그 후 상기 리플레이 메모리에 저장된 상기 데이터를 샘플링하여 상기 보상 예측 신경망을 포함한 강화학습 모델에 대한 학습을 진행하는, 강화 학습 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서, 상기 학습은 정책 기반 강화학습, 액터 크리틱 기반 강화학습 중 어느 하나에 의해 진행되는, 강화 학습 방법.</claim></claimInfo><claimInfo><claim>6. 제4항에 있어서, 상기 학습된 강화학습 모델을 이용하여 입력 환경으로부터 에이전트에 의해 수행될 행동을 선택하는, 강화 학습 방법.</claim></claimInfo><claimInfo><claim>7. 제1항 내지 제6항 중 어느 한 항의 방법을 수행하기 위한 명령어를 포함하는 하나 이상의 컴퓨터 프로그램을 저장한 컴퓨터 판독 가능 기록 매체.</claim></claimInfo><claimInfo><claim>8. 하나 이상의 컴퓨터 및 명령들을 저장하는 하나 이상의 저장 디바이스를 포함하는 강화학습 시스템으로서, 상기 명령들은 상기 하나 이상의 컴퓨터에 의해 실행될 때 상기 하나 이상의 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항의 개별 방법의 동작들을 수행하게 하는 강화학습 시스템.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 안산시 상록구...</address><code>120230059685</code><country>대한민국</country><engName>BRAINBOT, Inc.</engName><name>주식회사 브레인봇</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>서울특별시 서초구...</address><code> </code><country> </country><engName>Taejoon Park</engName><name>박태준</name></inventorInfo><inventorInfo><address>경기도 수원시 팔달구...</address><code> </code><country> </country><engName>Seunghwan Yu</engName><name>유승환</name></inventorInfo><inventorInfo><address>경기도 안산시 단원구...</address><code> </code><country> </country><engName>Jongwan Yoon</engName><name>윤종완</name></inventorInfo><inventorInfo><address>경기도 안산시 단원구...</address><code> </code><country> </country><engName>Byungjin Ko</engName><name>고병진</name></inventorInfo><inventorInfo><address>경기도 성남시 수정구...</address><code> </code><country> </country><engName>Yoonki Hong</engName><name>홍윤기</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 성동구 연무장*가길 ** (성수동*가) *층 ***호(바움국제특허법률사무소)</address><code>920130008425</code><country>대한민국</country><engName>DONG HWAN YU</engName><name>유동환</name></agentInfo><agentInfo><address>서울특별시 성동구 연무장*가길 ** (성수동*가) *층 ***호(바움국제특허법률사무소)</address><code>920130001674</code><country>대한민국</country><engName>BYUNGHUN JUNG</engName><name>정병훈</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.11.25</receiptDate><receiptNumber>1-1-2022-1265056-14</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Change of Applicant] Report on Change of Proprietary Status</documentEngName><documentName>[출원인변경]권리관계변경신고서</documentName><receiptDate>2023.02.03</receiptDate><receiptNumber>1-1-2023-0129117-66</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220160397.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=348aaf18c46825cf02d6c2de1c78338e0d982468ee18e9e1c05ca2ee0687184e467f481cce0c776b7879b3882c7da434aa36bcf0f4922d3ae86832562a6f647650f805464b716cf9</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf6a4430ef103a31f40d406b66e577b60e2153a1232534168f297b8cbe2b77c38ee4f220b81dde2ee9b1dbd11a5654ea8af5498925b491554b</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>