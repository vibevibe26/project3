<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:56:11.5611</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2024.03.11</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2025-7034154</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>확장 현실을 위한 공간 스캐닝</inventionTitle><inventionTitleEng> </inventionTitleEng><openDate>2025.11.03</openDate><openNumber>10-2025-0156819</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.10.14</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.10.14</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G06F 3/0346</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 현실 세계 장면에서 물리적 객체들의 3D 데이터를 결정하기 위한 서비스들을 제공하는 확장 현실(eXtended Reality, XR) 시스템. XR 시스템은 현실 세계 장면의 공간 스캔을 개시하라는 요청을 애플리케이션으로부터 수신한다. 이에 응답하여, XR 시스템은 현실 세계 장면의 비디오 프레임 데이터를 캡처하고 XR 시스템의 포즈를 캡처한다. XR 시스템은 비디오 프레임 데이터를 사용하여 현실 세계 장면에서 물리적 객체를 결정하고 물리적 객체의 2D 포지션을 결정한다. XR 시스템은 2D 포지션을 사용하여 물리적 객체의 깊이를 결정하고, 물리적 객체의 2D 포지션, 물리적 객체의 깊이, 및 XR 시스템의 포즈를 사용하여 현실 세계 장면에서의 물리적 객체의 3D 포지션을 결정한다. XR 시스템은 3D 포지션 데이터를 애플리케이션에 통신한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate>2024.03.11</internationalApplicationDate><internationalApplicationNumber>PCT/US2024/019382</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 컴퓨터에 의해 구현되는 방법(computer-implemented method)으로서,확장 현실(eXtended Reality, XR) 시스템의 하나 이상의 프로세서에 의해, 현실 세계 장면의 공간 스캔을 개시하라는 요청을 수신하는 단계;상기 하나 이상의 프로세서에 의해, 상기 XR 시스템의 하나 이상의 카메라를 사용하여 상기 현실 세계 장면의 비디오 프레임 데이터를 캡처하는 단계;상기 하나 이상의 프로세서에 의해, 상기 XR 시스템의 포즈 추적 컴포넌트를 사용하여 상기 XR 시스템의 포즈를 캡처하는 단계;상기 하나 이상의 프로세서에 의해, 상기 비디오 프레임 데이터를 사용하여, 상기 현실 세계 장면에서 물리적 객체를 검출하고 상기 물리적 객체의 2차원(2D) 포지션을 결정하는 단계;상기 하나 이상의 프로세서에 의해, 상기 2D 포지션을 사용하여 상기 물리적 객체의 깊이를 결정하는 단계; 및상기 하나 이상의 프로세서에 의해, 상기 물리적 객체의 2D 포지션, 상기 물리적 객체의 깊이, 및 상기 XR 시스템의 포즈를 사용하여 상기 현실 세계 장면에서의 상기 물리적 객체의 3차원(3D) 포지션을 결정하는 단계를 포함하는, 컴퓨터에 의해 구현되는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 물리적 객체의 3D 포지션을 상기 XR 시스템의 애플리케이션에 통신하는 단계를 추가로 포함하는, 컴퓨터에 의해 구현되는 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 물리적 객체에 대한 라벨을 생성하는 단계를 추가로 포함하는, 컴퓨터에 의해 구현되는 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 물리적 객체를 검출하고 상기 물리적 객체의 2D 포지션을 결정하는 단계는 객체 식별 모델을 사용하여 인공 지능 방법론들에 기초하여 상기 물리적 객체를 검출하는 단계를 포함하는, 컴퓨터에 의해 구현되는 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서, 상기 물리적 객체의 3D 포지션을 결정하는 단계는:상기 XR 시스템을 사용하는 사용자의 시점 및 상기 물리적 객체의 2D 포지션을 사용하여 기하학적 광선을 생성하는 단계; 및상기 기하학적 광선과 상기 물리적 객체 사이의 교차점을 사용하여 상기 물리적 객체의 3D 포지션을 결정하는 단계를 포함하는, 컴퓨터에 의해 구현되는 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 상기 XR 시스템은 머리에 착용가능한 장치(head-wearable apparatus)를 포함하는, 컴퓨터에 의해 구현되는 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서, 상기 XR 시스템은 모바일 디바이스를 포함하는, 컴퓨터에 의해 구현되는 방법.</claim></claimInfo><claimInfo><claim>8. 머신으로서,하나 이상의 프로세서; 및상기 하나 이상의 프로세서에 의해 실행될 때, 상기 머신으로 하여금 동작들을 수행하게 하는 명령어들을 저장한 메모리를 포함하고, 상기 동작들은:현실 세계 장면의 공간 스캔을 개시하라는 요청을 수신하는 것;XR 시스템의 하나 이상의 카메라를 사용하여 상기 현실 세계 장면의 비디오 프레임 데이터를 캡처하는 것;상기 XR 시스템의 포즈 추적 컴포넌트를 사용하여 상기 XR 시스템의 포즈를 캡처하는 것;상기 비디오 프레임 데이터를 사용하여, 상기 현실 세계 장면에서 물리적 객체를 검출하고 상기 물리적 객체의 2D 포지션을 결정하는 것;상기 2D 포지션을 사용하여 상기 물리적 객체의 깊이를 결정하는 것; 및상기 물리적 객체의 2D 포지션, 상기 물리적 객체의 깊이, 및 상기 XR 시스템의 포즈를 사용하여 상기 현실 세계 장면에서의 상기 물리적 객체의 3D 포지션을 결정하는 것을 포함하는, 머신.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서, 상기 동작들은 상기 물리적 객체의 3D 포지션을 상기 XR 시스템의 애플리케이션에 통신하는 것을 추가로 포함하는, 머신.</claim></claimInfo><claimInfo><claim>10. 제9항에 있어서, 상기 동작들은 상기 물리적 객체에 대한 라벨을 생성하는 것을 추가로 포함하는, 머신.</claim></claimInfo><claimInfo><claim>11. 제8항에 있어서, 상기 물리적 객체를 검출하고 상기 물리적 객체의 2D 포지션을 결정하는 것은 객체 식별 모델을 사용하여 인공 지능 방법론들에 기초하여 상기 물리적 객체를 검출하는 것을 포함하는, 머신.</claim></claimInfo><claimInfo><claim>12. 제8항에 있어서, 상기 물리적 객체의 3D 포지션을 결정하는 것은:상기 XR 시스템을 사용하는 사용자의 시점 및 상기 물리적 객체의 2D 포지션을 사용하여 기하학적 광선을 생성하는 것; 및상기 기하학적 광선과 상기 물리적 객체 사이의 교차점을 사용하여 상기 물리적 객체의 3D 포지션을 결정하는 것을 포함하는, 머신.</claim></claimInfo><claimInfo><claim>13. 제8항에 있어서, 상기 XR 시스템은 머리에 착용가능한 장치를 포함하는, 머신.</claim></claimInfo><claimInfo><claim>14. 제8항에 있어서, 상기 XR 시스템은 모바일 디바이스를 포함하는, 머신.</claim></claimInfo><claimInfo><claim>15. 비일시적 머신 판독가능 저장 매체로서, 상기 머신 판독가능 저장 매체는, 머신에 의해 실행될 때, 상기 머신으로 하여금 동작들을 수행하게 하는 명령어들을 포함하고, 상기 동작들은:현실 세계 장면의 공간 스캔을 개시하라는 요청을 수신하는 것;XR 시스템의 하나 이상의 카메라를 사용하여 상기 현실 세계 장면의 비디오 프레임 데이터를 캡처하는 것;상기 XR 시스템의 포즈 추적 컴포넌트를 사용하여 상기 XR 시스템의 포즈를 캡처하는 것;상기 비디오 프레임 데이터를 사용하여, 상기 현실 세계 장면에서 물리적 객체를 검출하고 상기 물리적 객체의 2D 포지션을 결정하는 것;상기 2D 포지션을 사용하여 상기 물리적 객체의 깊이를 결정하는 것; 및상기 물리적 객체의 2D 포지션, 상기 물리적 객체의 깊이, 및 상기 XR 시스템의 포즈를 사용하여 상기 현실 세계 장면에서의 상기 물리적 객체의 3D 포지션을 결정하는 것을 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서, 상기 동작들은 상기 물리적 객체의 3D 포지션을 상기 XR 시스템의 애플리케이션에 통신하는 것을 추가로 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서, 상기 동작들은 상기 물리적 객체에 대한 라벨을 생성하는 것을 추가로 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>18. 제15항에 있어서, 상기 물리적 객체를 검출하고 상기 물리적 객체의 2D 포지션을 결정하는 것은 객체 식별 모델을 사용하여 인공 지능 방법론들에 기초하여 상기 물리적 객체를 검출하는 것을 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>19. 제15항에 있어서, 상기 물리적 객체의 3D 포지션을 결정하는 것은:상기 XR 시스템을 사용하는 사용자의 시점 및 상기 물리적 객체의 2D 포지션을 사용하여 기하학적 광선을 생성하는 것; 및상기 기하학적 광선과 상기 물리적 객체 사이의 교차점을 사용하여 상기 물리적 객체의 3D 포지션을 결정하는 것을 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>20. 제15항에 있어서, 상기 XR 시스템은 머리에 착용가능한 장치를 포함하는, 비일시적 머신 판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 캘리포니아주 산타 모니카 써티퍼스트 스트리트 ****</address><code>520140253774</code><country>미국</country><engName>SNAP INC.</engName><name>스냅 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아 ...</address><code> </code><country>폴란드</country><engName>ZAKRZEWSKI, Tomasz</engName><name>자크르제우스키, 토마즈</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920010003368</code><country>대한민국</country><engName>Kim Yeon Song</engName><name>김연송</name></agentInfo><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2023.03.15</priorityApplicationDate><priorityApplicationNumber>18/184,108</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2025.10.14</receiptDate><receiptNumber>1-1-2025-1145423-01</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2025.10.22</receiptDate><receiptNumber>1-5-2025-0177212-27</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257034154.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9324f0ab33db7d39ad0b03c6ed69b5dfd84314c20e7fb5659a7ad98fac3704be94a4ae945e97bc5f25bb01d8ed6438f7af79dafbc4079f1f43</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cff0b851ef961ec9a72c322086234b256740b8fba3b5971f1374afd268c98e6b4083a4da99a8ad35579fbc03ac77b5345a5d25f6288f3a03ba</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>