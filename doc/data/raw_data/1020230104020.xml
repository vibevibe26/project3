<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:37:38.3738</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.08.09</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0104020</applicationNumber><claimCount>10</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>적응형 스테이지별 스케줄링을 사용한 GPU/NPU 서버 머신러닝 파이프라인 서빙 방법 및 시스템</inventionTitle><inventionTitleEng>Method and System for Serving Machine Learning Pipelines  on GPU/NPU Servers with Adaptive Per-Stage Scheduling</inventionTitleEng><openDate>2024.05.17</openDate><openNumber>10-2024-0067788</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.05.07</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>G06F 9/48</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>G06F 9/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06F 9/38</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>G06F 9/54</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06N 20/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 5/04</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 적응형 스테이지별 스케줄링을 사용한 GPU/NPU 서버 머신러닝 파이프라인 서빙 방법 및 시스템이 제시된다. 본 발명에서 제안하는 적응형 스테이지별 스케줄링을 사용한 GPU/NPU 서버 머신러닝 파이프라인 서빙 시스템은 각 서버에 대한 각 파이프라인의 최대 요청 속도를 지정하기 위한 요청 속도 테이블에 따라 수신 요청을 복수의 서버 관리부에게 발송하는 클러스터 관리부 및 스테이지별 스케줄링 알고리즘을 사용하여 수신 파이프라인 요청의 모델 작업을 이기종 장치에 할당하고, 서비스 레벨 목표(Service-Level Objective; SLO) 대기 시간 및 상기 수신된 요청 당 운영 비용을 감소시키도록 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 클러스터 관리부에 전송하는 각각의 서버 관리부의 스케줄러를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 각 서버에 대한 각 파이프라인의 최대 요청 속도를 지정하기 위한 요청 속도 테이블에 따라 수신 요청을 복수의 서버 관리부에게 발송하는 클러스터 관리부; 및 스테이지별 스케줄링 알고리즘을 사용하여 수신 파이프라인 요청의 모델 작업을 이기종 장치에 할당하고, 서비스 레벨 목표(Service-Level Objective; SLO) 대기 시간 및 상기 수신된 요청 당 운영 비용을 감소시키도록 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 클러스터 관리부에 전송하는 각각의 서버 관리부의 스케줄러 를 포함하고, 상기 클러스터 관리부는, 복수의 서버 관리부로부터 리소스 피드백을 수신하여 모니터링된 요청 변경 및 피드백 정보에 따라 리소스 할당을 조정하고, 상기 피드백 정보를 기반으로 서버 리소스 크기 조정 요청을 자동으로 리소스 관리부에 전송하고 요청 속도 테이블을 조정하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 시스템. </claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 서버 관리부는, GPU 및 NPU를 포함하는 이기종 장치의 활용률을 증가시키고, 요청 당 비용을 감소시키도록 처리량을 증가시키기 위한 상기 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 전송하기 위해 스테이지별 스케줄링 알고리즘을 통해 수신 요청에 따른 모델 작업을 이기종 장치에 할당하는 스케줄러(Scheduler); 및상기 스케줄러의 지시에 따라 할당된 모델 작업을 순서대로 실행하는 이기종 장치 별 작업자(Worker)를 포함하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 시스템. </claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 서버 관리부의 스케줄러는, 상태 데드라인 조정 알고리즘을 통해 배치 크기 및 이기종 장치에 대한 오프라인 프로파일 대기 시간 및 운영 비용 간의 균형을 맞추기 위해 각 파이프라인의 SLO 대기 시간을 적응형 스테이지 서브 데드라인을 사용하여 해당 스테이지로 분산하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 시스템.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서,상기 서버 관리부의 스케줄러는, 운영 비용 조정 알고리즘을 통해 적응형 운영 비용 및 이기종 장치의 비교 이점에 따라 이기종 장치 중 수요보다 더 많이 공급되는 장치 유형을 절하하고, 비용 함수에 따른 작업 당 운영 비용 및 상기 장치 유형의 절하율을 이용하여 운영 비용을 조정하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 시스템. </claim></claimInfo><claimInfo><claim>5. 제2항에 있어서,상기 이기종 장치 별 작업자는, 스케줄링 대기 시간을 숨기기 위해 스케줄러가 미리 작업을 할당하는 스케줄링 윈도우를 포함하고, 상기 스케줄러와 상기 이기종 장치 별 작업자는, 작업 스케줄링 및 완료 메시지를 위해 전용 채널을 통해 상호 통신하고, 상기 스케줄러는,SLO 대기 시간을 관찰하는 동안 상기 이기종 장치 활용률을 최대한 유지하기 위해 이기종 장치 활용률이 미리 정해진 기준 이하로 낮거나 구독이 미리 정해진 기준 이상으로 과다할 때 상기 클러스터 관리부에게 피드백을 전송하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 시스템. </claim></claimInfo><claimInfo><claim>6. 클러스터 관리부가 각 서버에 대한 각 파이프라인의 최대 요청 속도를 지정하기 위한 요청 속도 테이블에 따라 수신 요청을 복수의 서버 관리부에게 발송하는 단계; 각각의 서버 관리부의 스케줄러가 스테이지별 스케줄링 알고리즘을 사용하여 수신 파이프라인 요청의 모델 작업을 이기종 장치에 할당하고, 서비스 레벨 목표(Service-Level Objective; SLO) 대기 시간 및 상기 수신된 요청 당 운영 비용을 감소시키도록 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 클러스터 관리부에 전송하는 단계; 클러스터 관리부가 복수의 서버 관리부로부터 리소스 피드백을 수신하여 모니터링된 요청 변경 및 피드백 정보에 따라 리소스 할당을 조정하는 단계;상기 클러스터 관리부가 상기 피드백 정보를 기반으로 서버 리소스 크기 조정 요청을 자동으로 리소스 관리부에 전송하고 요청 속도 테이블을 조정하는 단계 를 포함하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 방법. </claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 각각의 서버 관리부의 스케줄러가 스테이지별 스케줄링 알고리즘을 사용하여 수신 파이프라인 요청의 모델 작업을 이기종 장치에 할당하고, 서비스 레벨 목표 대기 시간 및 상기 수신된 요청 당 운영 비용을 감소시키도록 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 클러스터 관리부에 전송하는 단계는, 상기 서버 관리부의 스케줄러(Scheduler)가 GPU 및 NPU를 포함하는 이기종 장치의 활용률을 증가시키고, 요청 당 비용을 감소시키도록 처리량을 증가시키기 위한 상기 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 전송하기 위해 스테이지별 스케줄링 알고리즘을 통해 수신 요청에 따른 모델 작업을 이기종 장치에 할당하는 단계; 및상기 서버 관리부의 이기종 장치 별 작업자(Worker)가 상기 스케줄러의 지시에 따라 할당된 모델 작업을 순서대로 실행하는 단계 를 포함하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 방법. </claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 서버 관리부의 스케줄러가 GPU 및 NPU를 포함하는 이기종 장치의 활용률을 증가시키고, 요청 당 비용을 감소시키도록 처리량을 증가시키기 위한 상기 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 전송하기 위해 스테이지별 스케줄링 알고리즘을 통해 수신 요청에 따른 모델 작업을 이기종 장치에 할당하는 단계는, 상기 서버 관리부의 스케줄러가 상태 데드라인 조정 알고리즘을 통해 배치 크기 및 이기종 장치에 대한 오프라인 프로파일 대기 시간 및 운영 비용 간의 균형을 맞추기 위해 각 파이프라인의 SLO 대기 시간을 적응형 스테이지 서브 데드라인을 사용하여 해당 스테이지로 분산하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 방법. </claim></claimInfo><claimInfo><claim>9. 제7항에 있어서,상기 서버 관리부의 스케줄러가 GPU 및 NPU를 포함하는 이기종 장치의 활용률을 증가시키고, 요청 당 비용을 감소시키도록 처리량을 증가시키기 위한 상기 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 전송하기 위해 스테이지별 스케줄링 알고리즘을 통해 수신 요청에 따른 모델 작업을 이기종 장치에 할당하는 단계는, 상기 서버 관리부의 스케줄러가 운영 비용 조정 알고리즘을 통해 적응형 운영 비용 및 이기종 장치의 비교 이점에 따라 이기종 장치 중 수요보다 더 많이 공급되는 장치 유형을 절하하고, 비용 함수에 따른 작업 당 운영 비용 및 상기 장치 유형의 절하율을 이용하여 운영 비용을 조정하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 방법. </claim></claimInfo><claimInfo><claim>10. 제7항에 있어서,상기 각각의 서버 관리부의 스케줄러가 스테이지별 스케줄링 알고리즘을 사용하여 수신 파이프라인 요청의 모델 작업을 이기종 장치에 할당하고, 서비스 레벨 목표 대기 시간 및 상기 수신된 요청 당 운영 비용을 감소시키도록 현재 부하의 상태 및 각 서버의 이기종 장치 상태에 관한 리소스 피드백을 클러스터 관리부에 전송하는 단계는,상기 이기종 장치 별 작업자가 스케줄링 대기 시간을 숨기기 위해 스케줄러가 미리 작업을 할당하는 스케줄링 윈도우를 포함하고, 상기 스케줄러와 상기 이기종 장치 별 작업자가 작업 스케줄링 및 완료 메시지를 위해 전용 채널을 통해 상호 통신하며, 상기 스케줄러가 SLO 대기 시간을 관찰하는 동안 상기 이기종 장치 활용률을 최대한 유지하기 위해 이기종 장치 활용률이 미리 정해진 기준 이하로 낮거나 구독이 미리 정해진 기준 이상으로 과다할 때 상기 클러스터 관리부에게 피드백을 전송하는 GPU 및 NPU 서버 머신러닝 파이프라인 서빙 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>대전광역시 유성구...</address><code>319980988661</code><country>대한민국</country><engName>Korea Advanced Institute of Science and Technology</engName><name>한국과학기술원</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>Kang, Jeehoon</engName><name>강지훈</name></inventorInfo><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>Park, Chunmyong</engName><name>박천명</name></inventorInfo><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>Shim, Sunghwan</engName><name>심성환</name></inventorInfo><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>An, Haechan</engName><name>안해찬</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 선릉로***길 ** (논현동) 삼성빌딩 *층(피앤티특허법률사무소)</address><code>920050004530</code><country>대한민국</country><engName>Yang,Sung Bo</engName><name>양성보</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.11.08</priorityApplicationDate><priorityApplicationNumber>1020220147445</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.08.09</receiptDate><receiptNumber>1-1-2023-0876048-24</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2024.05.07</receiptDate><receiptNumber>1-1-2024-0494696-75</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Request for Prior Art Search</documentEngName><documentName>선행기술조사의뢰서</documentName><receiptDate>2025.04.10</receiptDate><receiptNumber>9-1-9999-9999999-89</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230104020.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c936ce9252219f791d09aa8a931c3e623651d008c7a6ecfd1defe57d7f4defc33992488ea69b2672a77d59138e3d0b2f8c6070d8d1426422227</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf92fabec18568e917100f3e88367262488d3deb54a2755b8183b643573ed5e0064d59fbe11ee337564599e70dfdf7b2092d6010a95d798476</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>