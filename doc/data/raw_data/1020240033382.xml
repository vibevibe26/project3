<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:03:17.317</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2024.03.08</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-0033382</applicationNumber><claimCount>17</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>동적 및 정적 데이터의 하이브리드 융합을 통한 인간 행동 인식 장치 및 방법</inventionTitle><inventionTitleEng>APPARATUS AND METHOD FOR HUMAN ACTIVITY RECOGNITION THROUGH  HYBRID FUSION OF DYNAMIC AND STATIC DATA</inventionTitleEng><openDate>2025.09.16</openDate><openNumber>10-2025-0136642</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.03.08</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06F 18/241</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06F 18/213</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/096</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/04</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2025.01.01)</ipcDate><ipcNumber>A61B 5/11</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 일 실시예에 따른 인간 행동 인식 장치가 개시된다. 인간 행동 인식(human activity recognition, HAR)을 위한 전자 장치는, 생체 센서 및 관성 센서를 이용하여 대상자의 라이프로그 데이터(lifelog data)를 주기적으로 수집하고, 상기 라이프로그 데이터 중 시간에 의존적인 동적 데이터 및 시간에 독립적인 정적 데이터를 분리하며, 상기 동적 데이터를 딥러닝 기반의 시계열 모델(deep learning based Times Series model)에 적용함으로써, 상기 시계열 모델의 마지막 레이어에서 액티베이션 함수를 스킵(skip)하고 출력된 사전 액티베이션 출력(pre-activation output)을 획득하고, 상기 정적 데이터를 다층 퍼셉트론 구조 기반의 특징 추출기(multi-layer perceptron based feature extractor)에 적용함으로써, 정적 특징 벡터를 추출하며, 상기 사전 액티베이션 출력 및 상기 정적 특징 벡터를 결합함으로써, 결합 특징 벡터를 생성하고, 상기 결합 특징 벡터로부터, 다층 퍼셉트론 기반의 분류기(multi-layer perceptron based classifier)에 기초하여, 상기 대상자의 행동 예측 결과를 출력하는 프로세서; 및 상기 딥러닝 기반의 시계열 모델, 상기 다층 퍼셉트론 구조 기반의 특징 추출기, 상기 다층 퍼셉트론 기반의 분류기를 저장하는 메모리를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 프로세서에 의해 수행되는 인간 행동 인식(human activity recognition, HAR) 방법에 있어서,생체 센서 및 관성 센서를 이용하여 대상자의 라이프로그 데이터(lifelog data)를 주기적으로 수집하는 단계;상기 라이프로그 데이터 중 시간에 의존적인 동적 데이터 및 시간에 독립적인 정적 데이터를 분리하는 단계;상기 동적 데이터를 딥러닝 기반의 시계열 모델(deep learning based Times Series model)에 적용함으로써, 상기 시계열 모델의 마지막 레이어에서 액티베이션 함수를 스킵(skip)하고 출력된 사전 액티베이션 출력(pre-activation output)을 획득하는 단계;상기 정적 데이터를 다층 퍼셉트론 구조 기반의 특징 추출기(multi-layer perceptron based feature extractor)에 적용함으로써, 정적 특징 벡터를 추출하는 단계;상기 사전 액티베이션 출력 및 상기 정적 특징 벡터를 결합함으로써, 결합 특징 벡터를 생성하는 단계; 및상기 결합 특징 벡터로부터, 다층 퍼셉트론 기반의 분류기(multi-layer perceptron based classifier)에 기초하여, 상기 대상자의 행동 예측 결과를 출력하는 단계를 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 결합 특징 벡터를 생성하는 단계는,상기 마지막 레이어의 이전 레이어에 포함된 노드들로부터 상기 마지막 레이어로 전달된 노드 값들, 및 상기 이전 레이어에 포함된 노드들과 상기 마지막 레이어에 포함된 노드들 간의 연결 가중치들사이의 가중합 연산에 기초하는 가중합 벡터를 상기 사전 액티베이션 출력으로 획득하는 단계; 및상기 사전 액티베이션 출력 및 상기 정적 특징 벡터를 결합함으로써, 상기 결합 특징 벡터를 생성하는 단계를 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 대상자의 행동 예측 결과를 출력하는 단계는,상기 결합 특징 벡터에 대응하는 쿼리 벡터들(query vectors), 키 벡터들(key vectors) 및 밸류 벡터들(value vectors)을 생성하는 단계;상기 생성된 쿼리 벡터들(query vectors), 키 벡터들(key vectors) 및 밸류 벡터들(value vectors)에 내적 기반의 어텐션 함수를 적용함으로써, 상기 결합 특징 벡터에 대한 어텐션 출력을 산출하는 단계; 및상기 산출된 어텐션 출력 및 상기 결합 특징 벡터 간의 결합으로부터, 다층 퍼셉트론 기반의 분류기에 기초하여, 상기 대상자의 행동 예측 결과를 출력하는 단계를 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 어텐션 출력을 산출하는 단계는,상기 쿼리 벡터들 중 해당 쿼리 벡터 및 상기 키 벡터들 중 해당 키 벡터 간의 내적 연산 결과 및 미리 결정된 키 벡터 차원 크기에 기초하여, 중간 특징 벡터를 산출하는 단계; 및상기 산출된 중간 특징 벡터에 소프트맥스 함수(softmax function)를 적용함으로써 획득된 상기 밸류 벡터들 중 해당 밸류 벡터에 대한 가중치와 상기 해당 밸류 벡터에 기초하여, 상기 어텐션 출력을 산출하는 단계를 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 동적 데이터를 딥러닝 기반의 다른 시계열 모델에 적용함으로써, 상기 다른 시계열 모델의 마지막 레이어에서 액티베이션 함수를 스킵하고 출력된 다른 사전 액티베이션 출력을 획득하는 단계;상기 정적 데이터를 다층 퍼셉트론 구조 기반의 다른 특징 추출기에 적용함으로써, 다른 정적 특징 벡터를 추출하는 단계;상기 다른 사전 액티베이션 출력 및 상기 다른 정적 특징 벡터를 결합함으로써, 다른 결합 특징 벡터를 생성하는 단계;상기 다른 결합 특징 벡터로부터, 다층 퍼셉트론 기반의 다른 분류기에 기초하여, 상기 대상자의 다른 행동 예측 결과를 출력하는 단계; 및상기 출력된 대상자의 행동 예측 결과 및 다른 행동 예측 결과를 앙상블(ensemble)하는 단계를 더 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,상기 출력된 대상자의 행동 예측 결과 및 다른 행동 예측 결과를 앙상블(ensemble)하는 단계는,상기 대상자의 행동 예측 결과 및 상기 대상자의 다른 행동 예측 결과의 평균 로짓 값(logit value)에 대응하는 데이터에 기초하여, 앙상블 하는 단계를 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 시간에 의존적인 동적 데이터는, 가속도계(accelerometer), 광용적맥파 센서(photoplethysmography sensor), 피부전극활성도(electrodermal activity, EDA) 센서, 또는 적외선 서모파일(infrared thermopile) 센서 중 적어도 하나를 포함하는 생체 센서에 기초하여 수집되는 생물학적 시퀀스 데이터; 및상기 대상자의 웨어러블 장비를 포함하는 단말에 포함된 가속도계, 자이로스코프(gyroscope), 마그네토미터(magnetometer), 또는 GPS(global positioning system) 중 적어도 하나를 포함하는 관성 센서에 기초하여 수집되는 관성 시퀀스 데이터를 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 시간에 독립적인 정적 데이터는,상기 대상자를 식별하는 프로필 정보 또는 상기 대상자에 대한 수면 정보 중 적어도 하나를 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 동적 데이터에 기초하여, 대상자의 행동 예측 결과를 출력하도록 상기 딥러닝 기반의 시계열 모델을 사전 학습(pre-training)하는 단계; 및상기 결합 특징 벡터에 상기 다층 퍼셉트론 기반의 분류기를 적용하여 산출된 트레이닝 출력과 미리 결정된 라벨 출력 간의 오차에 대응하는 손실 함수 값이 미리 설정된 임계값에 대응하도록, 상기 분류기, 상기 특징 추출기 및 상기 시계열 모델의 파라미터를 업데이트하는 단계를 더 포함하는 HAR 방법.</claim></claimInfo><claimInfo><claim>10. 하드웨어와 결합되어 제1항 내지 제9항 중 어느 하나의 항의 방법을 실행시키기 위하여 컴퓨터 판독 가능한 기록매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>11. 인간 행동 인식(human activity recognition, HAR)을 위한 전자 장치에 있어서,생체 센서 및 관성 센서를 이용하여 대상자의 라이프로그 데이터(lifelog data)를 주기적으로 수집하고, 상기 라이프로그 데이터 중 시간에 의존적인 동적 데이터 및 시간에 독립적인 정적 데이터를 분리하며, 상기 동적 데이터를 딥러닝 기반의 시계열 모델(deep learning based Times Series model)에 적용함으로써, 상기 시계열 모델의 마지막 레이어에서 액티베이션 함수를 스킵(skip)하고 출력된 사전 액티베이션 출력(pre-activation output)을 획득하고, 상기 정적 데이터를 다층 퍼셉트론 구조 기반의 특징 추출기(multi-layer perceptron based feature extractor)에 적용함으로써, 정적 특징 벡터를 추출하며, 상기 사전 액티베이션 출력 및 상기 정적 특징 벡터를 결합함으로써, 결합 특징 벡터를 생성하고, 상기 결합 특징 벡터로부터, 다층 퍼셉트론 기반의 분류기(multi-layer perceptron based classifier)에 기초하여, 상기 대상자의 행동 예측 결과를 출력하는 프로세서; 및상기 딥러닝 기반의 시계열 모델, 상기 다층 퍼셉트론 구조 기반의 특징 추출기, 상기 다층 퍼셉트론 기반의 분류기를 저장하는 메모리를 포함하는 전자 장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 프로세서는,상기 마지막 레이어의 이전 레이어에 포함된 노드들로부터 상기 마지막 레이어로 전달된 노드 값들, 및 상기 이전 레이어에 포함된 노드들과 상기 마지막 레이어에 포함된 노드들 간의 연결 가중치들 사이의 가중합 연산에 기초하는 가중합 벡터를 상기 사전 액티베이션 출력으로 획득하고, 상기 사전 액티베이션 출력 및 상기 정적 특징 벡터를 결합함으로써, 상기 결합 특징 벡터를 생성하는전자 장치.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 프로세서는,상기 결합 특징 벡터에 대응하는 쿼리 벡터들(query vectors), 키 벡터들(key vectors) 및 밸류 벡터들(value vectors)을 생성하고, 상기 생성된 쿼리 벡터들(query vectors), 키 벡터들(key vectors) 및 밸류 벡터들(value vectors)에 내적 기반의 어텐션 함수를 적용함으로써, 상기 결합 특징 벡터에 대한 어텐션 출력을 산출하며, 상기 산출된 어텐션 출력 및 상기 결합 특징 벡터 간의 결합으로부터, 다층 퍼셉트론 기반의 분류기에 기초하여, 상기 대상자의 행동 예측 결과를 출력하는전자 장치.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 프로세서는,상기 쿼리 벡터들 중 해당 쿼리 벡터 및 상기 키 벡터들 중 해당 키 벡터 간의 내적 연산 결과 및 미리 결정된 키 벡터 차원 크기에 기초하여, 중간 특징 벡터를 산출하고, 상기 산출된 중간 특징 벡터에 소프트맥스 함수(softmax function)를 적용함으로써 획득된, 상기 밸류 벡터들 중 해당 밸류 벡터에 대한 가중치와 상기 해당 밸류 벡터에 기초하여, 상기 어텐션 출력을 산출하는전자 장치.</claim></claimInfo><claimInfo><claim>15. 제11항에 있어서,상기 프로세서는,상기 동적 데이터를 딥러닝 기반의 다른 시계열 모델에 적용함으로써, 상기 다른 시계열 모델의 마지막 레이어에서 액티베이션 함수를 스킵하고 출력된 다른 사전 액티베이션 출력을 획득하고, 상기 정적 데이터를 다층 퍼셉트론 구조 기반의 다른 특징 추출기에 적용함으로써, 다른 정적 특징 벡터를 추출하며, 상기 다른 사전 액티베이션 출력 및 상기 다른 정적 특징 벡터를 결합함으로써, 다른 결합 특징 벡터를 생성하고, 상기 다른 결합 특징 벡터로부터, 다층 퍼셉트론 기반의 다른 분류기에 기초하여, 상기 대상자의 다른 행동 예측 결과를 출력하며, 상기 출력된 대상자의 행동 예측 결과 및 다른 행동 예측 결과를 앙상블(ensemble)하는,전자 장치.</claim></claimInfo><claimInfo><claim>16. 제11항에 있어서, 상기 프로세서는,상기 대상자의 행동 예측 결과 및 상기 대상자의 다른 행동 예측 결과의 평균 로짓 값(logit value)에 대응하는 데이터에 기초하여, 앙상블 하는,전자 장치.</claim></claimInfo><claimInfo><claim>17. 제11항에 있어서,상기 프로세서는,상기 동적 데이터에 기초하여, 대상자의 행동 예측 결과를 출력하도록 상기 딥러닝 기반의 시계열 모델을 사전 학습(pre-training)하고, 상기 결합 특징 벡터에 상기 다층 퍼셉트론 기반의 분류기를 적용하여 산출된 트레이닝 출력과 미리 결정된 라벨 출력 간의 오차에 대응하는 손실 함수 값이 미리 설정된 임계값에 대응하도록, 상기 분류기, 상기 특징 추출기 및 상기 시계열 모델의 파라미터를 업데이트하는전자 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>울산광역시 울주군...</address><code>120150812047</code><country>대한민국</country><engName>UNIST(ULSAN NATIONAL INSTITUTE OF SCIENCE AND TECHNOLOGY)</engName><name>울산과학기술원</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>울산광역시 울주군...</address><code> </code><country> </country><engName>KIM Sung Il</engName><name>김성일 </name></inventorInfo><inventorInfo><address>울산광역시 울주군...</address><code> </code><country> </country><engName>OH Yong Kyung</engName><name>오용경</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2024.03.08</receiptDate><receiptNumber>1-1-2024-0266581-97</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020240033382.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93c36a90f6218edad0926340e7bcc794cc5f49860ed6fee79a2723f5dc33fb724c2287c092af3f5bf938fdefb08769f4afbf02cb2e2150e988</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfa86d01f579aa2bab0b8b8e8e0104184765925bd452391b52274cfe230c41b632a908cee8aba8201a86854acddc7faf098706d80597b7e200</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>