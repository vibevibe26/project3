<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:11:00.110</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.09.06</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0118558</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>온라인 깊이 스케일 이전을 사용하여 단일 이미지에서의 절대 깊이 추정</inventionTitle><inventionTitleEng>ABSOLUTE DEPTH ESTIMATION FROM A SINGLE IMAGE USING  ONLINE DEPTH SCALE TRANSFER</inventionTitleEng><openDate>2024.09.20</openDate><openNumber>10-2024-0137447</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 3/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/58</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/20</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 단일 이미지에서 절대 깊이를 추정하는 장치 및 이의 동작 방법이 개시된다. 본 개시의 예시적 실시예에 따른 방법은, 목표 고유 파라미터들(target intrinsic parameters)에 기초하여 단안 이미지를 획득하는 단계, 깊이 맵 네트워크를 사용하여 상기 단안 이미지에 기초하여 업-스케일 깊이 맵(up-to-scale depth map)을 생성하는 단계, 상기 목표 고유 파라미터들에 기초한 스케일링 함수를 사용하여 상기 업-스케일 깊이 맵에 기초하여 상기 단안 이미지에 대한 스케일링된 깊이 맵을 생성하는 단계를 포함하며, 상기 깊이 맵 네트워크는 목표 이미지 및 소스 이미지를 사용하여 학습되고, 상기 목표 이미지는 상기 목표 고유 파라미터들에 기초하여 생성되고, 상기 소스 이미지는 소스 고유 파라미터들(source intrinsic parameters)에 기초하여 생성되고, 상기 소스 이미지는 상기 목표 고유 파라미터들과 일치하도록 수정된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 목표 고유 파라미터들(target intrinsic parameters)에 기초하여 단안 이미지를 획득하는 단계;깊이 맵 네트워크를 사용하여 상기 단안 이미지에 기초하여 업-스케일 깊이 맵(up-to-scale depth map)을 생성하는 단계;상기 목표 고유 파라미터들에 기초한 스케일링 함수를 사용하여 상기 업-스케일 깊이 맵에 기초하여 상기 단안 이미지에 대한 스케일링된 깊이 맵을 생성하는 단계를 포함하며,상기 깊이 맵 네트워크는 목표 이미지 및 소스 이미지를 사용하여 학습되고,상기 목표 이미지는 상기 목표 고유 파라미터들에 기초하여 생성되고,상기 소스 이미지는 소스 고유 파라미터들(source intrinsic parameters)에 기초하여 생성되고, 상기 소스 이미지는 상기 목표 고유 파라미터들과 일치하도록 수정되는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,카메라를 이용하여 상기 단안 이미지를 획득하는 단계를 더 포함하며,상기 목표 고유 파라미터들은 상기 카메라의 초점 거리 및 센서 크기에 대응하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 목표 고유 파라미터들은 상기 단안 이미지의 시야 영역에 대응하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,이미지 표현을 얻기 위해서 단안 이미지를 인코딩하는 단계; 및상기 업-스케일 깊이 맵을 얻기 위해서 상기 이미지 표현을 디코딩하는 단계를 더 포함하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,실제 깊이 정보(ground truth depth information)에 기초하여 상기 스케일링 함수를 결정하는 단계를 더 포함하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 업-스케일 깊이 맵에 기초한 스케일링 네트워크를 사용하여 스케일링 팩터(scaling factor)를 계산하는 단계를 더 포함하며,상기 스케일링된 깊이 맵은 상기 스케일링 팩터에 기초하여 생성되는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 스케일링된 깊이 맵에 기초하여 장애물을 식별하는 단계; 및식별된 상기 장애물에 기초하여 내비게이션 정보(navigation information)를 생성하는 단계를 더 포함하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>8. 소스 고유 파라미터들에 기초한 소스 이미지를 포함하는 학습 데이터를 획득하는 단계;상기 소스 이미지에 기초하여 수정된 소스 이미지를 생성하는 단계 - 상기 수정된 소스 이미지는 목표 고유 파라미터들에 기초하여 생성됨-; 및상기 목표 고유 파라미터들에 기초한 단안 이미지에 대한 업-스케일 깊이 맵을 생성하도록 깊이 맵 네트워크를 학습시키는 단계를 포함하며,상기 깊이 맵 네트워크는 상기 수정된 소스 이미지 및 목표 학습 데이터를 사용하여 학습되는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서,상기 소스 이미지는 소스 카메라로부터 획득되고,상기 소스 고유 파라미터들은 상기 소스 카메라의 초점 거리 및 센서 크기에 대응하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제8항에 있어서,목표 카메라를 사용하여 추가 학습 데이터를 획득하는 단계를 더 포함하며,상기 목표 고유 파라미터들은 상기 목표 카메라의 초점 거리 및 센서 크기에 대응하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,SFM(Structure From Motion) 프로세스를 사용하여 상기 추가 학습 데이터에 대한 추가 업-스케일 깊이 정보를 생성하는 단계를 더 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>12. 제8항에 있어서,실제 깊이 정보를 사용하여 상기 업-스케일 깊이 맵에 기초한 스케일링 팩터를 생성하도록 스케일링 네트워크를 학습시키는 단계를 더 포함하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서,상기 소스 이미지에 대한 상기 실제 깊이 정보를 획득하는 단계; 및상기 목표 고유 파라미터들에 기초하여 상기 수정된 소스 이미지에 대응하도록 상기 실제 깊이 정보를 수정하는 단계를 더 포함하는 것을 특징으로 하는, 방법.</claim></claimInfo><claimInfo><claim>14. 카메라의 목표 고유 파라미터들에 기초한 단안 이미지를 획득하도록 구성된 카메라;상기 단안 이미지에 기초한 업-스케일 깊이 맵을 생성하도록 구성된 깊이 맵 네트워크; 및상기 목표 고유 파라미터들에 기초한 스케일링 함수를 사용하여 상기 업-스케일 깊이 맵에 기초하여 상기 단안 이미지에 대한 스케일링된 깊이 맵을 생성하도록 구성된 스케일링 컴포넌트(component)를 포함하며,상기 깊이 맵 네트워크는 목표 이미지 및 소스 이미지를 사용하여 학습되고,상기 목표 이미지는 상기 목표 고유 파라미터들에 기초하여 생성되고,상기 소스 이미지는 소스 고유 파라미터들에 기초하여 생성되고,상기 소스 이미지는 상기 목표 고유 파라미터들과 일치하도록 수정되는 것을 특징으로 하는, 장치.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,이미지 표현을 획득하기 위하여 상기 단안 이미지를 인코딩하도록 구성된 인코더; 및상기 업-스케일 깊이 맵을 획득하기 위하여 상기 이미지 표현을 디코딩하도록 구성된 디코더를 더 포함하는 것을 특징으로 하는, 장치.</claim></claimInfo><claimInfo><claim>16. 제14항에 있어서,상기 스케일링 컴포넌트는 상기 업-스케일 깊이 맵에 기초하여 스케일링 팩터를 계산하고,상기 스케일링된 깊이 맵은 상기 스케일링 팩터에 기초하여 생성되는 것을 특징으로 하는, 장치.</claim></claimInfo><claimInfo><claim>17. 제14항에 있어서,상기 스케일링된 깊이 맵에 기초하여 장애물을 식별하고, 식별된 상기 장애물에 기초하여 내비게이션 정보를 생성하도록 구성된 내비게이션 컴포넌트를 더 포함하는 것을 특징으로 하는, 장치.</claim></claimInfo><claimInfo><claim>18. 제14항에 있어서,업 스케일 깊이 정보를 사용하여 자기 지도의(self-supervised) 방식으로 상기 깊이 맵 네트워크를 학습시키도록 구성된 학습 컴포넌트를 더 포함하는 것을 특징으로 하는, 장치.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서,SFM(Structure From Motion) 프로세스를 사용하여 상기 단안 이미지에 대한 상기 업 스케일 깊이 정보를 생성하도록 구성된 포즈 예측 네트워크(pose prediction network)를 더 포함하는 것을 특징으로 하는, 장치.</claim></claimInfo><claimInfo><claim>20. 제14항에 있어서,상기 소스 이미지의 시야 영역을 상기 목표 시야 영역으로 정렬하도록 구성된 정렬 네트워크를 더 포함하고,상기 소스 이미지는 상기 목표 시야 영역에 대한 정렬에 기초하여 목표 고유 파라미터들과 일치하도록 수정되는 것을 특징으로 하는, 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>SHOMER, Amit</engName><name>쇼머 아밋</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>CARMEL, Nadav</engName><name>카멜 나다브</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>PELEG, Tomer</engName><name>펠레그 토머</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>TZABARI, Assaf</engName><name>차바리 아사프</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>DANA, Alexandra</engName><name>다나 알렉산드라</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2023.03.08</priorityApplicationDate><priorityApplicationNumber>18/180,643</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.09.06</receiptDate><receiptNumber>1-1-2023-0986832-16</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Amendment to Patent Application, etc.] Amendment</documentEngName><documentName>[출원서 등 보정]보정서</documentName><receiptDate>2023.09.07</receiptDate><receiptNumber>1-1-2023-0990303-14</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Submission of Priority Certificate(USPTO)</documentEngName><documentName>우선권주장증명서류제출서(USPTO)</documentName><receiptDate>2023.09.20</receiptDate><receiptNumber>9-1-2023-9010424-18</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230118558.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93cfee614daddf93245bfed8e41b1c0c357a2ff52bd4464a639831064b983257b64cec683e5e2de88a8be40ae668d750a30da9d19bf4524a98</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf4cefca99ddc9543e93b56d4bf68feeae221175090e323ff0a5ad548b3f2cc7ba9941330fb1d341d7d939805f0e2a1197bf19e20647c567c6</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>