<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:40:37.4037</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2021.09.16</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7013000</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>기계 학습 모델을 사용한 동작 탐지</inventionTitle><inventionTitleEng>ACTION DETECTION USING MACHINE LEARNING MODELS</inventionTitleEng><openDate>2023.05.18</openDate><openNumber>10-2023-0069216</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.09.12</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.04.17</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/24</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/52</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06N 20/00</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본원에 기술된 시스템 및 방법은 대상체 행동을 검출하도록 구성된 하나 이상의 학습된 모델을 사용하여 비디오 데이터를 처리함으로써 대상체 행동을 검출하는 기술을 제공한다. 설명된 시스템은 상이한 학습된 모델을 사용하여 비디오 데이터로부터의 프레임 세트를 처리한다. 시스템은 프레임 세트의 상이한 배향을 추가로 처리한다. 상이한 학습된 모델로부터의 다양한 출력 및 프레임 세트의 상이한 배향의 처리로부터의 다양한 출력은 조합된 다음, 대상체가 특정 프레임 동안 특정 행동을 나타내는지 여부에 대한 최종 결정을 내릴 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.03.24</internationOpenDate><internationOpenNumber>WO2022060919</internationOpenNumber><internationalApplicationDate>2021.09.16</internationalApplicationDate><internationalApplicationNumber>PCT/US2021/050572</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 컴퓨터 구현 방법으로서,대상체의 비디오 캡처 움직임을 나타내는 비디오 데이터를 수신하는 단계;상기 비디오 데이터로부터 제1 프레임 세트를 식별하는 단계;상기 제1 프레임 세트를 회전시킴으로써 회전된 프레임 세트를 결정하는 단계;상기 대상체가 소정의 행동 동작을 나타낼 가능성을 식별하도록 구성된 제1 학습된 모델을 사용하여 상기 제1 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 제1 프레임 세트의 처리에 기초하여, 상기 제1 프레임 세트의 제1 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제1 확률을 결정하되, 상기 제1 프레임은 상기 비디오 데이터의 지속시간에 해당하는 단계;상기 제1 학습된 모델을 사용하여 상기 회전된 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 회전된 프레임 세트의 처리에 기초하여, 상기 회전된 프레임 세트의 제2 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제2 확률을 결정하되, 상기 제2 프레임은 상기 제1 프레임의 지속시간에 해당하는 단계; 및상기 제1 확률 및 상기 제2 확률을 사용하여, 상기 제1 프레임에 대한 라벨을 식별하되, 상기 제1 라벨은 상기 대상체가 상기 소정의 행동 동작을 나타냄을 표시하는 단계를 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 대상체가 상기 소정의 행동 동작을 나타낼 가능성을 식별하도록 구성된 제2 학습된 모델을 사용하여 상기 제1 프레임 세트를 처리하는 단계;상기 제2 학습된 모델에 의한 상기 제1 프레임 세트의 처리에 기초하여, 상기 제1 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제3 확률을 결정하는 단계;상기 제2 학습된 모델을 사용하여 상기 회전된 프레임 세트를 처리하는 단계;상기 제2 학습된 모델에 의한 상기 회전된 프레임 세트의 처리에 기초하여, 상기 제2 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제4 확률을 결정하는 단계; 및상기 제1 확률, 상기 제2 확률, 상기 제3 확률 및 상기 제4 확률을 사용하여 상기 제1 라벨을 식별하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 제1 프레임 세트를 반사하여 반사된 프레임 세트를 결정하는 단계;상기 제1 학습된 모델을 사용하여 상기 반사된 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 반사된 프레임 세트의 처리에 기초하여, 상기 반사된 프레임 세트의 제3 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제3 확률을 결정하되, 상기 제3 프레임은 상기 제1 프레임에 상응하는 단계; 및상기 제1 확률, 상기 제2 확률, 및 상기 제3 확률을 사용하여 상기 제1 라벨을 식별하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>4. 제1항 내지 제3항 중 어느 한 항에 있어서, 상기 소정의 행동 동작은 그루밍 행동을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서, 상기 대상체는 마우스이고, 상기 소정의 행동은 발 핥기, 한쪽 얼굴 씻기, 양쪽 얼굴 씻기, 및 옆구리 핥기 중 적어도 하나를 포함하는 그루밍 행동을 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 상기 제1 프레임 세트는 일정 기간 동안 상기 비디오 데이터의 일부를 나타내고, 상기 제1 프레임은 상기 기간의 마지막 시간 프레임인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 비디오 데이터로부터 제2 프레임 세트를 식별하는 단계;상기 제2 프레임 세트를 회전시킴으로써 제2 회전된 프레임 세트를 결정하는 단계;상기 제1 학습된 모델을 사용하여 상기 제2 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 제2 프레임 세트의 처리에 기초하여, 상기 제2 프레임 세트의 제3 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제3 확률을 결정하는 단계;상기 제1 학습된 모델을 사용하여 상기 제2 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 제2 회전된 프레임 세트의 처리에 기초하여, 상기 회전된 프레임 세트의 제4 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제4 확률을 결정하되, 상기 제4 프레임은 상기 제3 프레임의 상응하는 단계; 및상기 제3 확률 및 상기 제4 확률을 사용하여, 상기 제4 프레임에 대한 제2 라벨을 식별하되, 상기 제1 라벨은 상기 대상체가 상기 소정의 행동 동작을 나타냄을 표시하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,적어도 상기 제1 라벨 및 상기 제2 라벨을 사용하여, 일정 기간 동안 상기 대상체의 소정의 행동 동작을 나타내는 에소그램을 생성하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서, 상기 제1 학습된 모델은 기계 학습 분류기인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서, 상기 비디오 데이터를 수신하기 전에,제1 복수의 비디오 프레임 및 제2 복수의 비디오 프레임을 포함하는 학습 데이터를 수신하되, 상기 제1 복수의 비디오 프레임의 각각은 상기 대상체가 상기 소정의 행동 동작을 나타내고 있음을 표시하는 양의 라벨과 연관되고, 상기 제2 복수의 비디오 프레임의 각각은 상기 대상체가 상기 소정의 행동 동작이 아닌 행동 동작을 나타내고 있음을 표시하는 음의 라벨과 연관되는 단계;제1 모델 파라미터 세트 및 제1 분류기 모델 데이터를 사용하여 상기 학습 데이터를 처리하여 상기 제1 학습된 모델을 결정하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서, 상기 제1 복수의 프레임 및 상기 제2 복수의 프레임은 복수의 대상체의 움직임을 나타내고, 상기 복수의 대상체 중 하나의 대상체는 하나 이상의 미리 식별된 물리적 특성(들)을 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서, 상기 미리 식별된 물리적 특성은 체형, 신체 크기, 코트 색상, 성별, 연령, 및 질환 또는 장애의 표현형 중 하나 이상인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서, 상기 질환 또는 장애는 유전적 질환, 부상, 또는 전염성 질환인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>14. 제10항에 있어서, 상기 제1 복수의 프레임 및 상기 제2 복수의 프레임은 복수의 마우스 대상체의 움직임을 나타내고, 상기 복수의 마우스 중 하나의 마우스는 코트 색상, 성별, 체형 및 크기를 갖는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>15. 제1항 내지 제14항 중 어느 한 항에 있어서, 상기 대상체는 포유동물인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>16. 제1항 내지 제15항 중 어느 한 항에 있어서, 상기 대상체는 유전자 조작된 대상체인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>17. 컴퓨터 구현 방법으로서,대상체의 비디오 캡처 움직임을 나타내는 비디오 데이터를 수신하는 단계;상기 비디오 데이터로부터 제1 프레임 세트를 식별하는 단계;상기 대상체가 소정의 행동 동작을 나타낼 가능성을 식별하도록 구성된 제1 학습된 모델을 사용하여 상기 제1 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 제1 프레임 세트의 처리에 기초하여, 상기 제1 프레임 세트의 제1 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제1 확률을 결정하는 단계;상기 대상체가 상기 소정의 행동 동작을 나타낼 가능성을 식별하도록 구성된 제2 학습된 모델을 사용하여 상기 제1 프레임 세트를 처리하는 단계;상기 제2 학습된 모델에 의한 상기 제1 프레임 세트의 처리에 기초하여, 상기 제1 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제2 확률을 결정하는 단계; 및상기 제1 확률 및 상기 제2 확률을 사용하여, 상기 제1 프레임에 대한 제1 라벨을 식별하되, 상기 제1 라벨은 상기 대상체가 상기 소정의 행동 동작을 나타냄을 표시하는 단계를 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>18. 제17항에 있어서,상기 제1 프레임 세트를 회전시킴으로써 회전된 프레임 세트를 결정하는 단계;상기 제1 학습된 것을 사용하여 상기 회전된 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 회전된 프레임 세트의 처리에 기초하여, 상기 회전된 프레임 세트의 제2 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제3 확률을 결정하되, 상기 제2 프레임은 상기 제1 프레임에 상응하는 단계;상기 제2 학습된 모델을 사용하여 상기 회전된 프레임 세트를 처리하는 단계;상기 제2 학습된 모델에 의한 상기 회전된 프레임 세트의 처리에 기초하여, 상기 제2 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제4 확률을 결정하는 단계; 및상기 제1 확률, 상기 제2 확률, 상기 제3 확률 및 상기 제4 확률을 사용하여 상기 제1 라벨을 식별하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>19. 제17항에 있어서,상기 제1 프레임 세트를 반사하여 반사된 프레임 세트를 결정하는 단계;상기 제1 학습된 것을 사용하여 상기 반사된 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 반사된 프레임 세트의 처리에 기초하여, 상기 반사된 프레임 세트의 제2 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제3 확률을 결정하되, 상기 제2 프레임은 상기 제1 프레임에 상응하는 단계;상기 제2 학습된 모델을 사용하여 상기 반사된 프레임 세트를 처리하는 단계;상기 제2 학습된 모델에 의한 상기 반사된 프레임 세트의 처리에 기초하여, 상기 제2 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제4 확률을 결정하는 단계; 및상기 제1 확률, 상기 제2 확률, 상기 제3 확률 및 상기 제4 확률을 사용하여 상기 제1 라벨을 식별하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>20. 제17항에 있어서, 상기 제1 모델 및 상기 제2 모델은 신경망 모델이고, 상기 제1 모델은 제1 파라미터 세트를 사용하여 초기화되고, 상기 제2 학습된 모델은 상기 제1 파라미터 세트와 상이한 제2 파라미터 세트를 사용하여 초기화되는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>21. 제17항에 있어서,상기 대상체가 소정의 행동 동작을 나타낼 가능성을 식별하도록 구성된 제3 학습된 모델을 사용하여 상기 제1 프레임 세트를 처리하는 단계;상기 제3 학습된 모델에 의한 상기 제1 프레임 세트의 처리에 기초하여, 상기 제1 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제3 확률을 결정하는 단계;상기 대상체가 상기 소정의 행동 동작을 나타낼 가능성을 식별하도록 구성된 제4 학습된 모델을 사용하여 상기 제1 프레임 세트를 처리하는 단계;상기 제4 학습된 모델에 의한 상기 제1 프레임 세트의 처리에 기초하여, 상기 제1 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제4 확률을 결정하는 단계; 및상기 제1 확률, 상기 제2 확률, 상기 제3 확률 및 상기 제4 확률을 사용하여 상기 제1 라벨을 식별하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>22. 제17항 내지 제21항 중 어느 한 항에 있어서, 상기 대상체는 포유동물인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>23. 제17항 내지 제22항 중 어느 한 항에 있어서, 상기 소정의 행동 동작은 그루밍 행동을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 제17항에 있어서, 상기 대상체는 마우스이고, 상기 소정의 행동 동작은 그루밍 행동을 포함하고, 상기 그루밍 행동은 발 핥기, 한쪽 얼굴 씻기, 양쪽 얼굴 씻기, 및 옆구리 핥기 중 적어도 하나인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>25. 제17항에 있어서, 상기 제1 프레임 세트는 일정 기간 동안 상기 비디오 데이터의 일부를 나타내고, 상기 제1 프레임은 상기 기간의 마지막 시간 프레임인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>26. 제17항에 있어서,상기 비디오 데이터로부터 제2 프레임 세트를 식별하는 단계;상기 제1 학습된 모델을 사용하여 상기 제2 프레임 세트를 처리하는 단계;상기 제1 학습된 모델에 의한 상기 제2 프레임 세트의 처리에 기초하여, 상기 제2 프레임 세트의 제3 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제3 확률을 결정하는 단계;상기 제2 학습된 모델을 사용하여 상기 제2 프레임 세트를 처리하는 단계;상기 제2 학습된 모델에 의한 상기 제2 프레임 세트의 처리에 기초하여, 상기 제3 프레임에서 상기 대상체가 상기 소정의 행동 동작을 나타낼 제4 확률을 결정하는 단계; 및상기 제3 확률 및 상기 제4 확률을 사용하여, 상기 제3 프레임에 대한 제2 라벨을 식별하되, 상기 제2 라벨은 상기 대상체가 상기 소정의 행동 동작을 나타냄을 표시하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>27. 제26항에 있어서,적어도 상기 제1 라벨 및 상기 제2 라벨을 사용하여, 일정 기간 동안 상기 대상체의 소정의 행동 동작을 나타내는 에소그램을 생성하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>28. 제17항에 있어서, 상기 제1 학습 모델 및 상기 제2 학습 모델은 기계 학습 분류기인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>29. 제17항에 있어서, 상기 비디오 데이터를 수신하기 전에,제1 복수의 비디오 프레임 및 제2 복수의 비디오 프레임을 포함하는 학습 데이터를 수신하되, 상기 제1 복수의 비디오 프레임의 각각은 상기 대상체가 상기 소정의 행동 동작을 나타내고 있음을 표시하는 양의 라벨과 연관되고, 상기 제2 복수의 비디오 프레임의 각각은 상기 대상체가 상기 소정의 행동이 아닌 행동 동작을 나타내고 있음을 표시하는 음의 라벨과 연관되는 단계;제1 모델 파라미터 세트 및 제1 분류기 모델 데이터를 사용하여 상기 학습 데이터를 처리하여 상기 제1 학습된 모델을 결정하는 단계; 및제2 모델 파라미터 세트 및 제2 분류기 모델 데이터를 사용하여 상기 학습 데이터를 처리하여 상기 제2 학습된 모델을 결정하는 단계를 더 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>30. 제29항에 있어서, 상기 제1 복수의 프레임 및 상기 제2 복수의 프레임은 복수의 대상체의 움직임을 나타내고, 상기 복수의 대상체 중 하나의 대상체는 하나 이상의 미리 식별된 물리적 특성(들)을 포함하는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>31. 제30항에 있어서, 상기 미리 식별된 물리적 특성은 체형, 신체 크기, 코트 색상, 성별, 연령, 및 질환 또는 장애의 표현형 중 하나 이상인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>32. 제31항에 있어서, 상기 질환 또는 장애는 유전적 질환, 부상, 또는 전염성 질환인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>33. 제29항에 있어서, 상기 제1 복수의 프레임 및 상기 제2 복수의 프레임은 복수의 마우스 대상체의 움직임을 나타내고, 상기 복수의 마우스 중 하나의 마우스는 코트 색상, 성별, 체형 및 크기를 갖는, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>34. 제17항 내지 제33항 중 어느 한 항에 있어서, 상기 대상체는 설치류이고, 선택적으로 마우스인, 컴퓨터 실행 방법.</claim></claimInfo><claimInfo><claim>35. 제17항 내지 제34항 중 어느 한 항에 있어서, 상기 대상체는 유전자 조작된 대상체인, 컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>36. 대상체에서 소정의 행동 동작을 평가하는 방법으로서, 상기 소정의 행동 동작은 발 핥기, 한쪽 얼굴 씻기, 양쪽 얼굴 씻기, 및 옆구리 핥기 중 적어도 하나를 포함하는 그루밍 행동을 포함하고, 상기 평가 수단은 제1항 또는 제17항의 컴퓨터 구현 방법을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>37. 제36항에 있어서, 상기 대상체는 소정의 행동 관련 질환 또는 장애를 가지며, 선택적으로 상기 소정의 행동 관련 질환 또는 장애의 동물 모델인, 방법.</claim></claimInfo><claimInfo><claim>38. 제36항에 있어서, 상기 대상체는 유전자 조작된 대상체인, 방법.</claim></claimInfo><claimInfo><claim>39. 제36항에 있어서, 상기 대상체는 설치류이고, 선택적으로 마우스인, 방법.</claim></claimInfo><claimInfo><claim>40. 제39항에 있어서, 상기 마우스는 유전자 조작된 마우스인, 방법.</claim></claimInfo><claimInfo><claim>41. 제36항에 있어서, 후보 치료제를 상기 대상체에게 투여하는 단계, 상기 후보 치료제의 투여 후 상기 대상체에서 상기 소정의 행동 동작을 평가하는 단계, 상기 투여 후 평가를 상기 소정의 행동 동작에 대한 대조군 평가와 비교하는 단계를 더 포함하되, 상기 대조군의 소정의 행동과 비교하여 상기 투여 후 소정의 행동 동작의 변화는 상기 소정의 행동 동작에 대한 상기 투여된 후보 치료제의 효과를 식별하는 것인, 방법.</claim></claimInfo><claimInfo><claim>42. 제41항에 있어서, 상기 변화는 상기 대상체에서의 상기 소정의 행동 동작의 시작, 증가, 중단, 및 감소 중 하나 이상을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>43. 제41항에 있어서, 상기 후보 치료제는 상기 소정의 행동 동작을 평가하기 전에 상기 대상체에게 투여되는, 방법.</claim></claimInfo><claimInfo><claim>44. 제41항에 있어서, 상기 후보 치료제는 상기 소정의 행동 동작을 평가하는 것과 동시에 상기 대상체에게 투여되는, 방법.</claim></claimInfo><claimInfo><claim>45. 제41항에 있어서, 상기 소정의 행동 동작의 대조군 평가는 상기 컴퓨터 구현 방법으로 모니터링되는 대조군 대상체에서의 상기 소정의 행동의 평가인, 방법.</claim></claimInfo><claimInfo><claim>46. 제41항에 있어서, 상기 대조군 대상체는 상기 소정의 행동 관련 질환 또는 장애의 동물 모델인, 방법.</claim></claimInfo><claimInfo><claim>47. 제46항에 있어서, 상기 소정의 행동 동작 관련 질환 또는 장애는 유전적 질환, 부상, 또는 전염성 질환인, 방법.</claim></claimInfo><claimInfo><claim>48. 제46항에 있어서, 상기 소정의 행동 관련 질환 또는 장애는 양극성 장애, 치매, 우울증, 운동과잉 장애, 불안 장애, 발달 장애, 수면 장애, 알츠하이머병, 파킨슨병, 또는 신체적 부상인, 방법.</claim></claimInfo><claimInfo><claim>49. 제45항 또는 제46항에 있어서, 상기 대조군 대상체에게 상기 후보 치료제가 투여되지 않는, 방법.</claim></claimInfo><claimInfo><claim>50. 제44항에 있어서, 상기 대상체에게 투여된 상기 후보 치료제의 용량과 상이한 상기 후보 치료제의 용량이 상기 대조군 대상체에게 투여되는, 방법.</claim></claimInfo><claimInfo><claim>51. 제44항에 있어서, 상기 대조군 결과는 상기 컴퓨터 구현 방법을 활용한 상기 대상체의 이전 모니터링으로부터의 결과이고, 선택적으로 상기 대상체의 이전 모니터링은 상기 후보 치료제의 투여 전에 발생하는, 방법.</claim></claimInfo><claimInfo><claim>52. 제37항에 있어서, 상기 대상체의 모니터링은 상기 대상체에서 상기 소정의 행동 관련 질환 또는 장애를 식별하는, 방법.</claim></claimInfo><claimInfo><claim>53. 제37항에 있어서, 상기 대상체의 모니터링은 상기 소정의 행동 관련 질환 또는 장애를 치료하기 위한 후보 치료제의 효능을 식별하는, 방법.</claim></claimInfo><claimInfo><claim>54. 대상체에서 소정의 행동 관련 질환 또는 장애를 치료하기 위한 후보 치료제의 효능을 식별하는 방법으로서,상기 후보 치료제를 대상체에게 투여하는 단계, 및상기 대상체에서 하나 이상의 소정의 행동 동작을 모니터링하는 단계를 포함하되, 상기 모니터링 수단은 제1항 또는 제17항의 컴퓨터 구현 방법을 포함하고, 상기 소정의 행동 동작은 발 핥기, 한쪽 얼굴 씻기, 양쪽 얼굴 씻기, 및 옆구리 핥기 중 적어도 하나를 포함하는 그루밍 행동을 포함하고, 상기 대상체에서 상기 소정의 행동 동작에서의 변화를 나타내는 상기 모니터링의 결과는 상기 소정의 행동 관련 질환 또는 장애를 치료하기 위한 상기 후보 치료제의 효능을 식별하는, 방법.</claim></claimInfo><claimInfo><claim>55. 제54항에 있어서, 상기 대상체는 소정의 행동 관련 질환 또는 장애를 갖고, 선택적으로 상기 소정의 행동-관련 질환 또는 장애의 동물 모델인, 방법.</claim></claimInfo><claimInfo><claim>56. 제54항에 있어서, 상기 대상체는 상기 소정의 행동 관련 질환 또는 장애의 동물 모델인, 방법.</claim></claimInfo><claimInfo><claim>57. 제54항에 있어서, 상기 소정의 행동 관련 질환 또는 장애는 유전적 질환, 부상 또는 전염성 질환인, 방법.</claim></claimInfo><claimInfo><claim>58. 제54항에 있어서, 상기 소정의 행동 관련 질환 또는 장애는 양극성 장애, 치매, 우울증, 운동과잉 장애, 불안 장애, 발달 장애, 수면 장애, 알츠하이머병, 파키슨병, 또는 신체적 부상인, 방법.</claim></claimInfo><claimInfo><claim>59. 제54항에 있어서, 상기 대상체는 유전자 조작된 대상체인, 방법.</claim></claimInfo><claimInfo><claim>60. 제54항에 있어서, 상기 대상체는 설치류이고, 선택적으로 마우스인, 방법.</claim></claimInfo><claimInfo><claim>61. 제63항에 있어서, 상기 마우스는 유전자 조작된 마우스인, 방법.</claim></claimInfo><claimInfo><claim>62. 제54항에 있어서, 상기 후보 치료제는 상기 소정의 행동을 모니터링하기 전에 상기 대상체에게 투여되는, 방법.</claim></claimInfo><claimInfo><claim>63. 제54항에 있어서, 상기 후보 치료제는 상기 소정의 행동을 모니터링하는 것과 동시에 상기 대상체에게 투여되는, 방법.</claim></claimInfo><claimInfo><claim>64. 제54항에 있어서, 상기 대상체에서의 상기 모니터링되는 소정의 행동은 상기 소정의 행동의 대조군 모니터링과 비교되고, 상기 대조군 모니터링은 상기 컴퓨터 구현 방법으로 대조군 대상체에서 상기 소정의 행동을 모니터링하는 것을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>65. 제64항에 있어서, 상기 대조군 대상체는 상기 소정의 행동 관련 질환 또는 장애의 동물 모델인, 방법.</claim></claimInfo><claimInfo><claim>66. 제64항에 있어서, 상기 대조군 대상체에게 상기 후보 치료제를 투여하지 않는, 방법.</claim></claimInfo><claimInfo><claim>67. 제64항에 있어서, 상기 대상체에게 투여된 상기 후보 치료제의 용량과 상이한 상기 후보 치료제의 용량이 상기 대조군 대상체에게 투여되는, 방법.</claim></claimInfo><claimInfo><claim>68. 제64항에 있어서, 상기 대조군 모니터링은 상기 후보 치료제의 투여 전 한 시점에 컴퓨터 구현 방법으로 상기 대상체에서의 상기 소정의 행동 동작을 모니터링하는 것인, 방법.</claim></claimInfo><claimInfo><claim>69. 제54항에 있어서, 상기 대상체의 모니터링은 상기 소정의 행동 관련 질환 또는 장애를 치료하기 위한 상기 후보 치료제의 효능을 식별하는, 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 메인(우편번호 *****) 바 하버 메인 스트리트 ***</address><code>520120146723</code><country>미국</country><engName>The Jackson Laboratory</engName><name>더 잭슨 래보라토리</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 메인 ***** 바 하버...</address><code> </code><country> </country><engName>KUMAR, Vivek</engName><name>쿠마르, 비벡</name></inventorInfo><inventorInfo><address>미국 메인 ***** 바 하버...</address><code> </code><country> </country><engName>GEUTHER, Brian Q.</engName><name>고이터, 브라이언 큐.</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 송파구  올림픽로  *** ,*층 (신천동, 대한제당)</address><code>920131000018</code><country>대한민국</country><engName>Envision Patent &amp; Law Firm</engName><name>인비전특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2020.09.16</priorityApplicationDate><priorityApplicationNumber>63/078,952</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.04.17</receiptDate><receiptNumber>1-1-2023-0429160-78</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.04.19</receiptDate><receiptNumber>1-5-2023-0064182-59</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2024.09.12</receiptDate><receiptNumber>1-1-2024-1006192-32</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2024.09.12</receiptDate><receiptNumber>1-1-2024-1006227-42</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237013000.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c939b55c6c8bf3df7de689a8788b3a5fda64f6b0d81b168718ba41b4461950affeed936011f3ff4361dc96525f779e265ac9c966a9039cf645c</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf2e1fe62b5002fb6b9b8d1cc7caf3edda244477fa3599cec79a43a49e208bdef514a957533695f566e768d3868a09a5838a0222260825d7d0</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>