<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:23:04.234</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.09.26</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2025-7001990</applicationNumber><claimCount>25</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>리포터 신경망을 사용한 에이전트 제어</inventionTitle><inventionTitleEng>CONTROLLING AGENTS USING REPORTER NEURAL NETWORKS</inventionTitleEng><openDate>2025.02.25</openDate><openNumber>10-2025-0026289</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.01.20</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.01.20</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 16/9032</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 16/903</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06F 16/9038</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/35</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/006</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0442</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0455</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0464</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/088</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/092</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/094</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 컴퓨터 저장 매체에 인코딩된 컴퓨터 프로그램을 포함하여 리포터 신경망을 사용하여 에이전트를 제어하기 위한 방법, 시스템 및 장치. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2024.04.04</internationOpenDate><internationOpenNumber>WO2024068610</internationOpenNumber><internationalApplicationDate>2023.09.26</internationalApplicationDate><internationalApplicationNumber>PCT/EP2023/076516</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 하나 이상의 컴퓨터에 의해 수행되는 방법으로서, 상기 방법은:태스크 설명을 수신하는 단계 - 상기 태스크 설명은 환경에서 에이전트가 수행할 태스크에 대한 자연어 설명임 -; 및에이전트로 하여금 태스크를 수행하게 하기 위해 시간 단계 시퀀스에 걸쳐 에이전트를 제어하는 단계를 포함하고, 상기 에이전트를 제어하는 단계는 시퀀스 내 복수의 시간 단계 각각에서: 시간 단계에서 환경의 상태를 특징짓는 제1 관찰(observation) 획득하는 단계; 에이전트가 태스크를 완료하는 진행 상황(progress)을 시간 단계별로 특징짓는 자연어 보고서(report)를 정의하는 리포터(reporter) 출력을 생성하기 위해 리포터 신경망을 사용하여 제1 관찰을 포함하는 리포터 입력을 처리하는 단계; 에이전트에 대한 자연어 명령(instruction)을 생성하기 위해 플래너(planner) 신경망을 사용하여 상기 태스크 설명과 자연어 보고서를 포함하는 플래너 입력을 처리하는 단계; 시간 단계에서 환경의 상태를 특징짓는 제2 관찰을 획득하는 단계; 에이전트가 수행할 액션을 정의하는 정책(policy) 출력을 생성하기 위해 정책 신경망을 사용하여 자연어 명령과 제2 관찰을 처리하는 단계; 정책 출력을 사용하여 액션을 선택하는 단계; 및 에이전트로 하여금 선택된 액션을 수행하게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 선행하는 어느 한 항에 있어서, 플래너 신경망은 언어 모델링 목표에 대한 비지도 학습(unsupervised learning)을 통해 미리 트레이닝된, 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 플래너 입력은 하나 이상의 프롬프트 입력을 더 포함하고, 하나 이상의 프롬프트 입력은 각각:예시 태스크 설명;하나 이상의 예시 리포터 입력; 및각 리포터 입력에 대해, 리포터 입력에 대한 응답으로 생성된 예시 자연어 보고서를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제2항 내지 제3항 중 어느 한 항에 있어서, 리포터 출력은 자연어 보고서인, 방법. </claim></claimInfo><claimInfo><claim>5. 제2항 내지 제4항 중 어느 한 항에 있어서, 리포터 출력은 복수의 카테고리에 대한 분류 출력을 포함하는, 방법. </claim></claimInfo><claimInfo><claim>6. 제5항에 있어서,분류 출력을 사용하여 복수의 카테고리 중에서 카테고리를 선택하는 단계; 및자연어 보고서를 생성하는 단계를 더 포함하고, 상기 자연어 보고서를 생성하는 단계는 자연어 보고서 내의 미리 결정된 위치에, 선택된 카테고리의 자연어 설명을 삽입하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 선행하는 어느 한 항에 있어서,에이전트가 선택된 액션을 수행한 것에 대한 응답으로 태스크에 대한 보상(reward)을 수신하는 단계; 및수신된 보상을 사용하여 강화 학습(reinforcement learning)을 통해 리포터 신경망을 트레이닝하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서, 상기 수신된 보상을 사용하여 강화 학습을 통해 리포터 신경망을 트레이닝하는 단계는:정책 신경망을 트레이닝하지 않고 리포터 신경망을 트레이닝하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서, 정책 신경망은 상기 태스크 설명에 의해 설명된 태스크와 상이한 태스크에 대해 트레이닝된, 방법.</claim></claimInfo><claimInfo><claim>10. 제7항 내지 제9항 중 어느 한 항에 있어서, 리포터 신경망은 미리 트레이닝된 하나 이상의 인코더 신경망을 포함하는, 방법. </claim></claimInfo><claimInfo><claim>11. 제10항에 있어서, 제1 관찰은 환경의 이미지를 포함하고, 리포터 신경망은 상기 이미지를 처리하도록 구성되고 시각적 표현 학습 태스크에 대해 미리 트레이닝된 제1 인코더 신경망을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 제10항 또는 제11항에 있어서, 리포터 입력은 이전 시간 단계로부터의 자연어 명령을 더 포함하고, 리포터 신경망은 상기 자연어 명령을 처리하도록 구성되고 텍스트 표현 학습 태스크에 대해 미리 트레이닝된 제2 인코더 신경망을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>13. 제10항 내지 제12항 중 어느 한 항에 있어서, 리포터 신경망은 각 인코더 신경망으로부터 각각의 출력을 수신하고 각각의 출력으로부터 리포터 출력을 생성하도록 구성된 출력 서브네트워크를 포함하고, 상기 리포터 신경망을 트레이닝하는 단계는:미리 트레이닝된 인코더를 고정된 상태로 유지하면서 강화 학습을 통해 출력 서브네트워크를 트레이닝하는 단계; 또는강화 학습을 통해 출력 서브네트워크와 미리 트레이닝된 인코더를 트레이닝하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>14. 선행하는 어느 한 항에 있어서, 제1 관찰은 제2 관찰과 동일한, 방법.</claim></claimInfo><claimInfo><claim>15. 선행하는 어느 한 항에 있어서, 에이전트는 기계적(mechanical) 에이전트이고, 환경은 실제 세계(real-world) 환경인, 방법.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서, 기계적 에이전트는 로봇인, 방법.</claim></claimInfo><claimInfo><claim>17. 제15항 또는 제16항에 잇어서, 제1 및 제2 관찰은 기계적 에이전트의 하나 이상의 센서에 의해 캡처된 센서 판독값으로부터 생성된 데이터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>18. 선행하는 어느 한 항에 있어서, 상기 태스크 설명을 획득하는 것은 사용자로부터 텍스트 입력 또는 음성 입력(spoken input)으로서 상기 태스크 설명을 획득하는 것을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>19. 선행하는 어느 한 항에 있어서, 플래너 입력은 시퀀스 내 하나 이상의 이전 시간 단계로부터의 하나 이상의 자연어 보고서를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>20. 선행하는 어느 한 항에 있어서, 데모(demonstration) 데이터를 획득하는 단계 - 상기 데모 데이터는 예시 태스크에 대한 자연어 설명 및 전문가 에이전트에 의한 예시 태스크의 수행을 특징짓는 데이터를 포함함 -; 및데모 데이터에 대한 모방 학습(imitation learning)을 통해 리포터 신경망을 트레이닝하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>21. 제20항에 있어서, 상기 데모 데이터에 대한 모방 학습(imitation learning)을 통해 리포터 신경망을 트레이닝하는 단계는:정책 신경망과 플래너 신경망을 고정된 상태로 유지하면서 상기 데모 데이터에 대한 리포터 신경망을 트레이닝하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>22. 선행하는 어느 한 항에 있어서, 환경은 복수의 전자 장비 항목을 포함하는 서비스 시설의 실제 세계 환경이고, 에이전트는 상기 서비스 시설의 동작을 제어하도록 구성된 전자 에이전트인, 방법.</claim></claimInfo><claimInfo><claim>23. 선행하는 어느 한 항에 있어서, 환경은 제품을 제조하기 위한 실제 세계 제조 환경이고, 에이전트는 상기 제품을 제조하기 위해 동작하는 제조 단위 또는 기계를 제어하도록 구성된 전자 에이전트를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 시스템으로서, 상기 시스템은:하나 이상의 컴퓨터; 및하나 이상의 컴퓨터에 통신적으로 결합된 하나 이상의 저장 장치를 포함하고, 상기 하나 이상의 저장 장치는 하나 이상의 컴퓨터에 의해 실행될 때, 하나 이상의 컴퓨터로 하여금 제1항 내지 제23항 중 어느 한 항의 각각의 방법의 동작들을 수행하게 하는 명령어들을 저장하는, 시스템.</claim></claimInfo><claimInfo><claim>25. 하나 이상의 비일시적 컴퓨터 저장 매체로서, 하나 이상의 컴퓨터에 의해 실행될 때, 하나 이상의 컴퓨터로 하여금 제1항 내지 제23항 중 어느 한 항의 각각의 방법의 동작들을 수행하게 하는 명령어들을 저장하는, 하나 이상의 비일시적 컴퓨터 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>영국 런던 이씨*에이 *티더블유 뉴 스트리트 스퀘어 *</address><code>520170032411</code><country>영국</country><engName>DeepMind Technologies Limited</engName><name>딥마인드 테크놀로지스 리미티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>영국 런던 엔*씨 ...</address><code> </code><country>인도</country><engName>DASGUPTA, Ishita</engName><name>다스굽타 이시타</name></inventorInfo><inventorInfo><address>영국 런던 엔*씨 ...</address><code> </code><country>미국</country><engName>CHEN, Shiqi</engName><name>첸 시치</name></inventorInfo><inventorInfo><address>영국 런던 엔*씨 ...</address><code> </code><country>미국</country><engName>MARINO, Kenneth Daniel</engName><name>마리노 케네스 다니엘</name></inventorInfo><inventorInfo><address>영국 런던 엔*씨 ...</address><code> </code><country>중국</country><engName>SHANG, Wenling</engName><name>상 웬링</name></inventorInfo><inventorInfo><address>영국 런던 엔*씨 ...</address><code> </code><country>미국</country><engName>AHUJA, Arun</engName><name>아후자 아룬</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 강남대로 *** (논현동) *-*F(박장원특허법률사무소)</address><code>919980002023</code><country>대한민국</country><engName>PARK, Jang Won</engName><name>박장원</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.09.26</priorityApplicationDate><priorityApplicationNumber>63/410,156</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2025.01.20</receiptDate><receiptNumber>1-1-2025-0074338-40</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2025.02.03</receiptDate><receiptNumber>1-5-2025-0017849-82</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257001990.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93020608318577d67684029bd58afea0f40a11083b283a131aab5edbf83963b0e27b5f79690d79762dfe22ca3d940c06fb3f13dc08144e1c64</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfcf67aa34dae3b73dd738e502103938015aa7d4c0dc99d149688e0889280f2be5d0d50e0322efe9760558bab52dceb9162a5834eedcdeaa42</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>