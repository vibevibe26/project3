<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:05:37.537</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2016.10.18</applicationDate><applicationFlag>특허</applicationFlag><applicationNumber>10-2024-7028601</applicationNumber><claimCount>32</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>3차원 공간에서 가상 객체들 선택</inventionTitle><inventionTitleEng>SELECTING VIRTUAL OBJECTS IN A THREE-DIMENSIONAL SPACE</inventionTitleEng><openDate>2024.09.05</openDate><openNumber>10-2024-0134054</openNumber><originalApplicationDate>2016.10.18</originalApplicationDate><originalApplicationKind>국제출원/분할</originalApplicationKind><originalApplicationNumber>10-2024-7006073</originalApplicationNumber><originalExaminationRequestDate>2024.08.26</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.08.26</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06F 3/04815</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 1/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G06F 3/0346</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06F 3/0482</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06F 3/04883</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo><familyApplicationNumber>1020247006073</familyApplicationNumber></familyInfo></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 웨어러블 시스템을 사용하여 3차원 공간에서 가상 객체들과 상호작용하기 위한 시스템들 및 방법들이 개시된다. 웨어러블 시스템은 사용자가 사용자 입력 디바이스 및 포즈들을 사용하여 가상 객체들과 상호작용하도록 프로그래밍될 수 있다. 웨어러블 시스템은 또한 사용자의 환경에서 가상 객체들의 레이아웃과 같은 맥락 관련 정보를 자동적으로 결정하고 맥락 관련 정보에 기반하여 사용자 입력 모드를 스위칭할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2017.04.27</internationOpenDate><internationOpenNumber>WO2017070121</internationOpenNumber><internationalApplicationDate>2016.10.18</internationalApplicationDate><internationalApplicationNumber>PCT/US2016/057554</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 3차원(3D) 공간에 위치한 가상 객체를 선택하기 위한 방법으로서,컴퓨터 하드웨어를 포함하는 증강 현실(AR) 시스템의 제어하에서 — 상기 AR 시스템은 사용자의 FOR(field of regard) 내의 상호작용가능 객체들과 사용자 상호작용을 허용하도록 구성되며, 상기 FOR은 상기 AR 시스템을 통해 상기 사용자가 인지할 수 있는, 상기 사용자 주위의 환경의 일부분을 포함함 —:상기 사용자의 FOR 내의 상호작용가능 객체들의 그룹을 결정하는 단계; 상기 사용자의 포즈를 결정하는 단계;상기 사용자의 포즈에 적어도 부분적으로 기반하여 상기 사용자의 FOV(field of view)를 결정하는 단계 — 상기 FOV는 주어진 시간에 상기 사용자가 인지하는 상기 FOR의 일부분을 포함함 —;상기 사용자의 포즈 또는 FOV의 변경에 기반하여, 상기 사용자의 FOV에 위치하는 상호작용가능 객체들의 서브그룹을 업데이트하는 단계;상기 상호작용가능 객체들의 서브그룹의 디스플레이를 확대하는 단계; 상기 상호작용가능 객체들의 서브그룹으로부터 상호작용가능 타겟 객체의 선택을 수신하는 단계; 및상기 상호작용가능 타겟 객체에 대한 선택 이벤트를 개시하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 상호작용가능 객체들의 그룹을 데이터 구조에 저장하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 상호작용가능 객체들의 그룹 내의 각 상호작용가능 객체는 상기 사용자의 FOV 내의 상기 상호작용가능 객체의 위치에 적어도 부분적으로 기반하여 상기 데이터 구조에서 표현되는, 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서, 상기 위치는 상기 사용자의 FOV의 에지로부터의 거리를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서, 상기 상호작용가능 객체들의 서브그룹으로부터 상기 상호작용가능 타겟 객체의 선택을 수신하는 단계는: 사용자 디바이스로부터 제1 입력을 수신하는 단계; 및 상기 제1 입력에 대한 응답으로, 상기 상호작용가능 객체들의 서브그룹으로부터 상기 상호작용가능 타겟 객체를 식별하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서, 상기 사용자 디바이스로부터 제2 입력을 수신하는 단계; 및 상기 제2 입력에 대한 응답으로, 상기 상호작용가능 타겟 객체에 대한 상호작용 이벤트를 개시하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서, 상기 상호작용가능 타겟 객체는 상기 사용자의 FOV의 중간점에 가장 가까운, 상기 상호작용가능 객체들의 서브그룹의 상호작용가능 객체인, 방법.</claim></claimInfo><claimInfo><claim>8. 제6항에 있어서, 상기 상호작용가능 타겟 객체는 상기 사용자의 FOV 내의 상호작용가능 객체들의 서브그룹의 최좌측 또는 최우측 상호작용가능 객체인, 방법.</claim></claimInfo><claimInfo><claim>9. 제6항에 있어서, 상기 상호작용 이벤트를 개시하는 단계는: 상기 상호작용가능 타겟 객체를 리사이징하는 단계; 상기 상호작용가능 타겟 객체의 메뉴를 디스플레이하는 단계; 상기 상호작용가능 타겟 객체의 메뉴를 브라우징하는 단계; 상기 메뉴 상의 제1 아이템을 선택하는 단계; 데이터베이스에서 제2 아이템을 검색하는 단계; 상기 상호작용가능 타겟 객체와 연관된 비디오 게임을 플레이하는 단계; 상기 비디오를 보는 단계; 또는 원격 화상회의를 수행하는 단계 중 하나 이상을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서, 상기 상호작용가능 타겟 객체의 선택을 수신하는 단계는: 상기 사용자의 포즈에 기반하여 상기 사용자의 응시 경로를 결정하는 단계; 및 상기 사용자의 응시 경로와 교차하는 객체를 상기 상호작용가능 타겟 객체로 선택하는 단계에 의해 수행되는, 방법.</claim></claimInfo><claimInfo><claim>11. 제1항에 있어서, 상기 상호작용가능 타겟 객체에 가시적 포커스 표시자를 할당하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서, 상기 가시적 포커스 표시자는 상기 상호작용가능 타겟 객체의 하이라이트, 헤일로 분위기, 컬러 변경, 크기 변경, 또는 인지된 깊이의 변경 중 적어도 하나를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>13. 제1항에 있어서, 상기 선택 이벤트를 개시하는 단계는: 상기 상호작용가능 타겟 객체를 상기 상호작용가능 타겟 객체가 되는 상이한 상호작용가능 객체로 변경하는 단계; 상기 상호작용가능 타겟 객체와 연관된 메뉴를 여는 단계; 또는 상기 상호작용가능 타겟 객체를 선택하기 위한 상기 사용자로부터의 확인을 수신하는 단계 중 하나 이상을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>14. 제1항에 있어서, 상기 상호작용가능 객체들의 그룹은 가상 객체들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>15. 제1항에 있어서, 상기 사용자의 포즈는 눈 포즈 또는 머리 포즈를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>16. 제1항에 있어서, 상기 상호작용가능 객체들의 서브그룹으로부터 상기 상호작용가능 타겟 객체의 선택을 수신하는 단계는 눈 제스처 또는 손 제스처를 사용하여 상기 선택을 수신하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>17. 제1항에 있어서, 상기 상호작용가능 객체들의 서브그룹으로부터 상기 상호작용가능 타겟 객체의 선택을 수신하는 단계는 재귀적으로 상기 선택을 수신하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>18. 3차원(3D) 공간에 위치한 가상 객체를 선택하기 위한 증강 현실(AR) 시스템으로서, 상기 시스템은:디스플레이 시스템;네트워크 인터페이스;상기 네트워크 인터페이스 및 상기 디스플레이 시스템과 통신하도록 구성된 컴퓨터 프로세서들을 포함하며, 상기 컴퓨터 프로세서들은:사용자의 FOR(field of regard) 내의 상호작용가능 객체들의 그룹을 결정하며;상기 사용자의 포즈를 결정하며;상기 사용자의 포즈에 적어도 부분적으로 기반하여 상기 사용자의 FOV(field of view)를 결정하며 — 상기 FOV는 주어진 시간에 상기 사용자가 인지하는 FOR의 일부분을 포함함 —;상기 사용자의 포즈 또는 FOV의 변경에 기반하여, 상기 사용자의 FOV에 위치하는 상기 상호작용가능 객체들의 서브그룹을 업데이트하며;상기 상호작용가능 객체들의 서브그룹의 디스플레이를 확대하며;상기 상호작용가능 객체들의 서브그룹으로부터 상호작용가능 타겟 객체의 선택을 수신하며; 그리고상기 상호작용가능 타겟 객체에 대한 선택 이벤트를 개시하도록 구성되는, 시스템.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서, 상기 컴퓨터 프로세서들은 상기 상호작용가능 객체들의 그룹을 데이터 구조에 저장하도록 추가로 구성되는, 시스템.</claim></claimInfo><claimInfo><claim>20. 제19항에 있어서, 상기 상호작용가능 객체들의 그룹 내의 하나 이상의 상호작용가능 객체는 상기 사용자의 FOV 내의 상호작용가능 객체의 위치에 적어도 부분적으로 기반하여 상기 데이터 구조에서 표현되는, 시스템.</claim></claimInfo><claimInfo><claim>21. 제20항에 있어서, 상기 위치는 상기 사용자의 FOV의 에지로부터의 거리를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>22. 제18항에 있어서, 상기 상호작용가능 객체들의 서브그룹으로부터 상기 상호작용가능 타겟 객체의 선택을 수신하는 것은: 사용자 디바이스로부터 제1 입력을 수신하는 것; 및 상기 제1 입력에 대한 응답으로, 상기 상호작용가능 객체들의 서브그룹으로부터 상기 상호작용가능 타겟 객체를 식별하는 것을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>23. 제18항에 있어서, 상기 상호작용가능 타겟 객체는 상기 사용자의 FOV의 중간점에 가장 가까운, 상기 상호작용가능 객체들의 서브그룹의 상호작용가능 객체인, 시스템.</claim></claimInfo><claimInfo><claim>24. 제18항에 있어서, 상기 상호작용가능 타겟 객체는 상기 사용자의 FOV 내의 상기 상호작용가능 객체들의 서브그룹의 최좌측 또는 최우측 상호작용가능 객체인, 시스템.</claim></claimInfo><claimInfo><claim>25. 제18항에 있어서, 상기 상호작용가능 객체들의 서브그룹으로부터 상기 상호작용가능 타겟 객체의 선택을 수신하는 것은: 상기 사용자의 포즈에 기반하여 상기 사용자의 응시 경로를 결정하는 것; 및 상기 사용자의 응시 경로와 교차하는 객체를 상기 상호작용가능 타겟 객체로 선택하는 것을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>26. 제18항에 있어서, 상기 컴퓨터 프로세서들은 상기 상호작용가능 타겟 객체에 가시적 포커스 표시자를 할당하도록 추가로 구성되는, 시스템.</claim></claimInfo><claimInfo><claim>27. 제26항에 있어서, 상기 가시적 포커스 표시자는 상기 상호작용가능 타겟 객체의 하이라이트, 헤일로 분위기, 컬러 변경, 크기 변경, 또는 인지된 깊이의 변경을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>28. 제18항에 있어서,선택 이벤트를 개시하는 것은: 상기 상호작용가능 타겟 객체를 상기 상호작용가능 타겟 객체가 되는 상이한 상호작용가능 객체로 변경하는 것; 상기 상호작용가능 타겟 객체와 연관된 메뉴를 여는 것; 또는 상기 상호작용가능 타겟 객체를 선택하기 위한 사용자로부터의 확인을 수신하는 것 중 하나 이상을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>29. 제18항에 있어서, 상기 상호작용가능 객체들의 그룹은 가상 객체들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>30. 제18항에 있어서, 상기 사용자의 포즈는 눈 포즈 또는 머리 포즈를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>31. 제18항에 있어서, 상기 프로세서들은 눈 제스처 또는 손 제스처를 사용하여 상기 상호작용가능 타겟 객체의 선택을 수신하도록 구성되는, 시스템.</claim></claimInfo><claimInfo><claim>32. 제18항에 있어서, 상기 프로세서들은 상기 상호작용가능 타겟 객체의 선택을 재귀적으로 수신하도록 구성되는, 시스템.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 플로리다 플랜타티온 웨스트 선라이즈 블러바드 **** (우: *****)</address><code>520140046502</code><country>미국</country><engName>MAGIC LEAP, INC.</engName><name>매직 립, 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 플로리다 플...</address><code> </code><country> </country><engName>POWDERLY, James, M.</engName><name>파우덜리, 제임스, 엠.</name></inventorInfo><inventorInfo><address>미국 ***** 플로리다 플...</address><code> </code><country> </country><engName>NILES, Savannah</engName><name>닐스, 사반나</name></inventorInfo><inventorInfo><address>미국 ***** 플로리다 플...</address><code> </code><country> </country><engName>HAMILTON, Frank</engName><name>해밀턴, 프랭크</name></inventorInfo><inventorInfo><address>미국 ***** 플로리다 플...</address><code> </code><country> </country><engName>FONTAINE, Marshal, A.</engName><name>폰테인, 마르샬, 에이.</name></inventorInfo><inventorInfo><address>미국 ***** 플로리다 플...</address><code> </code><country> </country><engName>ABOVITZ, Rony</engName><name>아보비츠, 로니</name></inventorInfo><inventorInfo><address>미국 ***** 플로리다 포트...</address><code> </code><country> </country><engName>NAPLES, Alysha</engName><name>네이플스, 알리샤</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 서소문로 **, *층(서소문동)</address><code>920241000417</code><country>대한민국</country><engName>NAM IP GROUP</engName><name>특허법인(유)남아이피그룹</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2015.10.20</priorityApplicationDate><priorityApplicationNumber>62/244,115</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2016.02.29</priorityApplicationDate><priorityApplicationNumber>62/301,422</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2016.03.31</priorityApplicationDate><priorityApplicationNumber>62/316,179</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Divisional Application(International Application)] Patent Application</documentEngName><documentName>[분할출원(국제출원)]특허출원서</documentName><receiptDate>2024.08.26</receiptDate><receiptNumber>1-1-2024-0929366-05</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247028601.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93738891018caad662a2afb461210e764aad387fdd4f33ce1ce81c1cc4b037f13b45c8a07bd838717a73174ffa9aa5a76048609eb1eac0f83a</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cff7833b8a52f73b6f06ebf49e63f05cf8c16f3b4ddc7d9dceee1d1336452488264510eaa9aae37e6140f9e801e46a16c4b02eb7e0f1c7896c</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>