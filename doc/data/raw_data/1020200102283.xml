<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:25:49.2549</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2020.08.14</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2020-0102283</applicationNumber><claimCount>18</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>로봇 및 이의 제어 방법</inventionTitle><inventionTitleEng>Robot and control method thereof</inventionTitleEng><openDate>2022.02.22</openDate><openNumber>10-2022-0021581</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.08.11</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 19/02</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 로봇 및 로봇의 제어 방법이 개시된다. 본 개시에 따른 로봇의 제어 방법은 사용자를 촬영한 영상을 획득하는 단계, 영상을 분석하여 사용자의 위치 및 사용자의 시선 방향에 대한 제1 정보를 획득하는 단계, 영상의 촬영 위치 및 촬영 방향을 바탕으로, 로봇이 동작하는 환경에 대응되는 지도 상에 제1 정보를 매칭 하기 위한 매칭 정보를 획득하는 단계, 매칭 정보 및 제1 정보를 바탕으로, 지도 상에서의 사용자의 위치 및 시선 방향에 대한 제2 정보를 획득하는 단계 및 지도 상에서 객체를 식별하도록 학습된 인공 지능 모델에 제2 정보를 입력하여, 지도 상에서 사용자의 시선 방향에 대응되는 객체를 식별하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 로봇의 제어 방법에 있어서,사용자(user)를 촬영한 영상을 획득하는 단계;상기 영상을 분석하여 상기 사용자의 위치 및 상기 사용자의 시선 방향에 대한 제1 정보를 획득하는 단계;상기 영상의 촬영 위치 및 촬영 방향을 바탕으로, 상기 로봇이 동작하는 환경에 대응되는 지도 상에 상기 제1 정보를 매칭 하기 위한 매칭 정보를 획득하는 단계;상기 매칭 정보 및 상기 제1 정보를 바탕으로, 상기 지도 상에서의 상기 사용자의 위치 및 상기 시선 방향에 대한 제2 정보를 획득하는 단계; 및상기 지도 상에서 객체를 식별하도록 학습된 인공 지능 모델에 상기 제2 정보를 입력하여, 상기 지도 상에서 상기 사용자의 시선 방향에 대응되는 객체를 식별하는 단계;를 포함하는 제어 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 제1 정보를 식별하는 단계는,상기 영상에서 상기 사용자의 머리(head)에 대응되는 영역을 식별하는 단계;상기 식별된 영역을 바탕으로, 상기 사용자의 위치를 식별하는 단계; 및상기 식별된 영역 내의 머리 자세(pose)를 바탕으로 상기 시선 방향을 식별하는 단계;를 더 포함하는 제어 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 매칭 정보를 획득하는 단계는,SLAM(Simultaneous Localization and Mapping) 방식을 통해 상기 지도 상에서 상기 영상을 촬영할 때의 상기 로봇의 위치 및 방향에 대한 정보를 식별함으로, 상기 매칭 정보를 획득하는 단계인 제어 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 제2 정보를 획득하는 단계는,상기 제2 정보를 상기 지도에 대응되는 그리드 맵 상에 매핑하여 입력 데이터를 획득하는 단계;를 더 포함하고,상기 객체를 식별하는 단계는,상기 입력 데이터를 상기 인공 지능 모델에 입력하여, 상기 그리드 맵 상에서 상기 객체에 대응되는 좌표를 식별하는 단계; 및상기 식별된 좌표를 바탕으로, 상기 지도 상에서 상기 사용자의 시선 방향에 대응되는 객체를 식별하는 단계;를 더 포함하는 것을 특징으로 하는 제어 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 인공 지능 모델은 상기 그리드 맵 상에서의 상기 객체에 대응되는 좌표에 대한 데이터를 출력 데이터로 하며,상기 그리드 맵 상의 일 좌표로부터 상기 객체를 향하는 방향을 바탕으로 상기 그리드 맵 상에 binary 데이터가 매핑된 제1 입력 데이터를 입력 데이터로 하여 상기 인공 지능 모델이 학습되는 것을 특징으로 하는 제어 방법.</claim></claimInfo><claimInfo><claim>6. 제4항에 있어서,상기 인공 지능 모델은 상기 그리드 맵 상에서의 상기 객체에 대응되는 좌표에 대한 데이터를 출력 데이터로 하며,상기 로봇이 동작하는 환경 내에서 상기 객체를 바라보는 사용자를 촬영한 영상을 바탕으로 획득된 제2 입력 데이터를 입력 데이터로 하여 상기 인공 지능 모델이 학습되는 것을 특징으로 하는 제어 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 객체가 식별되면, 상기 객체가 상기 로봇의 카메라의 시야각 내에 포함되도록 회전하는 단계;상기 회전하는 동안, 상기 사용자의 손(hand)이 상기 카메라의 시야각 내에 포함되면, 상기 손에 대응되는 방향 정보를 식별하는 단계;상기 손에 대응되는 방향 정보를 바탕으로, 상기 제2 정보에 포함된 상기 시선 방향에 대한 정보가 업데이트된 제3 정보를 획득하는 단계; 및 상기 제3 정보를 학습된 인공 지능 모델에 적용하여, 상기 지도 상에서 상기 사용자의 시선에 대응되는 객체를 식별하는 단계;를 더 포함하는 제어 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 사용자의 상기 객체에 대응되는 명령이 감지되면, 상기 영상을 획득하여 상기 지도 상에서 상기 사용자의 시선에 대응되는 객체를 식별하는 단계; 및상기 명령에 대응되는 테스크를 수행하는 단계;를 더 포함하는 제어 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 객체는 상기 영상 내에 포함되어 있지 않은 것을 특징으로 하는 제어 방법.</claim></claimInfo><claimInfo><claim>10. 로봇에 있어서,적어도 하나의 인스트럭션을 저장하는 메모리;사용자(user)를 촬영하기 위한 카메라; 및상기 메모리 및 상기 카메라와 연결되며 상기 로봇을 제어하는 프로세서를 포함하고,상기 프로세서는 상기 적어도 하나의 인스트럭션을 실행함으로써,상기 카메라를 통해 사용자를 촬영한 영상을 획득하고,상기 영상을 분석하여 상기 사용자의 위치 및 상기 사용자의 시선 방향에 대한 제1 정보를 획득하고,상기 영상의 촬영 위치 및 촬영 방향을 바탕으로, 상기 로봇이 동작하는 환경에 대응되는 지도 상에 상기 제1 정보를 매칭 하기 위한 매칭 정보를 획득하고,상기 매칭 정보 및 상기 제1 정보를 바탕으로, 상기 지도 상에서의 상기 사용자의 위치 및 상기 시선 방향에 대한 제2 정보를 획득하고,상기 지도 상에서 객체를 식별하도록 학습된 인공 지능 모델에 상기 제2 정보를 입력하여, 상기 지도 상에서 상기 사용자의 시선 방향에 대응되는 객체를 식별하는 로봇.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 프로세서는,상기 영상에서 상기 사용자의 머리(head)에 대응되는 영역을 식별하고,상기 식별된 영역을 바탕으로, 상기 사용자의 위치를 식별하고,상기 식별된 영역 내의 머리 자세(pose)를 바탕으로 상기 시선 방향을 식별하는 로봇.</claim></claimInfo><claimInfo><claim>12. 제10항에 있어서,상기 프로세서는,SLAM(Simultaneous Localization and Mapping) 방식을 통해 상기 지도 상에서 상기 영상을 촬영할 때의 상기 로봇의 위치 및 방향에 대한 정보를 식별함으로, 상기 매칭 정보를 획득하는 것을 특징으로 하는 로봇.</claim></claimInfo><claimInfo><claim>13. 제10항에 있어서,상기 프로세서는,상기 제2 정보를 상기 지도에 대응되는 그리드 맵 상에 매핑하여 입력 데이터를 획득하고, 상기 입력 데이터를 상기 인공 지능 모델에 입력하여, 상기 그리드 맵 상에서 상기 객체에 대응되는 좌표를 식별하고,상기 식별된 좌표를 바탕으로, 상기 지도 상에서 상기 사용자의 시선 방향에 대응되는 객체를 식별하는 것을 특징으로 하는 로봇.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 인공 지능 모델은 상기 그리드 맵 상에서의 상기 객체에 대응되는 좌표에 대한 데이터를 출력 데이터로 하며,상기 그리드 맵 상의 일 좌표로부터 상기 객체를 향하는 방향을 바탕으로 상기 그리드 맵 상에 binary 데이터가 매핑된 제1 입력 데이터를 입력 데이터로 하여 상기 인공 지능 모델이 학습되는 것을 특징으로 하는 로봇.</claim></claimInfo><claimInfo><claim>15. 제13항에 있어서,상기 인공 지능 모델은 상기 그리드 맵 상에서의 상기 객체에 대응되는 좌표에 대한 데이터를 출력 데이터로 하며,상기 로봇이 동작하는 환경 내에서 상기 객체를 바라보는 사용자를 촬영한 영상을 바탕으로 획득된 제2 입력 데이터를 입력 데이터로 하여 상기 인공 지능 모델이 학습되는 것을 특징으로 하는 로봇.</claim></claimInfo><claimInfo><claim>16. 제10항에 있어서,상기 프로세서는,상기 객체가 식별되면, 상기 객체가 상기 로봇의 카메라의 시야각 내에 포함되도록 회전하고,상기 회전하는 동안, 상기 사용자의 손(hand)이 상기 카메라의 시야각 내에 포함되면, 상기 손에 대응되는 방향 정보를 식별하고,상기 손에 대응되는 방향 정보를 바탕으로, 상기 제2 정보에 포함된 상기 시선 방향에 대한 정보가 업데이트된 제3 정보를 획득하고,상기 제3 정보를 학습된 인공 지능 모델에 적용하여, 상기 지도 상에서 상기 사용자의 시선에 대응되는 객체를 식별하는 로봇.</claim></claimInfo><claimInfo><claim>17. 제10항에 있어서,상기 프로세서는,상기 사용자의 상기 객체에 대응되는 명령이 감지되면, 상기 영상을 획득하여 상기 지도 상에서 상기 사용자의 시선에 대응되는 객체를 식별하고,상기 명령에 대응되는 테스크를 수행하는 것을 특징으로 하는 로봇.</claim></claimInfo><claimInfo><claim>18. 제10항에 있어서,상기 객체는 상기 영상 내에 포함되어 있지 않은 것을 특징으로 하는 제어 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>JU, Jae Yong</engName><name>주재용</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울시 서초구 강남대로 *** 신덕빌딩 *층(나우특허법률사무소)</address><code>919980005433</code><country>대한민국</country><engName>Jeong Hong Sik</engName><name>정홍식</name></agentInfo><agentInfo><address>서울시 서초구 강남대로 *** 신덕빌딩 *층(나우특허법률사무소)</address><code>920050001107</code><country>대한민국</country><engName>KIM TAEHUN</engName><name>김태헌</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2020.08.14</receiptDate><receiptNumber>1-1-2020-0857113-01</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2023.08.11</receiptDate><receiptNumber>1-1-2023-0887256-72</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Request for Prior Art Search</documentEngName><documentName>선행기술조사의뢰서</documentName><receiptDate>2025.01.08</receiptDate><receiptNumber>9-1-9999-9999999-89</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Report of Prior Art Search</documentEngName><documentName>선행기술조사보고서</documentName><receiptDate>2025.02.24</receiptDate><receiptNumber>9-6-2025-0075282-75</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for refusal</documentEngName><documentName>의견제출통지서</documentName><receiptDate>2025.05.22</receiptDate><receiptNumber>9-5-2025-0488161-56</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2025.07.22</receiptDate><receiptNumber>1-1-2025-0828496-75</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[거절이유 등 통지에 따른 의견]의견서·답변서·소명서</documentName><receiptDate>2025.07.22</receiptDate><receiptNumber>1-1-2025-0828495-29</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for final refusal</documentEngName><documentName>최후의견제출통지서</documentName><receiptDate>2025.11.13</receiptDate><receiptNumber>9-5-2025-1103418-45</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020200102283.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c935c61c8aee2c2151a89915adfc637dfe942c3e3d5bf0e912b4c06e381cc72cf2556e631b6ae3bca83bab4b4a811a8b6aeba139c1d12b1e4b0</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf0563b1653fd3893589dd05079e64d2f5d0b05ad7c7f954420211c6bc539162608ada39661cb105537941dccf4ae4c70ff58f1a2482469864</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>