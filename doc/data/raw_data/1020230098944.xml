<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:05:47.547</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.07.28</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0098944</applicationNumber><claimCount>23</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>가상 객체를 생성하는 전자 장치 및 그 동작 방법</inventionTitle><inventionTitleEng>AN ELECTRONIC DEVICE FOR GENERATING VIRTUAL OBJECTS AND A METHOD FOR OPERATING THE SAME</inventionTitleEng><openDate>2025.02.04</openDate><openNumber>10-2025-0017920</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G10L 15/26</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2020.01.01)</ipcDate><ipcNumber>G06F 40/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06F 3/0484</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>G06F 3/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/041</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06F 3/04883</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 멀티 모달리티(multi-modality) 정보에 기초하여 가상 객체를 생성하는 전자 장치 및 그 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 전자 장치는 카메라를 통해 현실 공간을 촬영하여 획득한 공간 이미지, 손 이미지로부터 획득한 사용자의 제스처 입력, 마이크로폰을 통해 수신된 사용자의 음성 입력, 및 외부로부터 획득된 2차원 가이드 이미지 중 적어도 하나를 포함하는 멀티 모달리티 정보를 획득하고, 멀티 모달리티 정보를 생성형 인공지능 모델에 입력하여 사용자의 의도에 따른 3차원 가상 객체를 생성할 수 있다. 본 개시의 일 실시예에 따른 전자 장치는 사용자로부터 수신된 수정 입력에 기초하여, 생성된 3차원 가상 객체를 수정할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 이미지를 획득하는 카메라(110); 적어도 하나의 명령어들(instructions)를 저장하는 메모리(140); 및상기 적어도 하나의 명령어들을 실행하는 적어도 하나의 프로세서(130); 를 포함하고, 상기 적어도 하나의 프로세서(130)는, 상기 카메라(110)를 통해 획득된 이미지에 기초하여 현실 공간에 관한 공간 정보를 획득하고, 상기 카메라(110)를 통해 획득된 이미지에 기초하여 사용자 입력을 획득하고, 상기 사용자 입력으로부터 객체 특성 정보를 획득하고, 상기 획득된 공간 정보 및 상기 객체 특성 정보에 기초하여 가상 객체를 생성하기 위한 객체 생성 정보를 획득하고, 공간 및 객체에 관한 정보에 기초하여 3차원 가상 객체를 생성하도록 학습된(trained) 생성형 인공지능 모델에 상기 획득된 객체 생성 정보를 입력하여, 상기 객체 생성 정보에 대한 가상 객체를 생성하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 카메라(110)는 현실 공간을 촬영하여 상기 현실 공간에 관한 공간 이미지를 획득하는 제1 카메라(112)를 포함하고, 상기 적어도 하나의 프로세서(130)는, 상기 제1 카메라(112)를 통해 획득된 상기 공간 이미지로부터 현실 공간의 종류, 카테고리, 컬러, 테마, 및 분위기 중 적어도 하나에 관한 공간 정보를 획득하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>3. 제1 항에 있어서,상기 카메라(110)는  사용자의 손을 촬영하여 손 이미지를 획득하는 제2 카메라(114)를 포함하고, 상기 적어도 하나의 프로세서(130)는, 상기 제2 카메라(114)를 통해 획득된 상기 손 이미지로부터 사용자의 제스처 입력을 인식하고, 상기 인식된 제스처 입력으로부터 객체의 형태, 위치, 크기 중 적어도 하나에 관한 상기 객체 특성 정보를 추출하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>4. 제3 항에 있어서,상기 제2 카메라(114)는 ToF(Time of Flight) 카메라, 스테레오 비전 카메라, 및 라이다 센서(Light Detection and Ranging, LiDAR) 중 적어도 하나를 포함하는 깊이 카메라로 구성되고, 상기 사용자의 손을 촬영하여 깊이 이미지(depth image)를 획득하고, 상기 적어도 하나의 프로세서(130)는, 상기 제2 카메라(114)를 통해 획득된 상기 깊이 이미지로부터 사용자의 제스처 입력을 인식하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>5. 제1 항에 있어서,사용자의 터치 입력을 수신하는 터치스크린;을 더 포함하고, 상기 적어도 하나의 프로세서(130)는, 상기 터치스크린을 통해 수신된 터치 입력으로부터 사용자의 제스처 입력을 인식하고, 상기 인식된 제스처 입력으로부터 객체의 형태, 위치, 크기 중 적어도 하나에 관한 상기 객체 특성 정보를 추출하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>6. 제1 항에 있어서,사용자로부터 음성 입력을 수신하는 마이크로폰(120);을 더 포함하고, 상기 적어도 하나의 프로세서(130)는,상기 마이크로폰(120)을 통해 수신된 음성 입력으로부터 음성 신호를 획득하고, 상기 음성 신호를 텍스트로 변환하고, 자연어 이해 모델(Natural Language Understanding)을 이용하여 상기 텍스트를 분석함으로써 객체의 타입, 형태, 컬러, 및 테마 중 적어도 하나를 포함하는 상기 객체 특성 정보를 추출하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>7. 제1 항 내지 제6 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(130)는,2차원 가이드 이미지(2D guide image)를 획득하고, 상기 2차원 가이드 이미지로부터 객체의 타입, 형태, 컬러, 및 테마 중 적어도 하나를 포함하는 상기 객체 특성 정보를 추출하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>8. 제7 항에 있어서, 상기 적어도 하나의 프로세서(130)는,상기 공간 정보를 벡터 임베딩(vector embedding)하여 제1 특징 데이터(feature data)로 변환하고, 상기 제스처 입력으로부터 획득된 객체 특성 정보를 벡터 임베딩하여 제2 특징 데이터로 변환하고, 상기 음성 입력으로부터 획득된 객체 특성 정보를 벡터 임베딩하여 제3 특징  데이터로 변환하고, 상기 2차원 가이드 이미지로부터 획득된 객체 특성 정보를 벡터 임베딩하여 제4 특징 데이터로 변환하며, 상기 제1 특징 데이터 내지 제4 특징 데이터에 기초하여 상기 객체 생성 정보를 나타내는 특징 데이터를 획득하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>9. 제1 항 내지 제8 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(130)는 사용자 입력에 기초하여 상기 객체 생성 정보를 수정하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>10. 제9 항에 있어서, 상기 적어도 하나의 프로세서(130)는,상기 사용자 입력에 기초하여 상기 공간 정보, 상기 객체 특성 정보, 및 2차원 가이드 이미지로부터 획득된 객체 특성 정보 각각에 할당된 가중치(weight) 값을 조정함으로써, 상기 객체 생성 정보를 수정하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>11. 제9 항에 있어서, 상기 적어도 하나의 프로세서(130)는,상기 사용자 입력에 기초하여 상기 객체의 형태, 크기, 위치, 컬러, 타입, 및 테마 중 적어도 하나를 변경함으로써, 상기 객체 생성 정보를 수정하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>12. 전자 장치(100)가 가상 객체를 생성하는 방법에 있어서, 카메라(110)를 통해 이미지를 획득하는 단계(S210); 상기 카메라(110)를 통해 획득된 이미지에 기초하여 사용자 입력을 획득하는 단계(S220);  상기 획득된 이미지로부터 현실 공간에 관한 공간 정보를 획득하는 단계(S230);상기 획득된 사용자 입력으로부터 객체 특성 정보를 획득하는 단계(S240);  상기 획득된 공간 정보 및 상기 객체 특성 정보에 기초하여 가상 객체를 생성하기 위한 객체 생성 정보를 획득하는 단계(S250); 및 공간 및 객체에 관한 정보에 기초하여 3차원 가상 객체를 생성하도록 학습된(trained) 생성형 인공지능 모델에 상기 획득된 객체 생성 정보를 입력하여, 상기 객체 생성 정보에 대한 가상 객체를 생성하는 단계(S260);를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>13. 제12 항에 있어서,상기 공간 정보를 획득하는 단계(S230)는,현실 공간을 촬영하여 상기 현실 공간에 관한 공간 이미지를 획득하도록 구성되는 제1 카메라(112)를 이용하여 공간 이미지를 획득하는 단계; 및상기 획득된 공간 이미지로부터 현실 공간의 종류, 카테고리, 컬러, 테마, 및 분위기 중 적어도 하나에 관한 공간 정보를 획득하는 단계;를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>14. 제12 항에 있어서,상기 객체 특성 정보를 획득하는 단계(S240)는,사용자의 손을 촬영하여 손 이미지를 획득하도록 구성되는 제2 카메라(114)를 통해 손 이미지를 획득하는 단계; 상기 획득된 손 이미지로부터 사용자의 제스처 입력을 인식하는 단계; 및상기 인식된 제스처 입력으로부터 객체의 형태, 위치, 크기 중 적어도 하나에 관한 상기 객체 특성 정보를 추출하는 단계;를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>15. 제14 항에 있어서,상기 제2 카메라(114)는 ToF(Time of Flight) 카메라, 스테레오 비전 카메라, 및 라이다 센서(Light Detection and Ranging, LiDAR) 중 적어도 하나를 포함하는 깊이 카메라로 구성되고, 상기 손 이미지를 획득하는 단계는, 상기 사용자의 손을 촬영하여 깊이 이미지(depth image)를 획득하고, 상기 사용자의 제스처 입력을 인식하는 단계는, 상기 제2 카메라(114)를 통해 획득된 상기 깊이 이미지로부터 사용자의 제스처 입력을 인식하는, 방법. </claim></claimInfo><claimInfo><claim>16. 제12 항에 있어서,상기 객체 특성 정보를 획득하는 단계(S240)는,터치스크린을 통해 사용자의 터치 입력을 수신하는 단계;상기 터치스크린을 통해 수신된 터치 입력으로부터 사용자의 제스처 입력을 인식하는 단계; 및 상기 인식된 제스처 입력으로부터 객체의 형태, 위치, 크기 중 적어도 하나에 관한 상기 객체 특성 정보를 추출하는 단계;를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>17. 제12 항에 있어서,상기 객체 특성 정보를 획득하는 단계(S240)는,마이크로폰(120)을 통해 사용자의 음성 입력을 수신하는 단계; 상기 수신된 음성 입력으로부터 음성 신호를 획득하는 단계; 상기 음성 신호를 텍스트로 변환하는 단계; 및 자연어 이해 모델(Natural Language Understanding)을 이용하여 상기 텍스트를 분석함으로써 객체의 타입, 형태, 컬러, 및 테마 중 적어도 하나를 포함하는 상기 객체 특성 정보를 획득하는 단계; 를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>18. 제12 항 내지 제17 항 중 어느 하나의 항에 있어서,2차원 가이드 이미지(2D guide image)를 획득하는 단계; 를 더 포함하고, 상기 객체 특성 정보를 획득하는 단계(S240)는, 상기 2차원 가이드 이미지로부터 객체의 타입, 형태, 컬러, 및 테마 중 적어도 하나를 포함하는 상기 객체 특성 정보를 추출하는 단계; 를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>19. 제18 항에 있어서, 상기 객체 생성 정보를 획득하는 단계(S240)는, 상기 공간 정보를 벡터 임베딩(vector embedding)하여 제1 특징 데이터로 변환하는 단계; 상기 제스처 입력으로부터 획득된 객체 특성 정보를 벡터 임베딩하여 제2 특징 데이터로 변환하는 단계;상기 음성 입력으로부터 획득된 객체 특성 정보를 벡터 임베딩하여 제3 특징 데이터로 변환하는 단계; 상기 2차원 가이드 이미지로부터 획득된 객체 특성 정보를 벡터 임베딩하여 제4 특징 데이터로 변환하는 단계; 및 상기 제1 특징 데이터 내지 제4 특징 데이터에 기초하여 상기 객체 생성 정보를 나타내는 특징 데이터를 획득하는 단계;를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>20. 제12 항 내지 제19 항 중 어느 하나의 항에 있어서, 상기 객체 생성 정보를 수정하는 사용자 입력을 수신하는 단계(S810);  및 상기 수신된 사용자 입력에 기초하여 상기 객체 생성 정보를 수정하는 단계(S820); 를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>21. 제20 항에 있어서, 상기 객체 생성 정보를 수정하는 단계(S820)는, 상기 사용자 입력에 기초하여 이미지로부터 획득된 공간 정보, 상기 제스처 입력으로부터 획득된 객체 특성 정보, 상기 음성 입력으로부터 획득된 객체 특성 정보, 및 2차원 가이드 이미지로부터 획득된 객체 특성 정보 각각에 할당된 가중치(weight) 값을 조정함으로써, 상기 객체 생성 정보를 수정하는, 방법. </claim></claimInfo><claimInfo><claim>22. 제20 항에 있어서, 상기 객체 생성 정보를 수정하는 단계(S820)는, 상기 사용자 입력에 기초하여 객체의 형태, 크기, 위치, 컬러, 타입, 및 테마 중 적어도 하나를 변경함으로써, 상기 객체 생성 정보를 수정하는, 방법. </claim></claimInfo><claimInfo><claim>23. 제12 항 내지 제22 항 중 어느 하나의 항에 기재된 방법을 구현하기 위한 적어도 하나의 프로그램이 기록된 컴퓨터로 판독 가능한 기록 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>JI, Seo Won</engName><name>지서원</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>SHIN, Seung Hak</engName><name>신승학</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>JEONG, Jae Yun</engName><name>정재윤</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>YEO, Yoon Jae</engName><name>여윤재</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>WON, Seung Jae</engName><name>원승재</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>JANG, Jae Hyun</engName><name>장재현</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.07.28</receiptDate><receiptNumber>1-1-2023-0836148-65</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230098944.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93369ec08af6388ff698522fc27611c6af13d4855367b80e7dee97d376227c7ca63b0a399734be5746224c8d335493b6cc318cc5e798d998b3</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf14196bb3950514c051683012865e617f34f9660de7df68d0421a08579a6f23b7b1365cb8348f4f2e19fb7f8cb3ec4bc1ac4f5d2cc00a2ad8</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>