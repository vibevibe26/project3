<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:05:02.52</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2021.10.04</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-7040508</applicationNumber><claimCount>21</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>셀프-어텐션 기반 뉴럴 네트워크들을 사용하는 이미지들의 프로세싱</inventionTitle><inventionTitleEng>PROCESSING IMAGES USING SELF-ATTENTION BASED NEURAL NETWORKS</inventionTitleEng><openDate>2023.01.06</openDate><openNumber>10-2023-0004710</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2022.11.18</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2022.11.18</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/084</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/764</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/00</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 셀프-어텐션 기반 뉴럴 네트워크들을 사용하여 이미지들을 프로세싱하기 위한, 방법들, 시스템들, 및 장치가 제공되고, 여기에는 컴퓨터 저장 매체들 상에 인코딩된 컴퓨터 프로그램들이 포함된다. 방법들 중 하나는, 복수의 픽셀들을 포함하는 하나 이상의 이미지들을 획득하는 것과; 하나 이상의 이미지들 중 각각의 이미지에 대해, 이미지의 복수의 이미지 패치들을 결정하는 것과, 여기서 각각의 이미지 패치는 이미지의 픽셀들의 상이한 서브세트를 포함하고; 하나 이상의 이미지들 중 각각의 이미지에 대해, 대응하는 복수의 이미지 패치들을 프로세싱하여 복수의 입력 위치들 각각에서 각각의 입력 요소를 포함하는 입력 시퀀스를 발생시키는 것과, 여기서 복수의 입력 요소들은 각각의 상이한 이미지 패치들에 대응하고; 그리고 뉴럴 네트워크를 사용하여 입력 시퀀스들을 프로세싱하여 하나 이상의 이미지들을 특징짓는 네트워크 출력을 발생시키는 것을 포함하고, 여기서 뉴럴 네트워크는 하나 이상의 셀프-어텐션 뉴럴 네트워크 계층들을 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.04.07</internationOpenDate><internationOpenNumber>WO2022072940</internationOpenNumber><internationalApplicationDate>2021.10.04</internationalApplicationDate><internationalApplicationNumber>PCT/US2021/053424</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 방법으로서, 상기 방법은,복수의 픽셀(pixel)들을 포함하는 하나 이상의 이미지(image)들을 획득하는 것과;상기 하나 이상의 이미지들 중 각각의 이미지에 대해, 상기 이미지의 복수의 이미지 패치(image patch)들을 결정하는 것과, 여기서 각각의 이미지 패치는 상기 이미지의 상기 픽셀들의 상이한 서브세트(subset)를 포함하고;상기 하나 이상의 이미지들 중 각각의 이미지에 대해, 대응하는 복수의 이미지 패치들을 프로세싱하여 복수의 입력 위치들 각각에서 각각의 입력 요소를 포함하는 입력 시퀀스(input sequence)를 발생시키는 것과, 여기서 복수의 상기 입력 요소들은 각각의 상이한 이미지 패치들에 대응하고; 그리고뉴럴 네트워크(neural network)를 사용하여 상기 입력 시퀀스들을 프로세싱하여 상기 하나 이상의 이미지들을 특징짓는 네트워크 출력을 발생시키는 것을 포함하고,상기 뉴럴 네트워크는 하나 이상의 셀프-어텐션 뉴럴 네트워크 계층(self-attention neural network layer)들을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,이미지에 대응하는 상기 복수의 이미지 패치들을 프로세싱하여 입력 시퀀스를 발생시키는 것은, 각각의 이미지 패치에 대해,상기 이미지 패치의 상기 픽셀들을 포함하는 각각의 1-차원 초기 입력 요소를 발생시키는 것과; 그리고상기 각각의 초기 입력 요소를 사용하여 각각의 입력 요소를 발생시키는 것을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,각각의 이미지 패치는 차원(dimensionality) L×W×C를 갖고, 여기서 C는 상기 이미지의 채널(channel)들의 수를 나타내고, 각각의 초기 입력 요소는 차원 1×(L・W・C)를 갖는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>4. 제2항 또는 제3항에 있어서,상기 각각의 초기 입력 요소를 사용하여 각각의 입력 요소를 발생시키는 것은, 제 2 뉴럴 네트워크를 사용하여 상기 초기 입력 요소를 프로세싱하는 것을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 제 2 뉴럴 네트워크는 하나 이상의 완전히-연결된 뉴럴 네트워크 계층(fully-connected neural network layer)들을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5항 중 어느 하나의 항에 있어서,이미지에 대응하는 상기 복수의 이미지 패치들을 프로세싱하여 입력 시퀀스를 발생시키는 것은,상기 복수의 이미지 패치들을 프로세싱하여 각각의 중간 입력 요소들을 발생시키는 것과; 그리고각각의 중간 입력 요소에 대해, 상기 중간 입력 요소를 상기 이미지 내의 대응하는 이미지 패치의 위치를 나타내는 위치 임베딩(positional embedding)과 결합시켜 각각의 입력 요소를 발생시키는 것을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,각각의 위치 임베딩은 정수인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>8. 제6항에 있어서,각각의 위치 임베딩은 머신-학습(machine-learn)되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>9. 제1항 내지 제8항 중 어느 하나의 항에 있어서,상기 입력 시퀀스 내의 특정 입력 요소는 머신-학습된 텐서(machine-learned tensor)인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>10. 제1항 내지 제9항 중 어느 하나의 항에 있어서,상기 뉴럴 네트워크를 사용하여 입력 시퀀스를 프로세싱하여 상기 이미지를 특징짓는 네트워크 출력을 발생시키는 것은,상기 뉴럴 네트워크를 사용하여 상기 입력 시퀀스를 프로세싱하여 상기 입력 시퀀스 내의 각각의 입력 요소에 대한 각각의 출력 요소를 발생시키는 것과; 그리고제 3 뉴럴 네트워크를 사용하여 상기 출력 요소들 중 하나 이상을 프로세싱하여 상기 네트워크 출력을 발생시키는 것을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서,상기 제 3 뉴럴 네트워크는 제 1 타입의 네트워크 출력들을 발생시키도록 구성되고,상기 뉴럴 네트워크는 상기 제 1 타입과는 다른 제 2 타입의 네트워크 출력들을 발생시키도록 제 4 뉴럴 네트워크와 동시에 훈련되었던 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 뉴럴 네트워크의 복수의 네트워크 파라미터(network parameter)들은 상기 제 3 뉴럴 네트워크의 훈련 동안 업데이트되었던 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>13. 제10항 내지 제12항 중 어느 하나의 항에 있어서,상기 제 3 뉴럴 네트워크는 복수-계층 퍼셉트론(multi-layer perceptron)인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>14. 제10항 내지 제13항 중 어느 하나의 항에 있어서,각각의 입력 시퀀스에 대해,상기 입력 시퀀스 내의 특정 입력 요소는 머신-학습된 텐서이고,상기 제 3 뉴럴 네트워크를 사용하여 하나 이상의 출력 요소들을 프로세싱하는 것은, 상기 제 3 뉴럴 네트워크를 사용하여 상기 특정 입력 요소에 대응하는 상기 출력 요소를 프로세싱하여 상기 이미지의 예측(prediction)을 발생시키는 것을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>15. 제1항 내지 제14항 중 어느 하나의 항에 있어서,상기 셀프-어텐션 뉴럴 네트워크 계층들 중 하나 이상은 복수-헤드 셀프-어텐션 뉴럴 네트워크 계층(multi-head self-attention neural network layer)들인 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>16. 제1항 내지 제15항 중 어느 하나의 항에 있어서,상기 뉴럴 네트워크는 하나 이상의 서브네트워크(subnetwork)들의 시퀀스를 포함하고,각각의 서브네트워크는, 상기 복수의 입력 위치들 각각에 대한 각각의 서브네트워크 입력을 수신하는 것과, 그리고 상기 복수의 입력 위치들 각각에 대한 각각의 서브네트워크 출력을 발생시키는 것을 수행하도록 구성되고,각각의 서브네트워크는 셀프-어텐션 뉴럴 네트워크 계층 및 위치-별 피드포워드 뉴럴 네트워크 계층(position-wise feedforward neural network layer)을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,각각의 서브네트워크는 또한, 상기 복수의 입력 위치들 각각에 대한 상기 서브네트워크 입력들에 계층 정규화(layer normalization)를 적용하는 제 1 계층 정규화 계층; 상기 복수의 입력 위치들 각각에 대한 상기 서브네트워크 입력들과 상기 셀프-어텐션 뉴럴 네트워크 계층의 출력을 결합시키는 제 1 잔차 연결 계층(residual connection layer); 상기 제 1 잔차 연결 계층의 출력에 계층 정규화를 적용하는 제 2 계층 정규화 계층; 또는 상기 제 1 잔차 연결 계층의 상기 출력과 상기 위치-별 피드-포워드 뉴럴 네트워크 계층의 출력들을 결합시키는 제 2 잔차 연결 계층중 하나 이상을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>18. 제1항 내지 제17항 중 어느 하나의 항에 있어서,상기 네트워크 출력은 복수의 범주들(categories) 각각에 대응하는 각각의 점수를 포함하는 분류 출력(classification output)을 포함하고, 범주에 대한 점수는 상기 이미지가 상기 범주에 속할 가능성을 표시하고,상기 네트워크 출력은 상기 이미지 내의 각각의 픽셀에 대해 복수의 범주들 각각에 대응하는 각각의 점수를 포함하는 픽셀-레벨 분류 출력(pixel-level classification output)을 포함하고, 여기서 범주에 대한 상기 점수는 상기 픽셀이 상기 범주에 속할 가능성을 표시하고,상기 네트워크 출력은 상기 이미지에서 묘사된 각각의 객체들을 둘러싸는 하나 이상의 경계 상자(bounding box)들에 대한 좌표들을 포함하고, 또는상기 뉴럴 네트워크는 비디오의 비디오 프레임(video frame)들인 복수의 이미지들을 수신하고,상기 네트워크 출력은 상기 비디오 프레임들을 특징짓는 출력을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>19. 제18항에 있어서,상기 비디오 프레임들을 특징짓는 상기 출력은 상기 비디오 프레임들이 특정 행동을 수행하는 사람을 묘사하는지 여부를 특징짓는 출력을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>20. 시스템으로서, 상기 시스템은,하나 이상의 컴퓨터들과, 그리고명령들을 저장하는 하나 이상의 저장 디바이스들을 포함하고,상기 명령들은 상기 하나 이상의 컴퓨터들에 의해 실행될 때 상기 하나 이상의 컴퓨터들로 하여금 청구항 제1항 내지 제19항 중 어느 하나의 항의 방법을 수행하도록 하는 것을 특징으로 하는 시스템.</claim></claimInfo><claimInfo><claim>21. 명령들을 저장하는 하나 이상의 컴퓨터 저장 매체들로서, 상기 명령들은 하나 이상의 컴퓨터들에 의해 실행될 때 상기 하나 이상의 컴퓨터들로 하여금 청구항 제1항 내지 제19항 중 어느 하나의 항의 방법을 수행하도록 하는 것을 특징으로 하는 하나 이상의 컴퓨터 저장 매체들.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 캘리포니아 마운틴 뷰 엠피시어터 파크웨이 **** (우:*****)</address><code>520050013456</code><country>미국</country><engName>Google LLC</engName><name>구글 엘엘씨</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>HOULSBY, Neil Matthew Tinmouth</engName><name>호울스비 네일 매튜 틴마우쓰</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>GELLY, Sylvain</engName><name>젤리 실바인</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>USZKOREIT, Jakob D.</engName><name>유스즈코레이트 야곱 디.</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>ZHAI, Xiaohua</engName><name>자이 샤오화</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>HEIGOLD, Georg</engName><name>헤이골드 게오르그</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>BEYER, Lucas Klaus</engName><name>베이어 루카스 클라우스</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>KOLESNIKOV, Alexander</engName><name>콜레스니코브 알렉산더</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>MINDERER, Matthias Johannes Lorenz</engName><name>민더러 마티아스 요하네스 로렌츠</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>WEISSENBORN, Dirk</engName><name>바이센본 디르크</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>DEGHANI, Mostafa</engName><name>데가니 모스타파</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>DOSOVITSKIY, Alexey</engName><name>도소비츠키 알렉세이</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>UNTERTHINER, Thomas</engName><name>운터씨너 토마스</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 강남대로 *** (논현동) *-*F(박장원특허법률사무소)</address><code>919980002023</code><country>대한민국</country><engName>PARK, Jang Won</engName><name>박장원</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2020.10.02</priorityApplicationDate><priorityApplicationNumber>63/087,135</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2022.11.18</receiptDate><receiptNumber>1-1-2022-1233503-38</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2022.11.23</receiptDate><receiptNumber>1-5-2022-0176218-18</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020227040508.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93a728c779fe4778cfe56162699a0787de5c02eef761c52a85e7e58f05ae115f7dab5f94bef027ac791d1b7c86e00ced5b800ebd2e606a7f0b</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfed68d684f2baa77b7e066638581a5bdf2307c0caff291ef2825bf48796a087c0b1953d9748b02d4cc39ad99ad9c31fccdaadf002314e7473</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>