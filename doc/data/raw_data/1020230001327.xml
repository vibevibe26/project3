<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:06:12.612</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.01.04</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0001327</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>손 관절의 3차원 위치 정보를 획득하는 증강 현실 디바이스 및 그 동작 방법</inventionTitle><inventionTitleEng>AN AUGMENTED REALITY DEVICE FOR OBTAINING  THREE-DIMENSIONAL POSITION INFORMATION OF JOINTS OF  USER'S HAND AND A METHOD FOR OPERATING THE SAME</inventionTitleEng><openDate>2024.07.11</openDate><openNumber>10-2024-0109482</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/09</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 복수의 카메라를 통해 획득된 복수의 이미지로부터 손 관절의 3차원 위치 정보를 획득하는 증강 현실 디바이스 및 그 동작 방법을 개시한다. 본 개시의 일 실시예에 따른 증강 현실 디바이스는 복수의 카메라를 이용하여 사용자의 손을 촬영함으로써 획득된 복수의 이미지로부터 손 관절에 관한 2차원 관절 좌표값을 획득하고, 메모리에 기 저장된 룩 업 테이블(look-up table, LUT)로부터 복수의 카메라의 왜곡 모델 파라미터, 복수의 카메라 간의 위치 관계, 및 획득된 2차원 관절 좌표값에 대응되는 3차원 위치 좌표값을 획득하고, 획득된 3차원 위치 좌표값에 기초하여 손 관절의 3차원 위치 정보를 출력할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 손 관절의 3차원 위치 정보를 획득하는 증강 현실 디바이스(100)에 있어서, 사용자의 손을 촬영하여 이미지를 획득하는 복수의 카메라(112, 114);룩 업 테이블(look-up table, LUT)(200)을 저장하는 메모리(130); 및적어도 하나의 프로세서(120);를 포함하고, 상기 적어도 하나의 프로세서(120)는, 상기 복수의 카메라(112, 114)를 통해 획득된 복수의 이미지로부터 손 관절을 인식하고, 상기 인식된 손 관절의 특징점에 관한 2차원 관절 좌표값을 획득하고,상기 룩 업 테이블(200)로부터 상기 복수의 카메라(112, 114)의 왜곡 모델 파라미터, 상기 복수의 카메라(112, 114) 간의 위치 관계, 및 상기 획득된 2차원 관절 좌표값에 대응되는 3차원 위치 좌표값을 획득하고,상기 획득된 3차원 위치 좌표값에 기초하여 상기 손 관절의 3차원 위치 정보를 출력하고, 상기 룩 업 테이블(200)은, 기 획득된 복수의 2차원 위치 좌표값, 복수의 왜곡 모델 파라미터, 복수의 카메라 위치 관계 파라미터, 및 복수의 3차원 위치 좌표값을 포함하고,상기 복수의 2차원 위치 좌표값은, 상기 복수의 3차원 위치 좌표값에 상기 복수의 왜곡 모델 파라미터, 및 상기 복수의 카메라 위치 관계 파라미터를 적용하는 시뮬레이션을 통해 획득된, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 룩 업 테이블(200)에 포함된 상기 복수의 3차원 위치 좌표값은, 인체의 근골격계의 해부학적 제약에 따른 상체 관절의 가동(可動) 각도 범위 내에서 상기 손 관절의 임의의 3차원 위치를 나타내는 좌표값인, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>3. 제1 항 및 제2 항 중 어느 하나의 항에 있어서,상기 룩 업 테이블(200)에 포함된 상기 복수의 2차원 위치 좌표값은,상기 복수의 3차원 위치 좌표값을 상기 복수의 카메라 위치 관계 파라미터에 기초하여 투영(projection)함으로써 복수의 2차원 프로젝션 좌표값을 획득하고, 상기 획득된 복수의 2차원 프로젝션 좌표값에 상기 복수의 왜곡 모델 파라미터를 적용하여 렌즈의 왜곡을 반영한 시뮬레이션을 통해 획득된, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>4. 제1 항 내지 제3 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(120)는, 상기 룩 업 테이블(200)에 액세스(access)하여, 상기 룩 업 테이블(200)로부터 상기 렌즈의 왜곡 모델 파라미터, 상기 복수의 카메라 간의 위치 관계, 및 상기 획득된 2차원 관절 좌표값과 동일 또는 유사한 왜곡 모델 파라미터, 카메라 위치 관계 파라미터, 및 2차원 위치 좌표값을 검색(search)하고, 검색된 왜곡 모델 파라미터, 카메라 위치 관계 파라미터, 및 2차원 위치 좌표값에 대응되는 상기 3차원 위치 좌표값을 상기 룩 업 테이블(200)로부터 획득하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>5. 제1 항 내지 제3 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(120)는,상기 룩 업 테이블(200)을 이용하여 학습된(trained) 인공지능 모델에 상기 렌즈의 왜곡 모델 파라미터, 상기 복수의 카메라 간의 위치 관계, 및 상기 획득된 2차원 관절 좌표값을 입력하고, 상기 인공지능 모델을 이용하는 추론을 통해 상기 3차원 위치 좌표값을 획득하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>6. 제5 항에 있어서,상기 인공지능 모델은, 상기 룩 업 테이블(200)에 포함된 상기 복수의 왜곡 모델 파라미터, 상기 복수의 카메라 위치 관계 파라미터, 및 상기 복수의 2차원 위치 좌표값을 입력 데이터로 적용하고, 상기 복수의 3차원 위치 좌표값을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning)을 통해 학습된 심층 신경망 모델(deep neural network)인, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>7. 제1 항 내지 제6 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(120)는,상기 렌즈의 왜곡 모델 파라미터 및 상기 복수의 카메라 간의 위치 관계에 기초하여 상기 2차원 관절 좌표값의 왜곡을 보정하고, 상기 복수의 이미지의 방향에 관한 조정(rectification)을 수행하고, 상기 왜곡 보정 및 조정 결과 보정된 2차원 관절 좌표값 및 상기 복수의 카메라 간의 위치 관계를 이용하여 삼각측량법(triangulation)을 통해 상기 손 관절의 제1 3차원 위치 좌표값을 산출하고,상기 산출된 제1 3차원 위치 좌표값과 상기 룩 업 테이블(200)로부터 획득된 제2 3차원 위치 좌표값을 비교하여, 상기 손 관절의 3차원 위치 정보의 오류(error)를 검출하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>8. 제7 항에 있어서,상기 적어도 하나의 프로세서(120)는,상기 제1 3차원 위치 좌표값과 상기 제2 3차원 위치 좌표값 간의 차이값, 상기 제1 3차원 위치 좌표값을 통해 산출된 손 관절의 제1 길이와 상기 제2 3차원 위치 좌표값을 통해 산출된 손 관절의 제2 길이 간의 차이값, 및 상기 제1 3차원 위치 좌표값의 절대값 중 적어도 하나에 기초하여, 상기 손 관절의 3차원 위치 인식의 오류 여부를 검출하는, 증강 현실 디바이스(100).  </claim></claimInfo><claimInfo><claim>9. 제7 항 및 제8 항 어느 하나의 항에 있어서,디스플레이부(150); 를 더 포함하고, 상기 적어도 하나의 프로세서(120)는, 오류가 검출된 손 관절의 위치를 오류가 검출되지 않는 위치와 구별되는 컬러로 디스플레이하도록 상기 디스플레이부(150)를 제어하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>10. 제7 항 내지 제9 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(120)는,상기 복수의 이미지의 전체 영역 중 삼각측량법을 통해 상기 제1 3차원 위치 좌표값이 획득되는 제1 영역과 상기 룩 업 테이블(200)로부터 상기 제2 3차원 위치 좌표값이 획득되는 제2 영역을 구별되는 컬러로 디스플레이하도록 상기 디스플레이부(150)를 제어하고, 상기 제1 영역의 크기를 조절하는 사용자의 손의 움직임을 감지하고,상기 감지된 사용자의 손의 움직임에 기초하여 상기 제1 영역의 가로축 및 세로축의 크기를 변경하는, 증강 현실 디바이스(100). </claim></claimInfo><claimInfo><claim>11. 증강 현실 디바이스(100)가 손 관절의 3차원 위치 정보를 획득하는 방법에 있어서, 복수의 카메라(112, 114)를 이용하여 사용자의 손을 촬영함으로써 획득된 복수의 이미지로부터 손 관절을 인식하는 단계(S210);상기 인식된 손 관절의 특징점에 관한 2차원 관절 좌표값을 획득하는 단계(S220); 메모리(130)에 기 저장된 룩 업 테이블(look-up table, LUT)(200)로부터 상기 복수의 카메라의 왜곡 모델 파라미터, 상기 복수의 카메라 간의 위치 관계, 및 상기 획득된 2차원 관절 좌표값에 대응되는 3차원 위치 좌표값을 획득하는 단계(S230); 및상기 획득된 3차원 위치 좌표값에 기초하여 상기 손 관절의 3차원 위치 정보를 출력하는 단계(S240); 를 포함하고, 상기 룩 업 테이블(200)은, 기 획득된 복수의 2차원 위치 좌표값, 복수의 왜곡 모델 파라미터, 복수의 카메라 위치 관계 파라미터, 및 복수의 3차원 위치 좌표값을 포함하고,상기 복수의 2차원 위치 좌표값은, 상기 복수의 3차원 위치 좌표값에 상기 복수의 왜곡 모델 파라미터, 및 상기 복수의 카메라 위치 관계 파라미터를 적용하는 시뮬레이션을 통해 획득된, 방법. </claim></claimInfo><claimInfo><claim>12. 제11 항에 있어서,상기 룩 업 테이블(200)에 포함된 상기 복수의 3차원 위치 좌표값은, 인체의 근골격계의 해부학적 제약에 따른 상체 관절의 가동(可動) 각도 범위 내에서 상기 손 관절의 임의의 3차원 위치를 나타내는 좌표값인, 방법. </claim></claimInfo><claimInfo><claim>13. 제11 항 및 제12 항 중 어느 하나의 항에 있어서,상기 룩 업 테이블(200)에 포함된 상기 복수의 2차원 위치 좌표값은,상기 복수의 3차원 위치 좌표값을 상기 복수의 카메라 위치 관계 파라미터에 기초하여 투영(projection)함으로써 복수의 2차원 프로젝션 좌표값을 획득하고, 상기 획득된 복수의 2차원 프로젝션 좌표값에 상기 복수의 왜곡 모델 파라미터를 적용하여 렌즈의 왜곡을 반영한 시뮬레이션을 통해 획득된, 방법. </claim></claimInfo><claimInfo><claim>14. 제11 항 내지 제13 항 중 어느 하나의 항에 있어서,상기 3차원 위치 좌표값을 획득하는 단계(S230)는, 상기 룩 업 테이블(200)에 액세스(access)하여, 상기 룩 업 테이블(200)로부터 상기 렌즈의 왜곡 모델 파라미터, 상기 복수의 카메라 간의 위치 관계, 및 상기 획득된 2차원 관절 좌표값과 동일 또는 유사한 왜곡 모델 파라미터, 카메라 위치 관계 파라미터, 및 2차원 위치 좌표값을 검색(search)하는 단계; 및 검색된 왜곡 모델 파라미터, 카메라 위치 관계 파라미터, 및 2차원 위치 좌표값에 대응되는 상기 3차원 위치 좌표값을 상기 룩 업 테이블(200)로부터 획득하는 단계;를 포함하는, 방법.  </claim></claimInfo><claimInfo><claim>15. 제11 항 내지 제13 항 중 어느 하나의 항에 있어서,상기 3차원 위치 좌표값을 획득하는 단계(S230)는, 상기 룩 업 테이블(200)을 이용하여 학습된(trained) 인공지능 모델에 상기 렌즈의 왜곡 모델 파라미터, 상기 복수의 카메라 간의 위치 관계, 및 상기 획득된 2차원 관절 좌표값을 입력하고, 상기 인공지능 모델을 이용하는 추론을 통해 상기 3차원 위치 좌표값을 획득하는 단계;를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>16. 제15 항에 있어서,상기 인공지능 모델은, 상기 룩 업 테이블(200)에 포함된 상기 복수의 왜곡 모델 파라미터, 상기 복수의 카메라 위치 관계 파라미터, 및 상기 복수의 2차원 위치 좌표값을 입력 데이터로 적용하고, 상기 복수의 3차원 위치 좌표값을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning)을 통해 학습된 심층 신경망 모델(deep neural network)인, 방법.</claim></claimInfo><claimInfo><claim>17. 제11 항 내지 제16 항 중 어느 하나의 항에 있어서,상기 렌즈의 왜곡 모델 파라미터 및 상기 복수의 카메라 간의 위치 관계에 기초하여 상기 2차원 관절 좌표값의 왜곡을 보정하고, 상기 복수의 이미지의 방향에 관한 조정(rectification)을 수행하는 단계(S1010);상기 왜곡 보정 및 조정 결과 보정된 2차원 관절 좌표값 및 상기 복수의 카메라 간의 위치 관계를 이용하여 삼각측량법(triangulation)을 통해 상기 손 관절의 제1 3차원 위치 좌표값을 산출하는 단계(S1020); 및상기 산출된 제1 3차원 위치 좌표값과 상기 룩 업 테이블(200)로부터 획득된 제2 3차원 위치 좌표값을 비교하여, 상기 손 관절의 3차원 위치 정보의 오류(error)를 검출하는 단계(S1030);를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>18. 제17 항에 있어서,오류가 검출된 손 관절의 위치를 오류가 검출되지 않는 위치와 구별되는 컬러로 디스플레이하는 단계; 를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>19. 제17 항 및 제18 항 중 어느 하나의 항에 있어서,상기 복수의 이미지의 전체 영역 중 삼각측량법을 통해 상기 제1 3차원 위치 좌표값이 획득되는 제1 영역과 상기 룩 업 테이블(200)로부터 상기 제2 3차원 위치 좌표값이 획득되는 제2 영역을 구별되는 컬러로 디스플레이하는 단계(S1320);상기 제1 영역의 크기를 조절하는 사용자의 손의 움직임을 감지하는 단계(S1340); 및상기 감지된 사용자의 손의 움직임에 기초하여 상기 제1 영역의 가로축 및 세로축의 크기를 변경하는 단계(S1350);를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>20. 제11 항 내지 제19 항 중 어느 하나의 항에 기재된 방법을 구현하기 위한 적어도 하나의 프로그램이 기록된 컴퓨터로 판독 가능한 기록 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>PARK, Hwang Pil</engName><name>박황필</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>KIM, Deok Ho</engName><name>김덕호</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>KIM, Woo Jae</engName><name>김우재</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>LEE, Gun Ill</engName><name>이건일</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>LEE, Won Woo</engName><name>이원우</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>JEONG, Ji Won</engName><name>정지원</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.01.04</receiptDate><receiptNumber>1-1-2023-0014580-18</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230001327.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c933714dfbb7d15b00697983b8c79694d9b3eee1e9779be50fb5545033cc9c840de786019b92bdf7cff532ba98bf95b145c281e4d5126438595</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfd90d0542664eb5406f690fb0416c6babbc5ebf4879c1f70c1f60b60142a70fe0dbf07fda62f0e6539a950de2c92e1f1319fa8d8754468076</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>