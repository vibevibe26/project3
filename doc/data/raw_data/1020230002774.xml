<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:53:38.5338</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.01.09</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0002774</applicationNumber><claimCount>39</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>오토 포커스를 수행하는 방법 및 장치</inventionTitle><inventionTitleEng>METHOD AND APPARATUS FOR PERFORMING AUTOFOCUS</inventionTitleEng><openDate>2024.05.08</openDate><openNumber>10-2024-0061539</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>H04N 23/67</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>H04N 23/54</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>H04N 23/55</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/02</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 오토 포커스를 수행하는 방법 및 장치가 개시된다. 일 실시예에 따른 인공 신경망 모델을 이용하여 오토 포커스를 수행하는 방법은 입력 이미지의 위상 정보를 포함하는 제1 입력 데이터를 획득하는 단계; 렌즈 위치 정보가 인코딩된 단일 채널의 제2 입력 데이터를 획득하는 단계; 및 상기 제1 입력 데이터 및 상기 제2 입력 데이터를 상기 인공 신경망 모델에 입력하여, 상기 오토 포커스에 대응하는 렌즈의 위치 정보를 획득하는 단계를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 인공 신경망 모델을 이용하여 오토 포커스(AF: Autofocus)를 수행하는 방법에 있어서,입력 이미지의 위상 정보를 포함하는 제1 입력 데이터를 획득하는 단계;렌즈 위치 정보가 인코딩된 제2 입력 데이터를 획득하는 단계; 및상기 제1 입력 데이터 및 상기 제2 입력 데이터를 상기 인공 신경망 모델에 입력하여, 상기 오토 포커스에 대응하는 렌즈의 위치 정보를 획득하는 단계를 포함하는 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 입력 이미지의 관심 영역에 대응하는 위치 정보가 인코딩된 제3 입력 데이터를 획득하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 제3 입력 데이터를 획득하는 단계는상기 제1 입력 데이터에서 상기 관심 영역의 상대적인 위치를 제1축 데이터 및 제2축 데이터로 인코딩하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서,상기 제3 입력 데이터를 획득하는 단계는렌즈를 통해 이미지 센서로 들어오는 빛의 세기 정보에 기초하여, 상기 관심 영역의 위치 정보를 인코딩하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>5. 제2항에 있어서,상기 제3 입력 데이터를 획득하는 단계는가우시안 필터(Gaussian filter)에 기초하여, 상기 관심 영역의 위치 정보를 인코딩하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>6. 제2항에 있어서,상기 렌즈의 위치 정보를 획득하는 단계는상기 제3 입력 데이터를 상기 제1 입력 데이터 및 상기 제2 입력 데이터와 함께 상기 인공 신경망 모델에 입력하여, 상기 렌즈의 위치 정보를 획득하는 단계를 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 제2 입력 데이터를 획득하는 단계는상기 렌즈의 위치가 연속적인 수치로 표현되도록 상기 렌즈 위치 정보를 인코딩하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 제2 입력 데이터를 획득하는 단계는상기 렌즈의 위치에 따른 초점거리에 맞춰진 비선형 함수값에 기초하여, 상기 렌즈 위치 정보를 인코딩하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 제2 입력 데이터를 획득하는 단계는상기 렌즈 위치 정보를 단일 채널 형태로 인코딩하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 제2 입력 데이터를 획득하는 단계는상기 렌즈 위치 정보를 스케일링 팩터(scaling factor)형태로 인코딩하는 단계를 더 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>11. 제1항에 있어서,상기 렌즈의 위치 정보를 획득하는 단계는미리 정해진 복수의 후보 위치들 중 상기 오토 포커스에 대응하는 하나의 위치를 예측하는 단계를 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>12. 제1항에 있어서,상기 렌즈의 위치 정보를 획득하는 단계는상기 오토 포커스에 대응하는 하나의 스칼라 값을 획득하는 단계를 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>13. 제1항에 있어서,상기 제1 입력 데이터는듀얼 픽셀(dual-pixel) 이미지를 포함하는, 오토 포커스를 수행하는 방법.</claim></claimInfo><claimInfo><claim>14. 하드웨어와 결합되어 제1항 내지 제13항 중 어느 하나의 항의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>15. 인공 신경망 모델을 이용하여 오토 포커스(AF: Autofocus)를 수행하는 전자 장치에 있어서,적어도 하나의 명령어를 저장하는 메모리; 및상기 메모리에 저장된 명령어를 실행함으로써,입력 이미지의 위상 정보를 포함하는 제1 입력 데이터를 획득하고,렌즈 위치 정보가 인코딩된 제2 입력 데이터를 획득하고,상기 제1 입력 데이터 및 상기 제2 입력 데이터를 상기 인공 신경망 모델에 입력하여, 상기 오토 포커스에 대응하는 렌즈의 위치 정보를 획득하는 프로세서를 포함하는 전자 장치.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 프로세서는상기 입력 이미지의 관심 영역에 대응하는 위치 정보가 인코딩된 제3 입력 데이터를 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 프로세서는상기 제1 입력 데이터에서 상기 관심 영역의 상대적인 위치를 제1축 데이터 및 제2축 데이터로 인코딩하는, 전자 장치.</claim></claimInfo><claimInfo><claim>18. 제16항에 있어서,상기 프로세서는상기 제3 입력 데이터를 상기 제1 입력 데이터 및 상기 제2 입력 데이터와 함께 상기 인공 신경망 모델에 입력하여, 상기 렌즈의 위치 정보를 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>19. 제15항에 있어서,상기 프로세서는상기 렌즈의 위치가 연속적인 수치로 표현되도록 상기 렌즈 위치 정보를 인코딩하는, 전자 장치.</claim></claimInfo><claimInfo><claim>20. 제15항에 있어서,상기 프로세서는상기 렌즈의 위치에 따른 초점거리에 맞춰진 비선형 함수값에 기초하여, 상기 렌즈 위치 정보를 인코딩하는, 전자 장치.</claim></claimInfo><claimInfo><claim>21. 제15항에 있어서,상기 프로세서는미리 정해진 복수의 후보 위치들 중 상기 오토 포커스에 대응하는 하나의 위치를 예측하는, 전자 장치.</claim></claimInfo><claimInfo><claim>22. 제15항에 있어서,상기 프로세서는상기 오토 포커스에 대응하는 하나의 스칼라 값을 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>23. 제15항에 있어서,상기 제1 입력 데이터는듀얼 픽셀(dual-pixel) 이미지를 포함하는, 전자 장치.</claim></claimInfo><claimInfo><claim>24. 이동 가능한 렌즈;상기 렌즈의 현재 위치에 의해 형성된 광학 상에 대응하는 영상 신호의 위상 정보를 획득하는 이미지 센서;적어도 하나의 명령어를 저장하는 메모리; 및상기 메모리에 저장된 명령어를 실행함으로써,상기 영상 신호 내 관심 영역의 위치 정보를 인코딩하고,상기 위상 정보 및 상기 인코딩된 상기 관심 영역의 위치 정보에 기초하여, 오토 포커스를 수행하는 프로세서를 포함하는, 전자 장치.</claim></claimInfo><claimInfo><claim>25. 제24항에 있어서,상기 프로세서는상기 렌즈를 통해 상기 이미지 센서로 들어오는 빛의 세기 정보에 기초하여, 상기 관심 영역의 위치 정보를 인코딩하는, 전자 장치.</claim></claimInfo><claimInfo><claim>26. 제24항에 있어서,상기 프로세서는가우시안 필터(Gaussian filter)에 기초하여, 상기 관심 영역의 위치 정보를 인코딩하는, 전자 장치.</claim></claimInfo><claimInfo><claim>27. 서로 이격된 제1 픽셀 및 제2 픽셀 쌍들을 포함하는 이미지 센서 및 상기 이미지 센서와 이격되고 이동 가능한 렌즈를 포함하는 전자 장치의 오토 포커스(AF: Auto Focus) 수행 방법에 있어서,상기 렌즈의 현재 위치를 통과하여 상기 제1 픽셀들에 맺힌 제1 이미지 및 상기 제2 픽셀들에 맺힌 제2 이미지를 획득하는 단계;상기 제1 이미지 및 상기 제2 이미지의 일부에 해당하는 관심 영역을 획득하는 단계;상기 관심 영역에 해당하는 상기 제1 이미지, 상기 관심 영역에 해당하는 상기 제2 이미지, 상기 렌즈의 현재 위치 및 상기 관심 영역의 위치를 인공 신경망 모델에 입력하는 단계;상기 인공 신경망 모델로부터 상기 렌즈의 포커스가 맞는 위치를 출력받는 단계; 및상기 렌즈를 상기 포커스가 맞는 위치로 이동시키는 단계를 포함하는 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>28. 제27항에 있어서,상기 이미지 센서의 모든 픽셀들은 상기 제1 픽셀 및 제2 픽셀 쌍들로 이루어진, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>29. 제27항에 있어서,상기 이미지 센서의 일부 픽셀들만 상기 제1 픽셀 및 제2 픽셀 쌍들로 이루어진, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>30. 제27항에 있어서,상기 관심 영역은사용자로부터 입력 받은 영역인, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>31. 제27항에 있어서,상기 관심 영역은특정 피사체에 대응하는 영역인, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>32. 제31항에 있어서,상기 특정 피사체는사람 얼굴인, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>33. 제27항에 있어서,상기 관심 영역은상기 제1 이미지 및 상기 제2 이미지 내부의 미리 정해진 영역인, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>34. 제33항에 있어서,상기 관심 영역은상기 제1 이미지 및 상기 제2 이미지의 중앙 영역인, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>35. 제27항에 있어서,상기 관심 영역의 x축 데이터와 y축 데이터가 개별적으로 상기 인공 신경망 모델에 입력되는, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>36. 제27항에 있어서,상기 렌즈의 현재 위치는 단일 채널로 상기 인공 신경망 모델에 입력되는, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>37. 제27항에 있어서,상기 렌즈의 현재 위치는 스칼라 값으로 상기 인공 신경망 모델에 반영되는, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>38. 제27항에 있어서,상기 렌즈의 포커스가 맞는 위치는 상기 렌즈의 가동 범위 내의 100개 이상의 연속적인 위치 중 하나로 출력되는, 오토 포커스 수행 방법.</claim></claimInfo><claimInfo><claim>39. 제27항에 있어서,상기 렌즈의 포커스가 맞는 위치는 상기 렌즈의 가동 범위 내의 1000개 이상의 연속적인 위치 중 하나로 출력되는, 오토 포커스 수행 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>서울특별시 성동구...</address><code>420230015102</code><country>대한민국</country><engName>CHOI, Myungsub</engName><name>최명섭</name></inventorInfo><inventorInfo><address>경기도 수원시 권선구...</address><code>420180507481</code><country>대한민국</country><engName>LEE, Hana</engName><name>이한아</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code>420180786048</code><country>대한민국</country><engName>LEE, Hyong Euk</engName><name>이형욱</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.10.31</priorityApplicationDate><priorityApplicationNumber>1020220142661</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.01.09</receiptDate><receiptNumber>1-1-2023-0028136-32</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230002774.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93a4f079e4cfcbe8d250c5386a4e24d2eed0bf327c34504e3c61027049efd696d991c63d7021653e48d4617b02555a6a26d5dedd23cf74f6ee</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf7d5da1c645db27287f544e332b7775cd9ea9cdde350a4a3c4fb2dd7062ac0aead5697eb86e4a79aeda9b133d722bc66101da117d0df8b836</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>