<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:56:00.560</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.07.01</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-7029827</applicationNumber><claimCount>34</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>주어진 음성 발화를 제공한 사용자를 기반으로 주어진 음성 발화 이행의 동적 적응</inventionTitle><inventionTitleEng>DYNAMICALLY ADAPTING FULFILLMENT OF A GIVEN SPOKEN UTTERANCE BASED ON A USER THAT PROVIDED THE GIVEN SPOKEN UTTERANCE</inventionTitleEng><openDate>2024.09.26</openDate><openNumber>10-2024-0141320</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.09.04</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.09.04</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>G06F 3/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B60R 16/037</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G10L 17/06</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G10L 17/22</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 명세서에 설명된 구현은 음성 발화를 제공한 사용자에 기초하여 음성 발화를 이행하는 방법을 결정하는 것에 관한 것이다. 예를 들어, 구현은 사용자로부터 음성 발화를 수신하고, 음성 발화에 대한 이행 액션의 세트를 결정하고, 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정할 수 있다. 또한, 사용자가 제1 사용자에 대응한다고 결정하는 것에 응답하여, 구현예는 세트로부터 제1 이행 액션(들)의 서브세트를 선택하고, 제1 이행 액션(들)의 서브세트가 음성 발화를 만족시키도록 구현되게 할 수 있다. 또한, 사용자가 제2 사용자에 대응한다고 결정하는 것에 응답하여, 구현예는 세트로부터 별개의 제2 이행 액션(들)의 서브세트를 선택하고, 제2 이행 액션(들)의 서브세트가 음성 발화를 만족시키도록 구현되게 할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2023.09.14</internationOpenDate><internationOpenNumber>WO2023172282</internationOpenNumber><internationalApplicationDate>2022.07.01</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/035962</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 하나 이상의 프로세서에 의해 구현되는 방법으로서,사용자로부터 컴퓨팅 디바이스를 통해, 주어진 음성 발화를 수신하는 단계 - 상기 주어진 음성 발화는 사용자가 사용자의 차량 내에 위치하는 동안 제공됨 -; 상기 주어진 음성 발화를 프로세싱하는 것에 기초하여, 이행 액션 세트를 결정하는 단계 - 상기 이행 액션 세트에 포함된 각 이행 액션은 구현 시 상기 주어진 음성 발화를 만족시키는 것을 촉진함 -;상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하는 단계; 및상기 주어진 음성 발화를 제공한 사용자가 제1 사용자에 대응한다고 결정하는 것에 응답하여: 제1 사용자에 대응하는 상기 주어진 음성 발화를 제공한 사용자에 기초하여, 상기 이행 액션 세트로부터 하나 이상의 제1 이행 액션의 서브세트를 선택하는 단계; 및  상기 하나 이상의 제1 이행 액션의 서브세트가 상기 주어진 음성 발화를 만족시키도록 구현되게 하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 주어진 음성 발화를 제공한 사용자가 제2 사용자에 대응한다고 결정하는 것에 응답하여:제2 사용자에 대응하는 상기 주어진 음성 발화를 제공한 사용자에 기초하여, 상기 이행 액션 세트로부터 하나 이상의 제2 이행 액션의 서브세트를 선택하는 단계 - 상기 제2 사용자는 제1 사용자 이외에 추가 사용자이고, 그리고상기 하나 이상의 제2 이행 액션의 서브세트는 하나 이상의 제1 이행 액션의 서브세트에 포함되지 않은 적어도 하나의 이행 액션을 포함함 -; 그리고 상기 하나 이상의 제2 이행 액션들이 상기 주어진 음성 발화를 만족시키도록 구현되게 하는 단계를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 제1 사용자 및 제2 사용자는 주어진 음성 발화가 수신될 때 차량에 함께 위치하고, 상기 하나 이상의 제1 이행 액션의 서브세트가 상기 주어진 음성 발화를 만족시키도록 구현되게 하는 단계는 차량의 복수의 구역 중 제1 사용자에 의해 점유되는 차량의 제1 구역에 대해 하나 이상의 제1 이행 액션을 구현하는 단계를 포함하고, 상기 하나 이상의 제2 이행 액션의 서브세트가 상기 주어진 음성 발화를 만족시키도록 구현되게 하는 단계는 차량의 복수의 구역 중 제2 사용자에 의해 점유되는 차량의 제2 구역에 대해 상기 하나 이상의 제2 이행 액션을 구현하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>4. 제2항 또는 제3항에 있어서, 상기 제1 사용자에 대응하는 상기 주어진 음성 발화를 제공한 사용자에 기초하여 상기 이행 액션 세트로부터 하나 이상의 제1 이행 액션들의 서브세트를 선택하는 단계는,제1 사용자에 특정된 제1 사용자 이행 액션 모델 및/또는 제1 사용자에 특정된 하나 이상의 제1 사용자 이행 규칙을 사용하여, 상기 이행 액션 세트를 프로세싱하여 하나 이상의 제1 이행 액션의 서브세트를 선택하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>5. 제4항에 있어서, 상기 제2 사용자에 대응하는 상기 주어진 음성 발화를 제공한 사용자에 기초하여 상기 이행 액션 세트로부터 하나 이상의 제2 이행 액션들의 서브세트를 선택하는 단계는,제2 사용자에 특정된 제2 사용자 이행 액션 모델 및/또는 제2 사용자에 특정된 하나 이상의 제2 사용자 이행 규칙을 사용하여, 상기 이행 액션 세트를 프로세싱하여 하나 이상의 제2 이행 액션의 서브세트를 선택하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5항 중 어느 한 항에 있어서,상기 하나 이상의 제1 이행 액션의 서브세트가 상기 주어진 음성 발화를 만족시키도록 구현되게 하는 단계에 후속하여: 상기 주어진 음성 발화를 만족시키기 위해 상기 하나 이상의 제1 이행 액션이 구현된 이유에 대한 표시가 제1 사용자에게 제시하기 위해 제공되게 하는 단계를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>7. 제1항 내지 제6항 중 어느 한 항에 있어서, 상기 주어진 음성 발화를 프로세싱하는 단계는,자동 음식 인식(ASR) 모델을 사용하여, 상기 주어진 음성 발화에 대한 ASR 데이터를 생성하기 위해 상기 주어진 음성 발화를 캡처하는 오디오 데이터를 프로세싱하는 단계; 및자연어 이해(NLU) 모델을 사용하여, 상기 주어진 음성 발화에 대한 NLU 데이터를 생성하기 위해 상기 주어진 음성 발화에 대한 ASR 데이터를 프로세싱하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>8. 제7항에 있어서, 상기 주어진 음성 발화를 프로세싱하는 것에 기초하여 이행 액션 세트를 결정하는 단계는,상기 주어진 음성 발화에 대한 NLU 데이터에 기초하여, 이행 액션 세트를 결정하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>9. 제1항 내지 제8항 중 어느 한 항에 있어서, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하는 단계는,화자 식별(SID) 모델을 사용하여, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하기 위해 상기 주어진 음성 발화를 캡처하는 오디오 데이터를 프로세싱하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>10. 제9항에 있어서, 상기 SID 모델을 사용하여, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하기 위해 상기 주어진 음성 발화를 캡처하는 오디오 데이터를 프로세싱하는 단계는,상기 음성 발화가 텍스트 독립(TI) SID를 수행하기에 충분한 길이가 아니라고 결정하는 것에 응답하여: 텍스트 독립(TD) SID 모델을 상기 SID 모델로서 사용하여, TD 화자 임베딩을 생성하도록 상기 오디오 데이터를 프로세싱하는 단계;  TD SID 임베딩 공간에서, 상기 TD 화자 임베딩을 하나 이상의 저장된 TD 화자 임베딩과 비교하는 단계; 및  상기 TD 화자 임베딩을 상기 하나 이상의 저장된 TD 화자 임베딩과 비교하는 것에 기초하여, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는  제2 사용자에 대응하는지 여부를 결정하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>11. 제10항에 있어서, 상기 SID 모델을 사용하여, 상기 주어진 음성 발화를 제공한 사용자의 신원을 결정하기 위해 상기 주어진 음성 발화를 캡처한 오디오 데이터를 프로세싱하는 단계는,상기 음성 발화가 TI SID를 수행하기에 충분한 길이라고 결정하는 것에 응답하여: TI SID 모델을 상기 SID 모델로서 사용하여, TI 화자 임베딩을 생성하도록 상기 오디오 데이터를 프로세싱하는 단계;  TI SID 임베딩 공간에서, 상기 TI 화자 임베딩을 하나 이상의 저장된 TI 화자 임베딩과 비교하는 단계; 및  상기 TI 화자 임베딩을 하나 이상의 저장된 TI 화자 임베딩과 비교하는 것에 기초하여, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>12. 제9항에 있어서, 상기 주어진 음성 발화를 캡처하는 오디오 데이터는 컴퓨팅 디바이스의 하나 이상의 마이크로폰을 통해 생성되는, 방법.</claim></claimInfo><claimInfo><claim>13. 제1항 내지 제8항 중 어느 한 항에 있어서, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하는 단계는,얼굴 식별(FID) 모델을 사용하여, 얼굴 임베딩을 생성하기 위해 상기 주어진 음성 발화를 제공한 사용자를 캡처하는 비전 데이터를 프로세싱하는 단계; 얼굴 임베딩 공간에서, 상기 얼굴 임베딩을 하나 이상의 저장된 얼굴 임베딩과 비교하는 단계; 및 상기 얼굴 임베딩을 하나 이상의 저장된 얼굴 임베딩과 비교하는 것에 기초하여, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>14. 제13항에 있어서, 상기 주어진 음성 발화를 제공한 사용자를 캡처하는 비전 데이터는,컴퓨팅 디바이스 또는 컴퓨팅 디바이스에 통신 가능하게 결합된 추가 컴퓨팅 디바이스의 하나 이상의 비전 컴포넌트를 통해 생성되는, 방법.</claim></claimInfo><claimInfo><claim>15. 제1항 내지 제8항 중 어느 한 항에 있어서, 상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하는 단계는:상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하기 위해 컴퓨팅 디바이스 또는 그 컴퓨팅 디바이스에 통신 가능하게 결합된 추가 컴퓨팅 디바이스의 해당 디바이스 식별자를 활용하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>16. 제1항 내지 제8항 중 어느 한 항에 있어서, 상기 주어진 음성 발화를 제공한 상기 사용자가 상기 제1 사용자 또는 상기 제2 사용자에 대응하는지 여부를 결정하는 단계는,상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하기 위해 컴퓨팅 디바이스 또는 그 컴퓨팅 디바이스에 통신 가능하게 결합된 추가 컴퓨팅 디바이스의 해당 사용자 계정을 활용하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>17. 하나 이상의 프로세서에 의해 구현되는 방법으로서,사용자로부터 컴퓨팅 디바이스를 통해, 주어진 음성 발화를 수신하는 단계 - 상기 주어진 음성 발화는 사용자가 사용자의 주거지에 있는 동안 제공됨 -; 상기 주어진 음성 발화를 프로세싱하는 것에 기초하여, 이행 액션 세트를 결정하는 단계 - 상기 이행 액션 세트에 포함된 각 이행 액션은 구현 시 상기 주어진 음성 발화를 만족시키는 것을 촉진함 -;상기 주어진 음성 발화를 제공한 사용자가 제1 사용자 또는 제2 사용자에 대응하는지 여부를 결정하는 단계; 및상기 주어진 음성 발화를 제공한 사용자가 제1 사용자에 대응한다고 결정하는 것에 응답하여: 상기 제1 사용자에 대응하는 상기 주어진 음성 발화를 제공한 사용자에 기초하여, 상기 이행 액션 세트로부터 하나 이상의 제1 이행 액션의 서브세트를 선택하는 단계; 및  상기 하나 이상의 제1 이행 액션의 서브세트가 상기 주어진 음성 발화를 만족시키도록 구현되게 하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>18. 하나 이상의 프로세서에 의해 구현되는 방법으로서,복수의 시간 인스턴스의 주어진 시간 인스턴스에서, 하나 이상의 스마트 디바이스와 사용자간의 사용자 상호작용의 발생을 식별하는 단계 - 상기 사용자 상호작용은 하나 이상의 이행 액션에 대응함 -;상기 주어진 시간 인스턴스에서의 사용자의 상태를 특징짓고 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태를 특징짓는 하나 이상의 컨텍스트 신호를 획득하는 단계;상기 사용자 상호작용 및 하나 이상의 컨텍스트 신호에 기초하여 주어진 트레이닝 인스턴스를 생성하는 단계; 하나 이상의 트레이닝 조건이 만족된다고 결정하는 것에 응답하여, 상기 주어진 트레이닝 인스턴스에 적어도 기초하여 상기 사용자에 특정된 이행 액션 모델이 트레이닝되게 하는 단계; 및 상기 사용자에 특정된 이행 액션 모델이 사용자로부터 수신된 음성 발화에 응답하는데 활용되게 하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>19. 제18항에 있어서,상기 사용자 상호작용에 및 하나 이상의 컨텍스트 신호에 기초하여 상기 주어진 트레이닝 인스턴스를 생성하는 단계 이전에: 상기 주어진 시간 인스턴스에서의 사용자의 상태 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태에 기초하여 상기 하나 이상의 이행 액션이 수행되었는지 여부를 사용자가 검증하도록 요청하는 프롬프트를 생성하는 단계; 상기 사용자에게 제시하기 위해 상기 프롬프트가 제공되게 하는 단계; 및  상기 프롬프트에 응답하여, 상기 주어진 시간 인스턴스에서의 사용자의 상태 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태에 기초하여 상기 하나 이상의 이행 액션이 수행되었음을 검증하는 사용자 입력을 수신하는 단계를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>20. 제19항에 있어서, 상기 사용자 상호작용 및 하나 이상의 컨텍스트 신호에 기초하여 상기 주어진 트레이닝 인스턴스를 생성하는 단계는,상기 주어진 시간 인스턴스에서의 사용자의 상태 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태에 기초하여 상기 하나 이상의 이행 액션이 수행되었다는 검증을 수신하는 것에 대한 응답인, 방법.</claim></claimInfo><claimInfo><claim>21. 제18항 내지 제20항 중 어느 한 항에 있어서, 상기 사용자 상호작용 및 상기 하나 이상의 컨텍스트 신호에 기초하여 상기 주어진 트레이닝 인스턴스를 생성하는 단계는,상기 주어진 트레이닝 인스턴스에 대해: 트레이닝 인스턴스 입력을 결정하는 단계 - 상기 트레이닝 인스턴스 입력은 (i) 상기 주어진 시간 인스턴스에서의 사용자 상태를 특징짓고 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태를 특징짓는 하나 이상의 컨텍스트 신호, 및 (ii) 그 하나 이상의 컨텍스트 신호와 연관된 이행 액션 세트를 포함함 -; 및  트레이닝 인스턴스 출력을 결정하는 단계 - 상기 트레이닝 인스턴스 출력은 사용자 상호작용의 하나 이상의 이행 액션을 포함함 - 를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>22. 제21항에 있어서, 상기 사용자에 특정한 상기 이행 액션 모델이 적어도 상기 주어진 트레이닝 인스턴스에 기초하여 트레이닝되게 하는 단계는,상기 이행 액션 모델을 사용하여, 예측 출력을 생성하도록 상기 트레이닝 인스턴스 입력을 프로세싱하는 단계;하나 이상의 손실을 생성하도록 상기 예측된 출력을 트레이닝 인스턴스 출력과 비교하는 단계; 및 상기 하나 이상의 손실들에 기초하여 이행 액션 모델을 업데이트하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>23. 제18항 내지 제22항 중 어느 한 항에 있어서, 상기 하나 이상의 트레이닝 조건은 이행 액션 모델을 트레이닝하는데 사용 가능한 트레이닝 인스턴스의 수량, 하루 중 시간 및/또는 요일을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 제18항 내지 제23항 중 어느 한 항에 있어서,상기 복수의 시간 인스턴스의 추가 주어진 시간 인스턴스에서, 상기 하나 이상의 스마트 디바이스와 추가 사용자 간의 추가 사용자 상호작용의 발생을 식별하는 단계 - 상기 추가 사용자 상호작용은 하나 이상의 추가 이행 액션에 대응하고, 그리고 상기 하나 이상의 추가 이행 액션은 그 하나 이상의 이행 액션과 상이한 적어도 하나의 이행 액션을 포함함 -;상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 상태를 특징짓고 및/또는 상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 환경 상태를 특징짓는 하나 이상의 추가 컨텍스트 신호를 획득하는 단계;상기 추가 사용자 상호작용 및 하나 이상의 추가 컨텍스트 신호에 기초하여 추가 주어진 트레이닝 인스턴스를 생성하는 단계; 상기 하나 이상의 트레이닝 조건이 만족된다고 결정하는 것에 응답하여, 상기 추가 주어진 트레이닝 인스턴스에 적어도 기초하여 추가 사용자에 특정된 추가 이행 액션 모델이 트레이닝되게 하는 단계; 및 상기 추가 사용자에 특정된 추가 이행 액션 모델이 추가 사용자로부터 수신된 음성 발화에 응답하는데 활용되게 하는 단계를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>25.  하나 이상의 프로세서에 의해 구현되는 방법으로서,복수의 시간 인스턴스의 주어진 시간 인스턴스에서, 하나 이상의 스마트 디바이스와 사용자 간의 사용자 상호작용의 발생을 식별하는 단계 - 상기 사용자 상호작용은 하나 이상의 이행 액션들에 대응함 -;상기 주어진 시간 인스턴스에서의 사용자의 상태를 특징짓고 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태를 특징짓는 하나 이상의 컨텍스트 신호들을 획득하는 단계;상기 주어진 시간 인스턴스에서의 사용자의 상태 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태에 기초하여 상기 하나 이상의 이행 액션이 수행되었는지 여부를 사용자가 검증하도록 요청하는 프롬프트를 생성하는 단계;상기 사용자에게 제시하기 위해 상기 프롬프트가 제공되게 하는 단계; 상기 프롬프트에 응답하여, 상기 주어진 시간 인스턴스에서의 사용자의 상태 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태에 기초하여 상기 하나 이상의 이행 액션이 수행되었음을 검증하는 사용자 입력을 수신하는 단계; 상기 주어진 시간 인스턴스에서의 사용자의 상태 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경 상태에 기초하여 상기 하나 이상의 이행 액션이 수행되었음을 검증하는 사용자 입력을 수신하는 것에 응답하여, 사용자에 특정된 하나 이상의 이행 액션 규칙을 생성하는 단계; 및 사용자에 특정된 상기 하나 이상의 이행 액션 규칙이 사용자로부터 수신된 음성 발화에 응답하는데 활용되게 하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>26. 제25항에 있어서,상기 복수의 시간 인스턴스의 추가 주어진 시간 인스턴스에서, 하나 이상의 스마트 디바이스와 추가 사용자 간의 추가 사용자 상호작용의 발생을 식별하는 단계 - 상기 추가 사용자 상호작용은 하나 이상의 추가 이행 액션에 대응하고, 그리고 상기 하나 이상의 추가 이행 액션은 그 하나 이상의 이행 액션과 상이한 적어도 하나의 이행 액션을 포함함 -;상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 상태를 특징짓고 및/또는 상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 환경 상태를 특징짓는 하나 이상의 추가 컨텍스트 신호를 획득하는 단계;상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 상태 및/또는 상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 환경 상태에 기초하여 상기 하나 이상의 추가 이행 액션이 수행되었는지 여부를 추가 사용자가 검증하도록 요청하는 추가 프롬프트를 생성하는 단계;상기 추가 사용자에게 제시하기 위해 추가 프롬프트가 제공되게 하는 단계; 상기 추가 프롬프트에 응답하여, 상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 상태 및/또는 상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 환경 상태에 기초하여 상기 하나 이상의 추가 이행 액션이 수행되었음을 검증하는 추가 사용자 입력을 수신하는 단계; 상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 상태 및/또는 상기 추가 주어진 시간 인스턴스에서의 추가 사용자의 추가 환경 상태에 기초하여 상기 하나 이상의 추가 이행 액션이 수행되었음을 검증하는 추가 사용자 입력을 수신하는 것에 응답하여, 상기 추가 사용자에 특정된 하나 이상의 추가 이행 액션 규칙을 생성하는 단계; 및 추가 사용자에 특정한 상기 하나 이상의 추가 이행 액션 규칙이 추가 사용자로부터 수신된 음성 발화에 응답하는데 활용되게 하는 단계를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>27. 하나 이상의 프로세서에 의해 구현되는 방법으로서,사용자에 대해, 하나 이상의 이행 액션에 대한 하나 이상의 음성 커맨드의 개인화 매핑(personalized mapping)을 생성하는 단계 - 상기 개인화 매핑을 생성하는 단계는 임의의 음성 발화를 제공하는 것과 무관하게 상기 하나 이상의 음성 커맨드와 상관관계가 있는 것으로 결정된 컨텍스트 시나리오에서 상기 하나 이상의 이행 액션을 수행하게 하는 사용자의 하나 이상의 과거 인스턴스을 결정하는 것에 대한 응답임 -;상기 사용자로부터 상기 개인화 매핑을 생성한 후에, 주어진 음성 발화를 수신하는 단계;상기 주어진 음성 발화를 프로세싱하는 것에 기초하여, 상기 주어진 음성 발화가 상기 음성 커맨드들 중 하나 이상 중 하나하고 결정하는 단계;상기 주어진 발화가 하나 이상의 음성 커맨드들 중 하나라고 결정하는 것에 응답하여: 상기 하나 이상의 이행 액션이 자동으로 구현되도록 상기 개인화 매핑을 사용하하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>28. 제27항에 있어서, 상기 하나 이상의 이행 액션에 대한 하나 이상의 음성 커맨드의 개인화 매핑을 생성하는 단계는,의미론적 공간(semantic space)에서, 상기 하나 이상의 과거 인스턴스에 기초하여, 상기 하나 이상의 음성 커맨드의 음성 커맨드 의미론적 표현과 상기 하나 이상의 이행 액션의 이행 액션 의미론적 표현 사이의 매핑을 생성하는 단계; 및하나 이상의 데이터베이스에 상기 매핑을 저장하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>29. 제27항 또는 제28항에 있어서, 상기 하나 이상의 이행 액션에 대한 하나 이상의 음성 커맨드의 개인화 매핑을 생성하는 단계는,상기 하나 이상의 과거 인스턴스에 기초하여, 상기 하나 이상의 음성 커맨드의 인텐트와 상기 하나 이상의 이행 액션 사이의 매핑을 생성하는 단계; 및하나 이상의 데이터베이스에 상기 매핑을 저장하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>30. 하나 이상의 프로세서에 의해 구현되는 방법으로서,복수의 시간 인스턴스의 주어진 시간 인스턴스에서, 하나 이상의 스마트 디바이스와 사용자 간의 사용자 상호작용의 발생을 식별하는 단계 - 상기 사용자 상호작용은 하나 이상의 이행 액션에 대응함 -;상기 사용자의 컨텍스트 시나리오를 특징짓는 하나 이상의 컨텍스트 신호를 획득하는 단계 - 상기 사용자의 컨텍스트 시나리오를 특징짓는 하나 이상의 컨텍스트 신호는 상기 주어진 시간 인스턴스에서의 사용자의 상태를 특징짓고 및/또는 상기 주어진 시간 인스턴스에서의 사용자의 환경의 상태를 특징지음 -;상기 사용자의 컨텍스트 시나리오에 할당할 하나 이상의 음성 커맨드를 식별하는 단계 - 상기 하나 이상의 음성 커맨드는 사용자에 의해 후속적으로 제공될 때, 상기 사용자 상호작용의 하나 이상의 이행 액션이 구현되게 함 -; 상기 사용자로부터 상기 복수의 시간 인스턴스의 후속 시간 인스턴스에서, 주어진 음성 발화의 인스턴스를 수신하는 단계 - 상기 후속 시간 인스턴스는 주어진 시간 인스턴스에 후속함 -;상기 주어진 음성 발화의 인스턴스를 프로세싱하는 것에 기초하여, 상기 주어진 음성 발화의 인스턴스가 상기 음성 커맨드들 중 하나 이상을 포함한다고 결정하는 단계;상기 주어진 음성 발화의 인스턴스를 만족시키기 위해 상기 하나 이상의 이행 액션이 구현되게 하도록 결정하는 단계; 및상기 하나 이상의 이행 액션이 상기 주어진 음성 발화의 인스턴스를 만족시키도록 구현되게 하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>31. 제30항에 있어서,상기 복수의 시간 인스턴스의 추가 후속 시간 인스턴스에서, 상기 하나 이상의 스마트 디바이스와 추가 사용자 간의 추가 사용자 상호작용의 추가 발생을 식별하는 단계 - 상기 추가 사용자 상호작용은 하나 이상의 추가 이행 액션에 대응하고, 상기 하나 이상의 추가 이행 액션은 상기 하나 이상의 이행 액션과 상이한 적어도 하나의 이행 액션을 포함하고, 그리고 상기 추가 후속 시간 인스턴스는 상기 주어진 시간 인스턴스에 후속함 -;상기 추가 사용자의 추가 컨텍스트 시나리오를 특징짓는 하나 이상의 추가 컨텍스트 신호를 획득하는 단계 - 상기 추가 사용자의 추가 컨텍스트 시나리오를 특징짓는 하나 이상의 추가 컨텍스트 신호는 추가 후속 시간 인스턴스에서 추가 사용자의 추가 상태를 특징짓고 및/또는 추가 후속 시간 인스턴스에서 추가 사용자의 추가 환경 상태를 특징지으며, 그리고 상기 추가 사용자의 추가 컨텍스트 시나리오는 사용자의 컨텍스트 시나리오와 일치(match)함 -;상기 추가 사용자의 추가 컨텍스트 시나리오에 할당할 상기 하나 이상의 음성 커맨드를 식별하는 단계 - 상기 하나 이상의 음성 커맨드는 추가 사용자에 의해 후속적으로 제공될 때 상기 추가 사용자 상호작용의 하나 이상의 추가 이행 액션이 구현되게 함 -; 상기 추가 사용자로부터 복수의 시간 인스턴스의 또 다른 후속 시간 인스턴스에서, 상기 주어진 음성 발화의 추가 인스턴스를 수신하는 단계 - 상기 또 다른 후속 시간 인스턴스는 추가 후속 시간 인스턴스에 후속함 -;상기 주어진 음성 발화의 추가 인스턴스를 프로세싱하는 것에 기초하여, 상기 주어진 음성 발화의 추가 인스턴스가 상기 음성 커맨드들 중 하나 이상을 포함한다고 결정하는 단계;상기 주어진 음성 발화의 추가 인스턴스를 만족시키기 위해 상기 하나 이상의 추가 이행 액션이 구현되게 하도록 결정하는 단계; 및상기 주어진 음성 발화의 추가 인스턴스를 만족시키기 위해 상기 하나 이상의 추가 이행 액션이 구현되게 하는 단계를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>32. 시스템으로서, 하나 이상의 프로세서; 및 명령어들을 저장하는 메모리를 포함하며, 상기 명령어들은 실행될 때 상기 하나 이상의 프로세서로 하여금 제1항 내지 제31항 중 어느 한 항의 동작들을 수행하게 하는, 시스템. </claim></claimInfo><claimInfo><claim>33.  명령어들을 저장하는 비-일시적 컴퓨터 판독가능 저장 매체로서, 상기 명령어들은 실행될 때 하나 이상의 프로세서로 하여금 제1항 내지 제31항 중 어느 한 항의 동작들을 수행하게 하는, 비-일시적 컴퓨터 판독가능 저장 매체. </claim></claimInfo><claimInfo><claim>34. 차량 내 컴퓨팅 디바이스로서,하나 이상의 프로세서; 및 명령어들을 저장하는 메모리를 포함하며, 상기 명령어들은 실행될 때 상기 하나 이상의 프로세서로 하여금 제1항 내지 제31항 중 어느 한 항의 동작들을 수행하게 하는, 차량 내 컴퓨팅 디바이스. </claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 캘리포니아 마운틴 뷰 엠피시어터 파크웨이 **** (우:*****)</address><code>520050013456</code><country>미국</country><engName>Google LLC</engName><name>구글 엘엘씨</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>SINGHAL, Amit</engName><name>싱할 아밋</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>PATEL, Dev</engName><name>파텔 데브</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>LIN, Yao</engName><name>린 야오</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>SHARMA, Arvind</engName><name>샤르마 아빈드</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>SUBRAMANIAN, Srikrishnan</engName><name>수브라마니안 스리크리슈난</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 강남대로 *** (논현동) *-*F(박장원특허법률사무소)</address><code>919980002023</code><country>대한민국</country><engName>PARK, Jang Won</engName><name>박장원</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.03.11</priorityApplicationDate><priorityApplicationNumber>17/692,743</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2024.09.04</receiptDate><receiptNumber>1-1-2024-0973012-24</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2024.09.12</receiptDate><receiptNumber>1-5-2024-0149988-92</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for refusal</documentEngName><documentName>의견제출통지서</documentName><receiptDate>2025.11.11</receiptDate><receiptNumber>9-5-2025-1096797-68</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247029827.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c939b8f15c86592c6c88ddfd6f784a86bc0f977b37800e77b9f6371fde98539192cd882e3455953aa36c89f5e07516c31bda45bc9ac050e5e99</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf3abef8b3818ed61dac8e6419c9414d5ed099568468dedf4a950b6bef87120b306514d7ffdfde01a12ee1b4b5e45d108ea8482725d6d0db54</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>