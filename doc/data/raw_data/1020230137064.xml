<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:10:24.1024</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.10.13</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0137064</applicationNumber><claimCount>12</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>비디오 영상 내 인간과 객체 간의 상호 작용을 분석 및 학습하여 인간 행동 의도를 추론하는 영상 분석 장치 및 영상 분석 방법</inventionTitle><inventionTitleEng>IMAGE ANALYSIS APPARATUS AND IMAGE ANALYSIS METHOD  OF ESTIMATING HUMAN BEHAVIOR INTENTION BY ANALYZING  AND LEARNING INTERACTION BETWEEN HUMAN AND OBJECTS  IN VIDEO IMAGE</inventionTitleEng><openDate>2025.04.24</openDate><openNumber>10-2025-0054845</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.10.13</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/774</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/80</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/62</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 30/182</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/82</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 5/04</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 발명은 비디오 영상 내 인간과 객체 간의 상호 작용을 분석 및 학습하여 인간 행동 의도를 추론하는 영상 분석 장치 및 영상 분석 방법에 관한 것으로, 본 발명의 일실시예에 따른 영상 분석 장치는 비디오 영상으로부터 시각 특징 정보, 바운딩 박스 특징 정보 및 객체 카테고리 특징 정보를 추출하는 제1 학습 정보 추출부, 언어 생성 모델 및 언어 인코딩 모델을 이용하여 상기 비디오 영상에 대한 레이블 텍스트로부터 텍스트 특징 정보를 추출하는 제2 학습 정보 추출부, 상기 텍스트 특징 정보와 상기 시각 특징 정보를 융합하여 시각 텍스트 결합 특징 정보를 생성하고, 상기 시각 텍스트 결합 특징 정보, 상기 바운딩 박스 특징 정보 및 상기 객체 카테고리 특징 정보 중 적어도 하나를 조합하여 학습 데이터를 생성하는 학습 데이터 생성부, 상기 학습 데이터를 시간순서 학습 네트워크에 적용하여 상기 비디오 영상의 영상 프레임들에 대해 시간 순서에 따른 예측 특징 정보를 추출하는 시간순서 학습 처리부 및 그래프 주의 신경망을 활용하여 상기 예측 특징 정보에 대하여 인간과 객체들의 특징을 연결해 공간 특징 추출 그래프 및 시간 특징 추출 그래프를 구성하고, 상기 공간 특징 추출 그래프 및 상기 시간 특징 추출 그래프를 이용하여 상기 인간과 상기 객체들에 대한 상호 작용 특징을 학습하는 상호작용 모델링 처리부를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 비디오 영상으로부터 시각 특징 정보, 바운딩 박스 특징 정보 및 객체 카테고리 특징 정보를 추출하는 제1 학습 정보 추출부;언어 생성 모델 및 언어 인코딩 모델을 이용하여 상기 비디오 영상에 대한 레이블 텍스트로부터 텍스트 특징 정보를 추출하는 제2 학습 정보 추출부;상기 텍스트 특징 정보와 상기 시각 특징 정보를 융합하여 시각 텍스트 결합 특징 정보를 생성하고, 상기 시각 텍스트 결합 특징 정보, 상기 바운딩 박스 특징 정보 및 상기 객체 카테고리 특징 정보 중 적어도 하나를 조합하여 학습 데이터를 생성하는 학습 데이터 생성부;상기 학습 데이터를 시간순서 학습 네트워크에 적용하여 상기 비디오 영상의 영상 프레임들에 대해 시간 순서에 따른 예측 특징 정보를 추출하는 시간순서 학습 처리부; 및그래프 주의 신경망을 활용하여 상기 예측 특징 정보에 대하여 인간과 객체들의 특징을 연결해 공간 특징 추출 그래프 및 시간 특징 추출 그래프를 구성하고, 상기 공간 특징 추출 그래프 및 상기 시간 특징 추출 그래프를 이용하여 상기 인간과 상기 객체들에 대한 상호 작용 특징을 학습하는 상호작용 모델링 처리부를 포함하는 것을 특징으로 하는 영상 분석 장치.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 학습된 상호 작용 특징을 상기 인간과 객체에 따라 분리하여 인간의 세부행위(sub-activity)와 객체 행위유발성(affordance)을 예측한 예측 결과에 따른 인간 행동 의도에 대한 추론 결과로 도출 및 제공하는 추론 결과 제공부를 더 포함하는 것을 특징으로 하는영상 분석 장치.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 제1 학습 정보 추출부는 상기 비디오 영상에서 객체를 추적하고, 상기 추적된 객체 위치를 추출하며, 상기 추출된 객체 위치를 연결하여 궤도 정보를 추출하고, 상기 추출된 궤도 정보에 기반하여 상기 시각 특징 정보, 상기 바운딩 박스 특징 정보 및 상기 객체 카테고리 특징 정보를 추출하는 것을 특징으로 하는영상 분석 장치.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 제2 학습 정보 추출부는 상기 언어 생성 모델을 이용하여 상기 레이블 텍스트를 하나의 문단으로 확장하고, 상기 확장된 문단의 각 단어를 토큰화 후 상기 언어 인코딩 모델에 입력하여 텍스트 특징 벡터를 상기 텍스트 특징 정보로 추출하는 것을 특징으로 하는영상 분석 장치.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 학습 데이터 생성부는 상기 바운딩 박스 특징 정보와 상기 객체 카테고리 특징 정보를 조합하여 공간 특징과 관련된 제1 학습 데이터를 생성하고, 상기 시각 텍스트 결합 특징 정보와 상기 바운딩 박스 특징 정보를 조합하여 시각 특징과 관련된 제2 학습 데이터를 생성하는 것을 특징으로 하는 영상 분석 장치.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 시간순서 학습 처리부는 상기 제1 학습 데이터 및 상기 제2 학습 데이터를 각각을 BiRNN 모델 기반 시간순서 학습 네트워크에 적용하여  상기 공간 특징과 상기 시각 특징 각각에 대하여 시간 순서를 예측하여 상기 예측 특징 정보를 추출하는 것을 특징으로 하는영상 분석 장치.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 상호작용 모델링 처리부는 상기 공간 특징 추출 그래프 및 상기 시간 특징 추출 그래프에서 상기 인간과 관련된 인간 행동을 중심으로 상기 객체들과 관련된 객체 상태의 분포에서 상기 인간과 상기 객체들 간에 연결 정도에 따라 상기 객체들에 대하여 가중치를 부여하는 것을 특징으로 하는영상 분석 장치.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 상호작용 모델링 처리부는 상기 공간 특징 추출 그래프 및 상기 시간 특징 추출 그래프를 노드 특징(X), 학습가능한 파라미터(W) 그리고 노드 간 연결된 상태를 나타내는 인접행렬로 구성하여 상기 노드 특징(X), 상기 학습가능한 파라미터(W) 및 상기 노드 간 연결된 상태의 행렬 곱 연산을 통해서 상기 인간과 상기 객체들에 대한 특징을 획득하는 것을 특징으로 하는영상 분석 장치.</claim></claimInfo><claimInfo><claim>9. 제1 학습 정보 추출부에서, 비디오 영상으로부터 시각 특징 정보, 바운딩 박스 특징 정보 및 객체 카테고리 특징 정보를 추출하는 단계;제2 학습 정보 추출부에서, 언어 생성 모델 및 언어 인코딩 모델을 이용하여 상기 비디오 영상에 대한 레이블 텍스트로부터 텍스트 특징 정보를 추출하는 단계;학습 데이터 생성부에서, 상기 텍스트 특징 정보와 상기 시각 특징 정보를 융합하여 시각 텍스트 결합 특징 정보를 생성하고, 상기 시각 텍스트 결합 특징 정보, 상기 바운딩 박스 특징 정보 및 상기 객체 카테고리 특징 정보 중 적어도 하나를 조합하여 학습 데이터를 생성하는 단계;시간순서 학습 처리부에서, 상기 학습 데이터를 시간순서 학습 네트워크에 적용하여 상기 비디오 영상의 영상 프레임들에 대해 시간 순서에 따른 예측 특징 정보를 추출하는 단계; 및상호작용 모델링 처리부에서, 그래프 주의 신경망을 활용하여 상기 예측 특징 정보에 대하여 인간과 객체들의 특징을 연결해 공간 특징 추출 그래프 및 시간 특징 추출 그래프를 구성하고, 상기 공간 특징 추출 그래프 및 상기 시간 특징 추출 그래프를 이용하여 상기 인간과 상기 객체들에 대한 상호 작용 특징을 학습하는 단계를 포함하는 것을 특징으로 하는 영상 분석 방법.</claim></claimInfo><claimInfo><claim>10. 제9항에 있어서,추론 결과 제공부에서, 상기 학습된 상호 작용 특징을 상기 인간과 객체에 따라 분리하여 인간의 세부행위(sub-activity)와 객체 행위유발성(affordance)을 예측한 예측 결과에 따른 인간 행동 의도에 대한 추론 결과로 도출 및 제공하는 단계를 더 포함하는 것을 특징으로 하는영상 분석 방법.</claim></claimInfo><claimInfo><claim>11. 제9항에 있어서,상기 비디오 영상으로부터 시각 특징 정보, 바운딩 박스 특징 정보 및 객체 카테고리 특징 정보를 추출하는 단계는,상기 비디오 영상에서 객체를 추적하고, 상기 추적된 객체 위치를 추출하며, 상기 추출된 객체 위치를 연결하여 궤도 정보를 추출하고, 상기 추출된 궤도 정보에 기반하여 상기 시각 특징 정보, 상기 바운딩 박스 특징 정보 및 상기 객체 카테고리 특징 정보를 추출하는 단계를 포함하고,상기 언어 생성 모델 및 언어 인코딩 모델을 이용하여 상기 비디오 영상에 대한 레이블 텍스트로부터 텍스트 특징 정보를 추출하는 단계는,상기 언어 생성 모델을 이용하여 상기 레이블 텍스트를 하나의 문단으로 확장하고, 상기 확장된 문단의 각 단어를 토큰화 후 상기 언어 인코딩 모델에 입력하여 텍스트 특징 벡터를 상기 텍스트 특징 정보로 추출하는 단계를 포함하는 것을 특징으로 하는영상 분석 방법.</claim></claimInfo><claimInfo><claim>12. 제9항에 있어서,상기 텍스트 특징 정보와 상기 시각 특징 정보를 융합하여 시각 텍스트 결합 특징 정보를 생성하고, 상기 시각 텍스트 결합 특징 정보, 상기 바운딩 박스 특징 정보 및 상기 객체 카테고리 특징 정보 중 적어도 하나를 조합하여 학습 데이터를 생성하는 단계는,상기 바운딩 박스 특징 정보와 상기 객체 카테고리 특징 정보를 조합하여 공간 특징과 관련된 제1 학습 데이터를 생성하고, 상기 시각 텍스트 결합 특징 정보와 상기 바운딩 박스 특징 정보를 조합하여 시각 특징과 관련된 제2 학습 데이터를 생성하는 단계를 포함하는 것을 특징으로 하는 영상 분석 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>대구광역시 북구...</address><code>220040016844</code><country>대한민국</country><engName>Kyungpook National  University Industry-Academic Cooperation Foundation</engName><name>경북대학교 산학협력단</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>대구광역시 북구...</address><code> </code><country> </country><engName>LEE Dong Gyu</engName><name>이동규</name></inventorInfo><inventorInfo><address>대구광역시 북구...</address><code> </code><country> </country><engName>HONG Hye Seong</engName><name>홍혜성</name></inventorInfo><inventorInfo><address>대구광역시 북구...</address><code> </code><country> </country><engName>LEE Jeong Cheol</engName><name>이정철</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 송파구 송파대로 *** (문정동) B동 ****호(시안특허법률사무소)</address><code>920030003990</code><country>대한민국</country><engName>Kim Youn Gwon</engName><name>김연권</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.10.13</receiptDate><receiptNumber>1-1-2023-1125261-63</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>특허고객번호 정보변경(경정)신고서·정정신고서</documentName><receiptDate>2024.01.03</receiptDate><receiptNumber>4-1-2024-5008436-87</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>특허고객번호 정보변경(경정)신고서·정정신고서</documentName><receiptDate>2024.11.25</receiptDate><receiptNumber>4-1-2024-5343480-00</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230137064.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93fbe846361c5573bb94cc1bdc002e55f84b67e44ae55367c23059768a2b9e4ab5e3ea0fe055b2ff85d06fc1addf0d24e496a4a60873717ec1</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf987dad5536ab1294893fe1fe02672cbc6cf6943afaff1e3b428153f2d54a4c258a3ea2b5f6ab156775a565b1ca30d73b4f20468304632ac1</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>