<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:29:25.2925</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.08.04</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2025-7012050</applicationNumber><claimCount>15</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>패스스루 증강 현실(AR) 시스템들에서 효율적인 깊이 재구성 및 필터링을 갖춘 메시 변환</inventionTitle><inventionTitleEng>MESH TRANSFORMATION WITH EFFICIENT DEPTH RECONSTRUCTION AND FILTERING IN PASSTHROUGH AUGMENTED REALITY (AR) SYSTEMS</inventionTitleEng><openDate>2025.05.15</openDate><openNumber>10-2025-0067902</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.04.11</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.04.11</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 17/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 15/10</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 방법이 패스스루 AR 기기와 연관된 이미징 센서들에 의해 캡처된 환경의 이미지들과, 이미지들과 연관된 위치 데이터 및 깊이 데이터를 획득하는 단계를 포함한다. 이 방법은 또한 이미지들, 위치 데이터, 및 깊이 데이터에 기반하여 환경을 대표하는 점 구름을 생성하는 단계를 포함한다. 이 방법은 특정된 이미지에 대한 메시를 생성하는 단계를 더 포함한다. 메시는 메시 라인들의 교차점들에 그리드 지점들을 포함한다. 이 방법은 또한 메시의 하나 이상의 그리드 지점의 하나 이상의 깊이를 결정하는 단계를 포함한다. 이 방법은 그리드 지점(들)의 깊이(들)에 기반하여, 특정된 이미지를 캡처했던 특정된 이미징 센서의 시점에서 패스스루 AR 기기의 사용자 시점으로 메시를 변환하는 단계를 더 포함한다. 추가적으로, 이 방법은 변환된 메시에 기반하여 패스스루 AR 기기에 의한 표시를 위해 특정된 이미지의 가상 뷰를 렌더링하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2024.05.30</internationOpenDate><internationOpenNumber>WO2024111783</internationOpenNumber><internationalApplicationDate>2023.08.04</internationalApplicationDate><internationalApplicationNumber>PCT/KR2023/011504</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 패스스루 증강 현실(augmented reality, AR) 기기와 연관된 이미징 센서들에 의해 캡처된 환경의 이미지들, 상기 이미지들과 연관된 위치 데이터 및 상기 이미지들과 연관된 깊이 데이터를 획득하는 단계;상기 이미지들, 상기 위치 데이터 및 상기 깊이 데이터에 기반하여 상기 환경을 대표하는 점 구름(point cloud)을 생성하는 단계;상기 이미지들 중 특정된 이미지에 대한 메시를 생성하되, 상기 메시는 메시 라인들의 교차점들에서의 그리드 지점들을 포함하는 단계;상기 메시의 상기 그리드 지점들 중 하나 이상의 그리드 지점의 하나 이상의 깊이를 결정하는 단계;상기 하나 이상의 그리드 지점의 상기 하나 이상의 깊이에 기반하여 상기 특정된 이미지를 캡처했던 상기 이미징 센서들 중 특정된 이미징 센서의 시점에서 상기 패스스루 AR 기기의 사용자 시점으로 상기 메시를 변환하는 단계; 및변환된 메시에 기반하여 상기 패스스루 AR 기기에 의한 표시를 위한 상기 특정된 이미지의 가상 뷰를 렌더링하는 단계;를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 하나 이상의 그리드 지점의 상기 하나 이상의 깊이를 결정하는 단계는, 이미지 특징 정보, 깊이 특징 정보, 물체 자세 정보, 이미지 세기 정보 및 공간 정보 중 적어도 하나에 기반하여 상기 하나 이상의 그리드 지점의 상기 하나 이상의 깊이를 결정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서, 상기 점 구름을 생성하는 단계는 (i) 상기 패스스루 AR 기기와 연관된 하나 이상의 깊이 센서에 의해 수집된 제1 깊이 데이터 및 (ii) 상기 특정된 이미지 및 상기 위치 데이터에 SLAM(simultaneous localization and mapping)을 적용함으로써 결정된 제2 깊이 데이터에 기반하여 상기 점 구름을 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 하나 이상의 그리드 지점의 상기 하나 이상의 깊이를 결정하는 단계는 상기 점 구름 내의 상기 특정된 그리드 지점의 임계 거리 내의 지점들과 연관된 알려진 깊이들의 가중된 평균에 기반하여 상기 하나 이상의 그리드 지점 중 특정된 그리드 지점의 깊이를 결정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서, 상기 가중된 평균은 이미지 특징 정보, 깊이 특징 정보, 물체 자세 정보, 이미지 세기 정보 및 공간 정보 중 적어도 하나에 기반하는 가중값들을 사용하여 결정되는, 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 상기 가상 뷰를 렌더링하는 단계는,상기 변환된 메시에 기반하여 상기 특정된 이미지 내의 하나 이상의 실제 물체에 대한 가상 물체의 시차 및 상기 가상 물체의 위치를 결정하는 단계; 및결정된 시차 및 결정된 위치에 기반하여 상기 가상 물체를 포함하도록 상기 특정된 이미지의 상기 가상 뷰를 렌더링하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 이미지들 중 상이한 이미지들에 대한 상이한 메시들을 생성하는 단계;상기 상이한 메시들 각각에 대해, 상기 메시 내의 하나 이상의 그리드 지점의 하나 이상의 깊이를 결정하고 상기 하나 이상의 깊이에 기반하여 상기 메시를 변환하는 단계; 및상기 상이한 메시들로부터 변환된 메시들에 기반하여 상기 패스스루 AR 기기에 의한 표시를 위해 상기 이미지들 중 상이한 이미지들의 상이한 가상 뷰들을 렌더링하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 패스스루 증강 현실(augmented reality, AR) 기기에 있어서,환경의 이미지들을 캡처하도록 구성되는 이미징 센서들; 상기 이미지들, 상기 이미지들과 연관된 위치 데이터 및 상기 이미지들과 연관된 깊이 데이터를 획득하며; 상기 이미지들, 상기 위치 데이터 및 상기 깊이 데이터에 기반하여 상기 환경을 대표하는 점 구름(point cloud)을 생성하며; 상기 이미지들 중 특정된 이미지에 대한 메시를 생성하되, 상기 메시는 메시 라인들의 교차점들에서의 그리드 지점들을 포함하며; 상기 메시의 상기 그리드 지점들 중 하나 이상의 그리드 지점의 하나 이상의 깊이를 결정하며; 상기 하나 이상의 그리드 지점의 상기 하나 이상의 깊이에 기반하여 상기 특정된 이미지를 캡처했던 상기 이미징 센서들 중 특정된 이미징 센서의 시점에서 상기 패스스루 AR 기기의 사용자 시점으로 상기 메시를 변환하며; 및 변환된 메시에 기반하여 상기 특정된 이미지의 가상 뷰를 렌더링하도록구성되는 적어도 하나의 프로세싱 기기; 및상기 가상 뷰를 표시하도록 구성되는 적어도 하나의 디스플레이;를 포함하는, 패스스루 AR 기기.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서, 상기 적어도 하나의 프로세싱 기기는 이미지 특징 정보, 깊이 특징 정보, 물체 자세 정보, 이미지 세기 정보 및 공간 정보 중 적어도 하나에 기반하여 상기 하나 이상의 그리드 지점의 상기 하나 이상의 깊이를 결정하도록 구성되는, 패스스루 AR 기기.</claim></claimInfo><claimInfo><claim>10. 제8항에 있어서, 상기 적어도 하나의 프로세싱 기기는 (i) 상기 패스스루 AR 기기와 연관된 하나 이상의 깊이 센서에 의해 수집된 제1 깊이 데이터 및 (ii) 상기 특정된 이미지 및 상기 위치 데이터에 SLAM(simultaneous localization and mapping)을 적용함으로써 결정된 제2 깊이 데이터에 기반하여 상기 점 구름을 생성하도록 구성되는, 패스스루 AR 기기.</claim></claimInfo><claimInfo><claim>11. 제8항에 있어서, 상기 적어도 하나의 프로세싱 기기는 상기 점 구름 내의 상기 특정된 그리드 지점의 임계 거리 내의 지점들과 연관된 알려진 깊이들의 가중된 평균에 기반하여 상기 하나 이상의 그리드 지점 중 특정된 그리드 지점의 깊이를 결정하도록 구성되는, 패스스루 AR 기기.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서, 상기 적어도 하나의 프로세싱 기기는 이미지 특징 정보, 깊이 특징 정보, 물체 자세 정보, 이미지 세기 정보 및 공간 정보 중 적어도 하나에 기반하는 가중값들을 사용하여 상기 가중된 평균을 결정하도록 구성되는, 패스스루 AR 기기.</claim></claimInfo><claimInfo><claim>13. 제8항에 있어서, 상기 가상 뷰를 렌더링하기 위해, 상기 적어도 하나의 프로세싱 기기는,상기 변환된 메시에 기반하여 상기 특정된 이미지 내의 하나 이상의 실제 물체에 대한 가상 물체의 시차 및 상기 가상 물체의 위치를 결정하며; 및결정된 시차 및 결정된 위치에 기반하여 상기 가상 물체를 포함하도록 상기 특정된 이미지의 상기 가상 뷰를 렌더링하도록구성되는, 패스스루 AR 기기.</claim></claimInfo><claimInfo><claim>14. 제8항에 있어서, 상기 적어도 하나의 프로세싱 기기는,상기 이미지들 중 상이한 이미지들에 대한 상이한 메시들을 생성하며;상기 상이한 메시들 각각에 대해, 상기 메시 내의 하나 이상의 그리드 지점의 하나 이상의 깊이를 결정하고 상기 하나 이상의 깊이에 기반하여 상기 메시를 변환하며; 및상기 상이한 메시들로부터 변환된 메시들에 기반하여 상기 패스스루 AR 기기에 의한 표시를 위해 상기 이미지들 중 상이한 이미지들의 상이한 가상 뷰들을 렌더링하도록구성되는, 패스스루 AR 기기.</claim></claimInfo><claimInfo><claim>15. 명령어들을 포함하는 머신 판독가능 매체에 있어서,상기 명령어들은, 실행될 때, 패스스루 증강 현실(augmented reality, AR) 기기의 적어도 하나의 프로세서로 하여금,패스스루 AR 기기와 연관된 이미징 센서들에 의해 캡처된 환경의 이미지들, 상기 이미지들과 연관된 위치 데이터 및 상기 이미지들과 연관된 깊이 데이터를 획득하게 하며;상기 이미지들, 상기 위치 데이터 및 상기 깊이 데이터에 기반하여 상기 환경을 대표하는 점 구름(point cloud)을 생성하게 하며;상기 이미지들 중 특정된 이미지에 대한 메시를 생성하게 하되, 상기 메시는 메시 라인들의 교차점들에서의 그리드 지점들을 포함하며;상기 메시의 상기 그리드 지점들 중 하나 이상의 그리드 지점의 하나 이상의 깊이를 결정하게 하며;상기 하나 이상의 그리드 지점의 상기 하나 이상의 깊이에 기반하여 상기 특정된 이미지를 캡처했던 상기 이미징 센서들 중 특정된 이미징 센서의 시점에서 상기 패스스루 AR 기기의 사용자 시점으로 상기 메시를 변환하게 하며; 및변환된 메시에 기반하여 상기 패스스루 AR 기기에 의한 표시를 위한 상기 특정된 이미지의 가상 뷰를 렌더링하게 하는, 머신 판독가능 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 캘리포니아 ****...</address><code> </code><country>미국</country><engName>XIONG, Yingen</engName><name>시옹 잉언</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.11.21</priorityApplicationDate><priorityApplicationNumber>63/427,062</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2023.04.05</priorityApplicationDate><priorityApplicationNumber>18/296,302</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2025.04.11</receiptDate><receiptNumber>1-1-2025-0414010-67</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2025.04.15</receiptDate><receiptNumber>1-5-2025-0062614-15</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2025.07.29</receiptDate><receiptNumber>1-1-2025-0861889-23</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257012050.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93431a24b4618a3c869f7f95dd3d24dc666ec306bc76e77f7db58a78f48fbf7ceadd3e4e761c0f9bb00cc659ede88559d579e60ca2c845dab2</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf2d1fbb3c52754cf565eb05afc17bbb75d7299091b8266a1e7f5adfae2d112abb844db8906683ac67430af5460a718212b216250ec4da6800</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>