<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:02:14.214</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.01.25</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7028472</applicationNumber><claimCount>28</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>객체 기반 로봇 제어</inventionTitle><inventionTitleEng>OBJECT-BASED ROBOT CONTROL</inventionTitleEng><openDate>2023.10.05</openDate><openNumber>10-2023-0138487</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.01.24</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.08.22</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 13/06</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 13/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G05D 1/246</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 방법(400)은 로봇(100) 주위의 환경(10)에 대한 센서 데이터(134)를 수신하는 단계를 포함한다. 센서 데이터는 로봇의 하나 이상의 센서들(132, 132a 내지 132n)에 의해 캡처된다. 상기 방법은 수신된 센서 데이터를 사용하여 환경에서 하나 이상의 객체들(212, 214)을 검출하는 단계를 포함한다. 각각의 검출된 객체에 대해, 상기 방법은 로봇이 대응하는 검출된 객체에 대해 수행할 수 있는 행동(222)을 나타내는 상호작용 행동(222)을 작성하는 단계를 포함한다. 상기 방법은 또한 각각의 검출된 객체의 개개의 상호작용 행동을 반영하도록 환경의 위치추정 맵(182)을 증강시키는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.08.04</internationOpenDate><internationOpenNumber>WO2022164793</internationOpenNumber><internationalApplicationDate>2022.01.25</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/013684</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 로봇(robot)(100)의 데이터 처리 하드웨어(data processing hardware)(142, 162, 510)에 의해 실행될 때 데이터 처리 하드웨어(142, 162, 510)가 동작들을 수행하게 하는, 컴퓨터 구현 방법(400)으로서,상기 동작들은,상기 로봇(100) 주위의 환경(10)에 대한 센서 데이터(sensor data)(134)를 수신하는 동작 — 상기 센서 데이터(134)는 상기 로봇(100)의 하나 이상의 센서(sensor)들(132, 132a 내지 132n)에 의해 캡처됨 —;수신된 센서 데이터(134)를 사용하여 상기 환경(10)에서 하나 이상의 객체(object)들(212, 214)을 검출하는 동작;각각의 검출된 객체(212, 214)에 대해, 상기 로봇(100)이 대응하는 검출된 객체(212, 214)에 대해 수행할 수 있는 행동(222)을 나타내는 상호작용 행동(interaction behavior)(222)을 작성하는 동작; 및각각의 검출된 객체(212, 214)의 개개의 상호작용 행동(222)을 반영하도록 상기 환경(10)의 위치추정 맵(localization map)(182)을 증강시키는 동작을 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 동작들은, 각각의 검출된 객체(212, 214)에 대해, 상기 로봇(100)의 조작자(12)에 의해 선택 가능한 상호작용 행동(222)의 그래픽 표현(graphical representation)(202)을 생성하는 동작을 더 포함하며;상기 환경(10)의 위치추정 맵(182)을 증강시키는 동작은 각각의 검출된 객체(212, 214)의 상호작용 행동(222)의 그래픽 표현(202)을 반영하도록 상기 환경(10)의 위치추정 맵(182)을 증강시키는 동작을 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>3. 제2 항에 있어서,상기 동작들은, 상기 로봇(100)이 상기 환경(10)을 내비게이션하는(navigating) 동안에 개개의 검출된 객체(212, 214)와 마주치는 경우, 뷰포트(viewport)(24)에서 상기 개개의 검출된 객체(212, 214)와 연관된 개개의 그래픽 표현(202)을 표시하는 동작을 더 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>4. 제3 항에 있어서,상기 뷰포트(24)는 상기 로봇(100)의 하나 이상의 센서들(132, 132a 내지 132n)로부터의 시각적 피드(visual feed)를 표시하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>5. 제1 항 내지 제4 항 중 어느 한 항에 있어서,상기 동작들은, 상기 로봇(100)이 상기 환경(10)을 내비게이션하는 동안에 개개의 검출된 객체(212, 214)와 마주치는 경우,상기 개개의 검출된 객체(212, 214)와 연관된 개개의 상호작용 행동(222)의 선택(16)을 나타내는 조작자 입력 표시를 수신하는 동작; 및상기 개개의 상호작용 행동(222)을 실행하도록 상기 로봇(100)을 작동시키는 동작을 더 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>6. 제2 항 내지 제5 항 중 어느 한 항에 있어서,상기 그래픽 표현(202)은 상기 로봇(100)의 조작자(12)에 의해 선택 가능한 상기 대응하는 검출된 객체(212, 214)와 연관된 상호작용 행동들(222)의 메뉴(menu)를 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>7. 제1 항 내지 제6 항 중 어느 한 항에 있어서,상기 동작들은, 상기 로봇(100)이 상기 환경(10)을 내비게이션하는 동안에 개개의 검출된 객체(212, 214)와 마주치는 경우,상기 개개의 검출된 객체(212, 214)와 연관된 개개의 상호작용 행동(222)의 선택(16)을 수신하는 동작; 및상기 개개의 상호작용 행동(222)을 실행하도록 상기 로봇(100)을 작동시키는 동작을 더 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>8. 제1 항 내지 제7 항 중 어느 한 항에 있어서,상기 수신된 센서 데이터(134)를 사용하여 상기 환경(10)에서 상기 하나 이상의 객체들(212, 214)을 검출하는 동작은 상기 로봇(100)이 상호작용할 수 있는 객체들(212)을 인식하도록 구성된 객체 검출 모델(object detection model)(218)을 사용하여 상기 하나 이상의 객체들(212, 214)을 식별하는 동작을 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>9. 제1 항 내지 제8 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)은 개개의 객체(212, 214)에 기초하여 상기 로봇(100)이 상기 환경(10)에서 내비게이션해야 하는 방법을 나타내는 내비게이션 행동(navigational behavior)(222)에 대응하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>10. 제9 항에 있어서,상기 내비게이션 행동(222)은 상기 개개의 객체(212, 214)와 관련하여 상기 로봇(100)에 대한 개개의 포지셔닝(positioning)을 규정하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>11. 제1 항 내지 제10 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)은 로봇(100)이 상기 상호작용 행동(222)과 연관된 개개의 객체(212, 214)와 상호작용하는 경우에 가정해야 하는 상호작용 포즈(interaction pose)(P)를 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>12. 제1 항 내지 제11 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)을 작성하는 동작은 상기 로봇(100)의 조작자(12)로부터 상기 대응하는 검출된 객체(212, 214)에 대한 수동 작성 행동(222)을 수신하는 동작을 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>13. 제1 항 내지 제12 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)을 작성하는 동작은,상기 로봇(100)이 상호작용할 수 있는 객체들(212)의 리스트(list)(216)로부터의 객체(212)에 상기 대응하는 검출된 객체(212, 214)를 매칭시키는 동작; 및상기 대응하는 검출된 객체(212)와 매칭되는 상기 객체들(212)의 리스트(216)로부터의 객체(212)와 연관된 하나 이상의 사전구성된 상호작용 행동들(222)을 선택하는 동작을 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>14. 제1 항 내지 제13 항 중 어느 한 항에 있어서,상기 동작들은 상기 로봇(100)의 하나 이상의 센서들(132, 132a 내지 132n)에 의해 캡처된 상기 환경(10)에 대한 상기 센서 데이터(134)를 사용하여 상기 위치추정 맵(182)을 생성하는 동작을 더 포함하며, 상기 위치추정 맵(182)은 상기 로봇(100)이 상기 환경(10)에 대해 이동할 때 상기 환경(10) 내에서 상기 로봇(100)을 위치추정하기 위한 위치추정 기준점들로서 상기 환경(10)의 피처(feature)들을 포함하는,컴퓨터 구현 방법.</claim></claimInfo><claimInfo><claim>15. 로봇(100)에 있어서,몸체(110);상기 몸체(110) 상에 적어도 부분적으로 배치되고 하나 이상의 센서들(132, 132a 내지 132n)을 포함하는 센서 시스템(sensor system)(130);상기 센서 시스템(130)과 통신하는 데이터 처리 하드웨어(142, 162, 510); 및상기 데이터 처리 하드웨어(142, 162, 510)와 통신하는 메모리 하드웨어(memory hardware)(144, 164, 520)를 포함하며, 상기 메모리 하드웨어(144, 164, 520)는, 상기 데이터 처리 하드웨어(142, 162, 510)에서 실행될 때, 상기 데이터 처리 하드웨어(142, 162, 510)가 동작들을 수행하게 하는 명령들을 저장하고, 상기 동작들은, 상기 로봇(100) 주위의 환경(10)에 대한 센서 데이터(134)를 수신하는 동작— 상기 센서 데이터(134)는 상기 로봇(100)의 하나 이상의 센서들(132, 132a 내지 132n)에 의해 캡처됨 —;수신된 센서 데이터(134)를 사용하여 상기 환경(10)에서 하나 이상의 객체들(212, 214)을 검출하는 동작;각각의 검출된 객체(212, 214)에 대해, 상기 로봇(100)이 대응하는 검출된 객체(212, 214)에 대해 수행할 수 있는 행동(222)을 나타내는 상호작용 행동(222)을 작성하는 동작; 및각각의 검출된 객체(212, 214)의 개개의 상호작용 행동(222)을 반영하도록 상기 환경(10)의 위치추정 맵(182)을 증강시키는 동작을 포함하는,로봇.</claim></claimInfo><claimInfo><claim>16. 제15 항에 있어서,상기 동작들은, 각각의 검출된 객체(212, 214)에 대해, 상기 로봇(100)의 조작자(12)에 의해 선택 가능한 상호작용 행동(222)의 그래픽 표현(202)을 생성하는 동작을 더 포함하며;상기 환경(10)의 위치추정 맵(182)을 증강시키는 동작은 각각의 검출된 객체(212, 214)의 상호작용 행동(222)의 그래픽 표현(202)을 반영하도록 상기 환경(10)의 위치추정 맵(182)을 증강시키는 동작을 포함하는,로봇.</claim></claimInfo><claimInfo><claim>17. 제16 항에 있어서,상기 동작들은, 상기 로봇(100)이 상기 환경(10)을 내비게이션하는 동안에 개개의 검출된 객체(212, 214)와 마주치는 경우, 뷰포트(24)에서 상기 개개의 검출된 객체(212, 214)와 연관된 개개의 그래픽 표현(202)을 표시하는 동작을 더 포함하는,로봇.</claim></claimInfo><claimInfo><claim>18. 제17 항에 있어서,상기 뷰포트(24)는 상기 로봇(100)의 하나 이상의 센서들(132, 132a 내지 132n)로부터의 시각적 피드를 표시하는,로봇.</claim></claimInfo><claimInfo><claim>19. 제15 항 내지 제18 항 중 어느 한 항에 있어서,상기 동작들은, 상기 로봇(100)이 상기 환경(10)을 내비게이션하는 동안에 개개의 검출된 객체(212, 214)와 마주치는 경우,상기 개개의 검출된 객체(212, 214)와 연관된 개개의 상호작용 행동(222)의 선택(16)을 나타내는 조작자 입력 표시를 수신하는 동작; 및상기 개개의 상호작용 행동(222)을 실행하도록 상기 로봇(100)을 작동시키는 동작을 더 포함하는,로봇.</claim></claimInfo><claimInfo><claim>20. 제16 항 내지 제19 항 중 어느 한 항에 있어서,상기 그래픽 표현(202)은 상기 로봇(100)의 조작자(12)에 의해 선택 가능한 상기 대응하는 검출된 객체(212, 214)와 연관된 상호작용 행동들(222)의 메뉴를 포함하는,로봇.</claim></claimInfo><claimInfo><claim>21. 제15 항 내지 제20 항 중 어느 한 항에 있어서,상기 동작들은, 상기 로봇(100)이 상기 환경(10)을 내비게이션하는 동안에 개개의 검출된 객체(212, 214)와 마주치는 경우,상기 개개의 검출된 객체(212, 214)와 연관된 개개의 상호작용 행동(222)의 선택(16)을 수신하는 동작; 및상기 개개의 상호작용 행동(222)을 실행하도록 상기 로봇(100)을 작동시키는 동작을 더 포함하는,로봇.</claim></claimInfo><claimInfo><claim>22. 제15 항 내지 제21 항 중 어느 한 항에 있어서,상기 수신된 센서 데이터(134)를 사용하여 상기 환경(10)에서 상기 하나 이상의 객체들(212, 214)을 검출하는 동작은 상기 로봇(100)이 상호작용할 수 있는 객체들(212)을 인식하도록 구성된 객체 검출 모델(218)을 사용하여 상기 하나 이상의 객체들(212, 214)을 식별하는 동작을 포함하는,로봇.</claim></claimInfo><claimInfo><claim>23. 제15 항 내지 제22 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)은 개개의 객체(212, 214)에 기초하여 상기 로봇(100)이 상기 환경(10)에서 내비게이션해야 하는 방법을 나타내는 내비게이션 행동(222)에 대응하는,로봇.</claim></claimInfo><claimInfo><claim>24. 제23 항에 있어서,상기 내비게이션 행동(222)은 상기 개개의 객체(212, 214)와 관련하여 상기 로봇(100)에 대한 개개의 포지셔닝을 규정하는,로봇.</claim></claimInfo><claimInfo><claim>25. 제15 항 내지 제24 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)은 로봇(100)이 상기 상호작용 행동(222)과 연관된 개개의 객체(212, 214)와 상호작용하는 경우에 가정해야 하는 상호작용 포즈(P)를 포함하는,로봇.</claim></claimInfo><claimInfo><claim>26. 제15 항 내지 제25 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)을 작성하는 동작은 상기 로봇(100)의 조작자(12)로부터 상기 대응하는 검출된 객체(212, 214)에 대한 수동 작성 행동(222)을 수신하는 동작을 포함하는,로봇.</claim></claimInfo><claimInfo><claim>27. 제15 항 내지 제26 항 중 어느 한 항에 있어서,상기 상호작용 행동(222)을 작성하는 동작은,상기 로봇(100)이 상호작용할 수 있는 객체들(212)의 리스트(216)로부터의 객체(212)에 상기 대응하는 검출된 객체(212, 214)를 매칭시키는 동작; 및상기 대응하는 검출된 객체(212)와 매칭되는 상기 객체들(212)의 리스트(216)로부터의 객체(212)와 연관된 하나 이상의 사전구성된 상호작용 행동들(222)을 선택하는 동작을 포함하는,로봇.</claim></claimInfo><claimInfo><claim>28. 제15 항 내지 제27 항 중 어느 한 항에 있어서,상기 동작들은 상기 로봇(100)의 하나 이상의 센서들(132, 132a 내지 132n)에 의해 캡처된 상기 환경(10)에 대한 상기 센서 데이터(134)를 사용하여 상기 위치추정 맵(182)을 생성하는 동작을 더 포함하며, 상기 위치추정 맵(182)은 상기 로봇(100)이 상기 환경(10)에 대해 이동할 때 상기 환경(10) 내에서 상기 로봇(100)을 위치추정하기 위한 위치추정 기준점들로서 상기 환경(10)의 피처들을 포함하는,로봇.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 매사추세츠주 월섬 스미스 스트리트 *** 유닛 ****</address><code>520200430545</code><country>미국</country><engName>Boston Dynamics, Inc.</engName><name>보스턴 다이나믹스, 인크.</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 매사추세츠...</address><code> </code><country> </country><engName>BOLLINI, Mario</engName><name>볼리니, 마리오</name></inventorInfo><inventorInfo><address>미국 ***** 매사추세츠...</address><code> </code><country> </country><engName>HEPLER, Leland</engName><name>헤플러, 리랜드</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 중구 서소문로**(서소문동, 정안빌딩*층)</address><code>920121001826</code><country>대한민국</country><engName>NAM &amp; NAM</engName><name>특허법인 남앤남</name></agentInfo><agentInfo><address>서울 중구 서소문로 **, *층(서소문동)</address><code>920241000417</code><country>대한민국</country><engName>NAM IP GROUP</engName><name>특허법인(유)남아이피그룹</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.01.29</priorityApplicationDate><priorityApplicationNumber>63/143,053</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.08.22</receiptDate><receiptNumber>1-1-2023-0922488-13</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.09.04</receiptDate><receiptNumber>1-5-2023-0140348-03</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Appointment of Agent] Report on Agent (Representative)</documentEngName><documentName>[대리인선임]대리인(대표자)에 관한 신고서</documentName><receiptDate>2024.03.13</receiptDate><receiptNumber>1-1-2024-0281102-48</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.01.24</receiptDate><receiptNumber>1-1-2025-0102047-61</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2025.01.24</receiptDate><receiptNumber>1-1-2025-0102048-17</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237028472.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9367e202351212a42004c63707e32aa7b4ccd612e2ebeef12d96e126a1599aa2988e3c2cbb7946874e58418c1ade73ccfb9e212e40cb71aa2f</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfbd98389d5627f7213b130db0683bf924c1c97e46bd3a8f9e438e6f52968859dcc1b7da02b2cc1b44048be85d19ca47c576f58aba4cc9715d</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>