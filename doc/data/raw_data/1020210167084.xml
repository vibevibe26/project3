<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:04:00.40</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2021.11.29</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2021-0167084</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>이종 특징들의 통합 최적화 프레임워크를 이용한 영상 처리 방법 및 장치</inventionTitle><inventionTitleEng>METHOD AND APPARATUS OF IMAGE PROCESSING USING  INTEGRATED OPTIMIZATION FRAMEWORK OF HETEROGENEOUS  FEATURES</inventionTitleEng><openDate>2023.06.07</openDate><openNumber>10-2023-0079884</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.11.07</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 17/05</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/593</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 15/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 이종 특징들의 통합 프레임워크를 이용한 영상 처리 방법 및 장치가 제공된다. 일 실시예에 따르면, 그 방법은 타겟 공간을 센싱하여 제1 조밀도 레벨에 대응하는 제1 특징들을 포함하는 제1 특징 프레임들을 생성하고, 타겟 공간을 센싱하여 제2 조밀도 레벨에 대응하는 제2 특징들을 포함하는 제2 특징 프레임들을 생성하고, 제1 특징 프레임들 및 제2 특징 프레임들 적어도 일부에 대응하는 센서 포즈들을 추정하고, 센서 포즈들에 기초하여 제1 특징들의 제1 재투영 오차 및 제2 특징들의 제2 재투영 오차를 각각 계산하고, 제1 재투영 오차 및 제2 재투영 오차를 포함하는 통합 오차가 감소하도록, 센서 포즈들, 제1 특징들, 제2 특징들, 제1 특징들의 재투영 특징들, 제2 특징들의 재투영 특징들 중 적어도 일부를 조절하여, 최적화 결과를 획득하는 단계들을 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 타겟 공간을 센싱하여 제1 조밀도(density) 레벨에 대응하는 제1 특징들을 포함하는 제1 특징 프레임들을 생성하는 단계;상기 타겟 공간을 센싱하여 제2 조밀도 레벨에 대응하는 제2 특징들을 포함하는 제2 특징 프레임들을 생성하는 단계;상기 제1 특징 프레임들 및 상기 제2 특징 프레임들 적어도 일부에 대응하는 센서 포즈(sensor pose)들을 추정하는 단계;상기 센서 포즈들에 기초하여 상기 제1 특징들의 제1 재투영(reprojection) 오차 및 상기 제2 특징들의 제2 재투영 오차를 각각 계산하는 단계; 및상기 제1 재투영 오차 및 상기 제2 재투영 오차를 포함하는 통합 오차가 감소하도록, 상기 센서 포즈들, 상기 제1 특징들, 상기 제2 특징들, 상기 제1 특징들의 역투영 특징들, 상기 제2 특징들의 역투영 특징들 중 적어도 일부를 조절하여, 최적화 결과를 획득하는 단계를 포함하고,상기 제2 조밀도 레벨은 상기 제1 조밀도 레벨에 비해 더 높은,영상 처리 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 제1 재투영 오차 및 제2 재투영 오차를 각각 계산하는 단계는상기 센서 포즈들에 기초하여 상기 제1 특징들을 상기 타겟 공간에 대응하는 3차원 공간에 역투영(unprojection)하여 제1 공통 공간 특징들을 결정하는 단계;상기 센서 포즈들에 기초하여 상기 제1 공통 공간 특징들을 상기 제1 특징 프레임들에 대응하는 2차원 평면들에 재투영하여 제1 재투영 특징들을 결정하는 단계; 및상기 제1 특징들과 상기 제1 재투영 특징들 간의 오차에 기초하여 상기 제1 재투영 오차를 계산하는 단계를 포함하는, 영상 처리 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,상기 제1 재투영 오차 및 제2 재투영 오차를 각각 계산하는 단계는상기 센서 포즈들에 기초하여 상기 제2 특징들을 상기 타겟 공간에 대응하는 3차원 공간에 역투영(unprojection)하여 제2 공통 공간 특징들을 결정하는 단계;상기 센서 포즈들에 기초하여 상기 제2 공통 공간 특징들을 상기 제2 특징 프레임들에 대응하는 2차원 평면들에 재투영하여 제2 재투영 특징들을 결정하는 단계; 및상기 제2 특징들과 상기 제2 재투영 특징들 간의 오차에 기초하여 상기 제2 재투영 오차를 계산하는 단계를 포함하는, 영상 처리 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서,상기 제1 특징 프레임들은 제1 프레임 레이트로 생성되고,상기 제2 특징 프레임들은 제2 프레임 레이트로 생성되고,상기 제1 프레임 레이트는 상기 제2 프레임 레이트에 비해 더 큰,영상 처리 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서,상기 제1 특징들은 핸드 크래프트(hand-crafted) 특징에 해당하고,상기 제2 특징들은 깊이, 또는 딥 피처(deep feature)에 해당하는,영상 처리 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,상기 통합 오차는상기 제1 재투영 오차 및 상기 제2 재투영 오차의 합에 대응하는,영상 처리 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,상기 제1 특징 프레임들은 상기 제1 조밀도 레벨의 SLAM(simultaneous localization and mapping)에 이용되고,상기 제2 특징 프레임들은 상기 제2 조밀도 레벨의 3차원 공간 모델링에 이용되는,영상 처리 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 3차원 공간 모델링에 따른 공간 모델 내 타겟 지점에서의 조명 정보를 추정하는 단계; 및상기 조명 정보에 기초하여 상기 타겟 지점의 가상 객체를 렌더링하는 단계를 더 포함하는, 영상 처리 방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서,상기 공간 모델은 상기 공간 모델의 각 조명의 위치, 형태, 형상, 밝기, 색상, 및 HDR(high dynamic range) 정보 중 적어도 일부를 나타내는 조명 정보 채널을 포함하고,상기 조명 정보를 추정하는 단계는상기 조명 정보 채널을 이용하여 상기 조명 정보를 추정하는 단계를 포함하는,영상 처리 방법.</claim></claimInfo><claimInfo><claim>10. 하드웨어와 결합되어 제1항 내지 제9항 중 어느 하나의 항의 방법을 실행시키기 위하여 컴퓨터 판독 가능한 기록매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>11. 프로세서; 및상기 프로세서에서 실행가능한 명령어들을 포함하는 메모리를 포함하고,상기 명령어들이 상기 프로세서에서 실행되면, 상기 프로세서는타겟 공간을 센싱하여 제1 조밀도(density) 레벨에 대응하는 제1 특징들을 포함하는 제1 특징 프레임들을 생성하고,상기 타겟 공간을 센싱하여 제2 조밀도 레벨에 대응하는 제2 특징들을 포함하는 제2 특징 프레임들을 생성하고,상기 제1 특징 프레임들 및 상기 제2 특징 프레임들 적어도 일부에 대응하는 센서 포즈(sensor pose)들을 추정하고,상기 센서 포즈들에 기초하여 상기 제1 특징들의 제1 재투영(reprojection) 오차 및 상기 제2 특징들의 제2 재투영 오차를 각각 계산하고,상기 제1 재투영 오차 및 상기 제2 재투영 오차를 포함하는 통합 오차가 감소하도록, 상기 센서 포즈들, 상기 제1 특징들, 상기 제2 특징들, 상기 제1 특징들의 역투영 특징들, 상기 제2 특징들의 역투영 특징들 중 적어도 일부를 조절하여, 최적화 결과를 획득하고,상기 제2 조밀도 레벨은 상기 제1 조밀도 레벨에 비해 더 높은,영상 처리 장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 프로세서는상기 센서 포즈들에 기초하여 상기 제1 특징들을 상기 타겟 공간에 대응하는 3차원 공간에 역투영(unprojection)하여 제1 공통 공간 특징들을 결정하고,상기 센서 포즈들에 기초하여 상기 제1 공통 공간 특징들을 상기 제1 특징 프레임들에 대응하는 2차원 평면들에 재투영하여 제1 재투영 특징들을 결정하고,상기 제1 특징들과 상기 제1 재투영 특징들 간의 오차에 기초하여 상기 제1 재투영 오차를 계산하는,영상 처리 장치.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 프로세서는상기 센서 포즈들에 기초하여 상기 제2 특징들을 상기 타겟 공간에 대응하는 3차원 공간에 역투영(unprojection)하여 제2 공통 공간 특징들을 결정하고,상기 센서 포즈들에 기초하여 상기 제2 공통 공간 특징들을 상기 제2 특징 프레임들에 대응하는 2차원 평면들에 재투영하여 제2 재투영 특징들을 결정하고,상기 제2 특징들과 상기 제2 재투영 특징들 간의 오차에 기초하여 상기 제2 재투영 오차를 계산하는,영상 처리 장치.</claim></claimInfo><claimInfo><claim>14. 제11항에 있어서,상기 제1 특징 프레임들은 제1 프레임 레이트로 생성되고,상기 제2 특징 프레임들은 제2 프레임 레이트로 생성되고,상기 제1 프레임 레이트는 상기 제2 프레임 레이트에 비해 더 큰,영상 처리 장치.</claim></claimInfo><claimInfo><claim>15. 제11항에 있어서,상기 제1 특징들은 핸드 크래프트(hand-crafted) 특징에 해당하고,상기 제2 특징들은 깊이, 또는 딥 피처(deep feature)에 해당하는,영상 처리 장치.</claim></claimInfo><claimInfo><claim>16. 제11항에 있어서,상기 제2 특징 프레임들은 상기 제2 조밀도 레벨의 3차원 공간 모델링에 이용되고,상기 3차원 공간 모델링에 따른 공간 모델은 상기 공간 모델의 각 조명의 위치, 형태, 형상, 밝기, 색상, 및 HDR(high dynamic range) 정보 중 적어도 일부를 나타내는 조명 정보 채널을 포함하고,상기 프로세서는상기 조명 정보 채널을 이용하여 상기 공간 모델 내 타겟 지점에서의 조명 정보를 추정하고,상기 조명 정보에 기초하여 상기 타겟 지점의 가상 객체를 렌더링하는,영상 처리 장치.</claim></claimInfo><claimInfo><claim>17. 타겟 공간을 센싱하여 센싱 데이터를 생성하는 적어도 하나의 센서; 및상기 센싱 데이터에 기초하여 제1 조밀도(density) 레벨에 대응하는 제1 특징들을 포함하는 제1 특징 프레임들을 생성하고,상기 센싱 데이터에 기초하여 제2 조밀도 레벨에 대응하는 제2 특징들을 포함하는 제2 특징 프레임들을 생성하고,상기 제1 특징 프레임들 및 상기 제2 특징 프레임들 적어도 일부에 대응하는 센서 포즈(sensor pose)들을 추정하고,상기 센서 포즈들에 기초하여 상기 제1 특징들의 제1 재투영(reprojection) 오차 및 상기 제2 특징들의 제2 재투영 오차를 각각 계산하고,상기 제1 재투영 오차 및 상기 제2 재투영 오차를 포함하는 통합 오차가 감소하도록, 상기 센서 포즈들, 상기 제1 특징들, 상기 제2 특징들, 상기 제1 특징들의 역투영 특징들, 상기 제2 특징들의 역투영 특징들 중 적어도 일부를 조절하여, 최적화 결과를 획득하는, 프로세서를 포함하고,상기 제2 조밀도 레벨은 상기 제1 조밀도 레벨에 비해 더 높은,전자 장치.</claim></claimInfo><claimInfo><claim>18. 제17항에 있어서,상기 프로세서는상기 센서 포즈들에 기초하여 상기 제1 특징들을 상기 타겟 공간에 대응하는 3차원 공간에 역투영(unprojection)하여 제1 공통 공간 특징들을 결정하고,상기 센서 포즈들에 기초하여 상기 제1 공통 공간 특징들을 상기 제1 특징 프레임들에 대응하는 2차원 평면들에 재투영하여 제1 재투영 특징들을 결정하고,상기 제1 특징들과 상기 제1 재투영 특징들 간의 오차에 기초하여 상기 제1 재투영 오차를 계산하고,상기 센서 포즈들에 기초하여 상기 제2 특징들을 상기 타겟 공간에 대응하는 3차원 공간에 역투영(unprojection)하여 제2 공통 공간 특징들을 결정하고,상기 센서 포즈들에 기초하여 상기 제2 공통 공간 특징들을 상기 제2 특징 프레임들에 대응하는 2차원 평면들에 재투영하여 제2 재투영 특징들을 결정하고,상기 제2 특징들과 상기 제2 재투영 특징들 간의 오차에 기초하여 상기 제2 재투영 오차를 계산하는,전자 장치.</claim></claimInfo><claimInfo><claim>19. 제17항에 있어서,상기 제1 특징 프레임들은 제1 프레임 레이트로 생성되고,상기 제2 특징 프레임들은 제2 프레임 레이트로 생성되고,상기 제1 특징들은 핸드 크래프트(hand-crafted) 특징에 해당하고,상기 제2 특징들은 깊이, 또는 딥 피처(deep feature)에 해당하고,상기 제1 프레임 레이트는 상기 제2 프레임 레이트에 비해 더 큰,영상 처리 장치.</claim></claimInfo><claimInfo><claim>20. 제17항에 있어서,상기 제2 특징 프레임들은 상기 제2 조밀도 레벨의 3차원 공간 모델링에 이용되고,상기 3차원 공간 모델링에 따른 공간 모델은 상기 공간 모델의 각 조명의 위치, 형태, 형상, 밝기, 색상, 및 HDR(high dynamic range) 정보 중 적어도 일부를 나타내는 조명 정보 채널을 포함하고,상기 프로세서는상기 조명 정보 채널을 이용하여 상기 공간 모델 내 타겟 지점에서의 조명 정보를 추정하고,상기 조명 정보에 기초하여 상기 타겟 지점의 가상 객체를 렌더링하는,영상 처리 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경상북도 포항시 남구...</address><code>420180163128</code><country>대한민국</country><engName>HA, Inwoo</engName><name>하인우</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2021.11.29</receiptDate><receiptNumber>1-1-2021-1378934-79</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2024.11.07</receiptDate><receiptNumber>1-1-2024-1222481-33</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Request for Prior Art Search</documentEngName><documentName>선행기술조사의뢰서</documentName><receiptDate>2025.07.10</receiptDate><receiptNumber>9-1-9999-9999999-89</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020210167084.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93e581c55c90ffd26821ede75ead1e7045b5728b8b7f620b357b86068c9d267d3f5265495f9801c431143f5c13272135681f9a23a49fb80b2e</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfb4c1e48a651fb0d66780ee443a0d0222bf1ecedf74856cadaa5cda58bbea3c3c69933aa50c4c2c2f101df2b80eeb070e36f5e9a754f8103e</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>