<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:41.4141</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.08.31</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0115699</applicationNumber><claimCount>26</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>딥러닝을 이용하여 프레임을 예측하는 방법</inventionTitle><inventionTitleEng>METHOD FOR PREDICTING FRAME USING DEEP LEARNING</inventionTitleEng><openDate>2025.03.07</openDate><openNumber>10-2025-0032627</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.08.31</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/09</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0455</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/096</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 개시의 일 실시예에 따라 컴퓨팅 장치에 의해 수행되는, 비디오 예측 모델을 활용하여 프레임을 예측하는 방법이 개시된다. 상기 방법은, 입력 프레임들과 타겟(target) 시간을 입력하는 단계와, 상기 입력 프레임들을 인코딩하여 관측 정보를 생성하는 단계와, 상기 타겟 시간에 기초하여 상기 입력 프레임들에 대한 예측 정보를 생성하는 단계와, 상기 관측 정보와 상기 예측 정보를 연결하는 단계와, 상기 연결된 정보에 기초하여 상기 타겟 시간의 출력 프레임을 예측하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 컴퓨팅 장치에 의해 수행되는, 비디오 예측 모델을 활용하여 프레임을 예측하는 방법으로서,입력 프레임들과 타겟(target) 시간을 입력하는 단계;상기 입력 프레임들을 인코딩하여 관측 정보를 생성하는 단계;상기 타겟 시간에 기초하여 상기 입력 프레임들에 대한 예측 정보를 생성하는 단계;상기 관측 정보와 상기 예측 정보를 연결하는 단계; 및상기 연결된 정보에 기초하여 상기 타겟 시간의 출력 프레임을 예측하는 단계를 포함하는,방법.</claim></claimInfo><claimInfo><claim>2. 제 1 항에 있어서,상기 타겟 시간의 출력 프레임을 예측하는 단계는,상기 타겟 시간에 대해 위치 인코딩을 수행하여 시간 임베딩 벡터를 생성하는 단계; 및상기 연결된 정보와 상기 시간 임베딩 벡터에 기초하여 상기 타겟 시간의 출력 프레임을 예측하는 단계를 포함하는,방법.</claim></claimInfo><claimInfo><claim>3. 제 1 항에 있어서,상기 출력 프레임을 인코딩하여 예측 결과를 생성하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>4. 제 3 항에 있어서,상기 관측 정보는 입력 큐에 저장되고,상기 예측 결과는 예측 큐에 저장되는,방법.</claim></claimInfo><claimInfo><claim>5. 제 4 항에 있어서,상기 비디오 예측 모델의 제 1 인코더를 이용하여 상기 관측 정보를 생성하고,상기 비디오 예측 모델의 제 2 인코더를 이용하여 상기 예측 결과를 생성하며,상기 제 1 인코더와 상기 제 2 인코더는 서로 다른 방식으로 학습된 것인방법.</claim></claimInfo><claimInfo><claim>6. 제 5 항에 있어서,상기 제 2 인코더는 사전 학습된 상기 제 1 인코더를 이용하여 상기 출력 프레임에 대응하는 정답 프레임을 인코딩하는 단계; 및상기 인코딩 된 정답 프레임과 상기 예측 결과 사이의 오차를 계산하는 단계에 기초하여 학습된 것인,방법.</claim></claimInfo><claimInfo><claim>7. 제 1 항에 있어서,상기 타겟 시간에 기초하여 상기 입력 프레임들에 대한 예측 정보를 생성하는 단계는,상기 타겟 시간 이전의 예측 결과들 중에서 적어도 하나를 이용하여, 상기 입력 프레임들에 대한 예측 정보를 생성하는 단계를 포함하는,방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 입력 프레임들에 대한 예측 정보는,상기 타겟 시간 이전의 예측 결과들 중에서 랜덤으로 선택된 예측 결과들의 조합인,방법.</claim></claimInfo><claimInfo><claim>9. 제8항에 있어서,상기 타겟 시간의 출력 프레임을 예측하는 단계는,상기 입력 프레임들에 대한 예측 정보에 따라 상기 타겟 시간의 출력 프레임이 서로 다르게 예측되는,방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,주변 시공간 정보를 집계하여 상기 예측된 출력 프레임을 정제하는 단계를 더 포함하는,방법.</claim></claimInfo><claimInfo><claim>11. 컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램으로서, 상기 컴퓨터 프로그램은 하나 이상의 프로세서에서 실행되는 경우, 비디오 예측 모델을 활용하여 프레임을 예측하기 위한 이하의 동작들을 수행하도록 하며, 상기 동작들은:입력 프레임들과 타겟(target) 시간을 입력하는 동작;상기 입력 프레임들을 인코딩하여 관측 정보를 생성하는 동작;상기 타겟 시간에 기초하여 상기 입력 프레임들에 대한 예측 정보를 생성하는 동작;상기 관측 정보와 상기 예측 정보를 연결하는 동작; 및상기 연결된 정보를 이용하여 상기 타겟 시간의 출력 프레임을 예측하는 동작을 포함하는,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>12. 제 11 항에 있어서,상기 타겟 시간의 출력 프레임을 예측하는 동작은,상기 타겟 시간에 대해 위치 인코딩을 수행하여 시간 임베딩 벡터를 생성하는 동작; 및상기 연결된 정보와 상기 시간 임베딩 벡터에 기초하여 상기 타겟 시간의 출력 프레임을 예측하는 동작을 포함하는,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>13. 제 11 항에 있어서,상기 출력 프레임을 인코딩하여 예측 결과를 생성하는 동작을 더 포함하는,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>14. 제 13 항에 있어서,상기 관측 정보는 입력 큐에 저장되고,상기 예측 결과는 예측 큐에 저장되는,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>15. 제 14 항에 있어서,상기 비디오 예측 모델의 제 1 인코더를 이용하여 상기 관측 정보를 생성하고,상기 비디오 예측 모델의 제 2 인코더를 이용하여 상기 예측 결과를 생성하며,상기 제 1 인코더와 상기 제 2 인코더는 서로 다른 방식으로 학습된 것인컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>16. 제 15 항에 있어서,상기 제 2 인코더는 사전 학습된 상기 제 1 인코더를 이용하여 상기 출력 프레임에 대응하는 정답 프레임을 인코딩하는 동작; 및상기 인코딩 된 정답 프레임과 상기 예측 결과 사이의 오차를 계산하는 동작에 기초하여 학습된 것인,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>17. 제 11 항에 있어서,상기 타겟 시간에 기초하여 상기 입력 프레임들에 대한 예측 정보를 생성하는 동작은,상기 타겟 시간 이전의 예측 결과들 중에서 적어도 하나를 이용하여, 상기 입력 프레임들에 대한 예측 정보를 생성하는 동작을 포함하는,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>18. 제11항에 있어서,상기 입력 프레임들에 대한 예측 정보는,상기 타겟 시간 이전의 예측 결과들 중에서 랜덤으로 선택된 예측 결과들의 조합인,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>19. 컴퓨팅 장치로서,적어도 하나의 프로세서; 및메모리를 포함하고,상기 적어도 하나의 프로세서는,입력 프레임들과 타겟(target) 시간을 입력하고,상기 입력 프레임들을 인코딩하여 관측 정보를 생성하고,상기 타겟 시간에 기초하여 상기 입력 프레임들에 대한 예측 정보를 생성하고,상기 관측 정보와 상기 예측 정보를 연결하고, 그리고상기 연결된 정보를 이용하여 상기 타겟 시간의 출력 프레임을 예측하도록 구성되는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>20. 제 19 항에 있어서,상기 타겟 시간에 대해 위치 인코딩을 수행하여 시간 임베딩 벡터를 생성하고, 그리고,상기 연결된 정보와 상기 시간 임베딩 벡터에 기초하여 상기 타겟 시간의 출력 프레임을 예측하도록 추가로 구성되는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>21. 제 19 항에 있어서,상기 출력 프레임을 인코딩하여 예측 결과를 생성하도록 추가로 구성되는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>22. 제 21 항에 있어서,상기 관측 정보는 입력 큐에 저장되고,상기 예측 결과는 예측 큐에 저장되는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>23. 제 22 항에 있어서,제 1 인코더를 이용하여 상기 관측 정보를 생성하고,제 2 인코더를 이용하여 상기 예측 결과를 생성하며,상기 제 1 인코더와 상기 제 2 인코더는 서로 다른 방식으로 학습된 것인컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>24. 제 23 항에 있어서,상기 제 2 인코더는 사전 학습된 상기 제 1 인코더를 이용하여 상기 출력 프레임에 대응하는 정답 프레임을 인코딩하는 단계; 및상기 인코딩 된 정답 프레임과 상기 예측 결과 사이의 오차를 계산하는 단계에 기초하여 학습된 것인,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>25. 제 19 항에 있어서,상기 타겟 시간 이전의 예측 결과들 중에서 적어도 하나를 이용하여, 상기 입력 프레임들에 대한 예측 정보를 생성하도록 추가로 구성되는,컴퓨팅 장치.</claim></claimInfo><claimInfo><claim>26. 제 19 항에 있어서,상기 입력 프레임들에 대한 예측 정보는,상기 타겟 시간 이전의 예측 결과들 중에서 랜덤으로 선택된 예측 결과들의 조합인,컴퓨팅 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>대전광역시 유성구...</address><code>120190868043</code><country>대한민국</country><engName>SI Analytics Co., Ltd</engName><name>주식회사 에스아이에이</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>SEO, Minseok</engName><name>서민석</name></inventorInfo><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>LEE, Hakjin</engName><name>이학진</name></inventorInfo><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>SEO, Junghoon</engName><name>서정훈</name></inventorInfo><inventorInfo><address>대전광역시 유성구...</address><code> </code><country> </country><engName>KIM, Doyi</engName><name>김도이</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 강남대로 *** (논현동,어반하이브빌딩) **층(파이특허법률사무소)</address><code>920090037635</code><country>대한민국</country><engName>LEE, Dae Ho</engName><name>이대호</name></agentInfo><agentInfo><address>서울특별시 강남구 강남대로 *** (논현동,어반하이브빌딩) **층(파이특허법률사무소)</address><code>920120001378</code><country>대한민국</country><engName>Park, Gun Hong</engName><name>박건홍</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.08.31</receiptDate><receiptNumber>1-1-2023-0964671-46</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230115699.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9375e27486a6964472ccf85e18ea8818d10089a2abb28bca05a03dcd66b3b371af5cb873712518e07a4db362d13ea624f890f7d2ad8698cf84</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf7c6eea605f5b888645cce1d40f87344941ff93d21a3d36d85227ba494ccda3a77ceea0292e212f217da3a5f16bdd3b0b6b17ef9217d533c4</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>