<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:38.4138</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.05.13</applicationDate><applicationFlag>특허</applicationFlag><applicationNumber>10-2025-7008460</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>증강 현실 안내식 깊이 추정</inventionTitle><inventionTitleEng>AUGMENTED REALITY GUIDED DEPTH ESTIMATION</inventionTitleEng><openDate>2025.04.02</openDate><openNumber>10-2025-0046328</openNumber><originalApplicationDate>2022.05.13</originalApplicationDate><originalApplicationKind>국제출원/분할</originalApplicationKind><originalApplicationNumber>10-2023-7042783</originalApplicationNumber><originalExaminationRequestDate>2025.03.14</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.03.14</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G06F 3/0346</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo><familyApplicationNumber>1020237042783</familyApplicationNumber></familyInfo></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> AR-안내식 깊이 추정을 위한 방법이 설명된다. 본 방법은 증강 현실(AR) 디바이스의 제1 자세에 기초하여 생성되는 제1 프레임에서 렌더링된 가상 객체를 식별하는 단계, AR 디바이스의 제2 자세를 결정하는 단계- 제2 자세는 제1 자세에 후속함 -, 제1 프레임에서 렌더링된 가상 객체 및 제2 자세에 기초하여 제2 프레임에서 증강 영역을 식별하는 단계, 제2 프레임에서 증강 영역에 대한 깊이 정보를 결정하는 단계, 및 깊이 정보에 기초하여 제2 프레임에서 가상 객체를 렌더링하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.11.24</internationOpenDate><internationOpenNumber>WO2022245649</internationOpenNumber><internationalApplicationDate>2022.05.13</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/029183</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 방법으로서,MR(mixed reality) 디바이스의 제1 자세(pose)를 결정하는 단계;상기 제1 자세에 기초하여, 제1 프레임을 생성하는 단계;상기 제1 프레임에서, 물리적 객체에 결합된 것처럼 보이는 가상 객체를 상기 물리적 객체에 대해 미리 정의된 포지션에서 렌더링하는 단계;상기 제1 자세 이후에, 상기 MR 디바이스의 제2 자세를 결정하는 단계;상기 제2 자세에 기초하여, 제2 프레임을 생성하는 단계;상기 제2 자세에서의 상기 MR 디바이스에 대한 상기 물리적 객체의 위치, 및 상기 물리적 객체에 대한 상기 미리 정의된 포지션에 기초하여 상기 제2 프레임에서 증강 영역을 식별하는 단계;상기 제2 프레임에서 상기 증강 영역으로 제한된 깊이 정보를 결정하는 단계; 및상기 제2 프레임에서, 상기 깊이 정보에 기초하여 상기 가상 객체를 렌더링하는 단계 - 상기 가상 객체는 상기 미리 정의된 위치에서 상기 물리적 객체에 결합된 것처럼 보임 -를 포함하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 제2 자세에서의 상기 MR 디바이스에 대한 상기 물리적 객체의 위치, 및 상기 물리적 객체에 대한 상기 미리 정의된 포지션에 기초하여 상기 가상 객체의 워핑 변환을 적용하는 단계; 및상기 워핑 변환에 기초하여 상기 제2 프레임 내의 상기 가상 객체의 투영된 위치를 식별하는 단계를 더 포함하고, 상기 증강 영역을 식별하는 단계는 상기 제2 프레임 내의 상기 가상 객체의 상기 투영된 위치에 기초하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서, 상기 가상 객체의 미리 구성된 동적 거동에 기초하여 상기 가상 객체의 투영된 경로를 식별하는 단계를 더 포함하고, 상기 제2 프레임 내의 상기 증강 영역은 상기 제2 프레임 내의 상기 가상 객체의 상기 투영된 경로에 기초한 투영된 위치를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 증강 영역의 크기는 상기 제2 프레임 내의 상기 가상 객체의 크기에 대응하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항에 있어서, 상기 제1 자세 및 상기 제2 자세를 결정하는 단계는 6DOF(six-degrees of freedom) 추적기에 기초하고, 상기 6DOF 추적기는 VIO(visual-inertial odometry) 시스템 또는 SLAM 시스템을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 상기 제2 자세를 결정하는 단계는:상기 MR 디바이스로부터 최신 IMU 데이터에 액세스하는 단계; 및상기 최신 IMU 데이터 및 상기 제1 자세에 기초하여 상기 제2 자세를 예측하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서, 상기 깊이 정보를 결정하는 단계는:상기 MR 디바이스의 깊이 센서에 액세스하는 단계- 상기 깊이 센서는 구조화된 광 센서, 비행 시간 센서, 수동 스테레오 센서, 및 초음파 디바이스 중 적어도 하나를 포함함 -를 포함하는 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,깊이 감지를 상기 증강 영역으로 제한하도록 상기 깊이 센서의 설정을 구성하는 단계를 추가로 포함하는 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서, 상기 깊이 정보를 결정하는 단계는:단안 이미지 또는 3D 재구성된 장면에 기초하여 깊이를 계산하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서, 상기 MR 디바이스는 증강 현실 안경을 포함하고, 상기 제1 프레임 및 상기 제2 프레임은 상기 물리적 객체를 묘사하지 않는, 방법.</claim></claimInfo><claimInfo><claim>11. 컴퓨팅 디바이스로서,그래픽 처리 유닛;프로세서; 및명령어들을 저장한 메모리를 포함하고, 상기 명령어들은, 상기 프로세서에 의해 실행될 때, 상기 컴퓨팅 디바이스가:MR(mixed reality) 디바이스의 제1 자세(pose)를 결정하는 것;상기 제1 자세에 기초하여, 제1 프레임을 생성하는 것;상기 제1 프레임에서, 물리적 객체에 결합된 것처럼 보이는 가상 객체를 상기 물리적 객체에 대해 미리 정의된 포지션에서 렌더링하는 것;상기 제1 자세 이후에, 상기 MR 디바이스의 제2 자세를 결정하는 것;상기 제2 자세에 기초하여, 제2 프레임을 생성하는 것;상기 제2 자세에서의 상기 MR 디바이스에 대한 상기 물리적 객체의 위치, 및 상기 물리적 객체에 대한 상기 미리 정의된 포지션에 기초하여 상기 제2 프레임에서 증강 영역을 식별하는 것;상기 제2 프레임에서 상기 증강 영역으로 제한된 깊이 정보를 결정하는 것; 상기 제2 프레임에서, 상기 깊이 정보에 기초하여 상기 가상 객체를 렌더링하는 것 - 상기 가상 객체는 상기 미리 정의된 위치에서 상기 물리적 객체에 결합된 것처럼 보임 -을 포함하는 동작들을 수행하게 하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서, 상기 동작들은,상기 제2 자세에서의 상기 MR 디바이스에 대한 상기 물리적 객체의 위치, 및 상기 물리적 객체에 대한 상기 미리 정의된 포지션에 기초하여 상기 가상 객체의 워핑 변환을 적용하는 것; 및상기 워핑 변환에 기초하여 상기 제2 프레임 내의 상기 가상 객체의 투영된 위치를 식별하는 것을 더 포함하고, 상기 증강 영역을 식별하는 것은 상기 제2 프레임 내의 상기 가상 객체의 상기 투영된 위치에 기초하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서, 상기 동작들은,상기 가상 객체의 미리 구성된 동적 거동에 기초하여 상기 가상 객체의 투영된 경로를 식별하는 것을 더 포함하고, 상기 제2 프레임 내의 상기 증강 영역은 상기 제2 프레임 내의 상기 가상 객체의 상기 투영된 경로에 기초한 투영된 위치를 포함하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>14. 제11항에 있어서, 상기 증강 영역의 크기는 상기 제2 프레임 내의 상기 가상 객체의 크기에 대응하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>15. 제11항에 있어서, 상기 제1 자세 및 상기 제2 자세를 결정하는 것은 6DOF(six-degrees of freedom) 추적기에 기초하고, 상기 6DOF 추적기는 VIO(visual-inertial odometry) 시스템 또는 SLAM 시스템을 포함하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>16. 제11항에 있어서, 상기 제2 자세를 결정하는 것은:상기 MR 디바이스로부터 최신 IMU 데이터에 액세스하는 것; 및상기 최신 IMU 데이터 및 상기 제1 자세에 기초하여 상기 제2 자세를 예측하는 것을 포함하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>17. 제11항에 있어서, 상기 깊이 정보를 결정하는 것은:상기 MR 디바이스의 깊이 센서에 액세스하는 것- 상기 깊이 센서는 구조화된 광 센서, 비행 시간 센서, 수동 스테레오 센서, 및 초음파 디바이스 중 적어도 하나를 포함함 -을 포함하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>18. 제17항에 있어서, 깊이 감지를 상기 증강 영역으로 제한하게 상기 깊이 센서의 설정을 구성하는 것을 추가로 포함하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>19. 제11항에 있어서, 상기 깊이 정보를 결정하는 것은:단안 이미지 또는 3D 재구성된 장면에 기초하여 깊이를 계산하는 것을 포함하는, 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>20. 비일시적 컴퓨터 판독가능 저장 매체로서, 상기 컴퓨터 판독가능 저장 매체는 명령어들을 포함하고, 상기 명령어들은, 컴퓨터에 의해 실행될 때, 상기 컴퓨터로 하여금:MR(mixed reality) 디바이스의 제1 자세(pose)를 결정하는 것;상기 제1 자세에 기초하여, 제1 프레임을 생성하는 것;상기 제1 프레임에서, 물리적 객체에 결합된 것처럼 보이는 가상 객체를 상기 물리적 객체에 대해 미리 정의된 포지션에서 렌더링하는 것;상기 제1 자세 이후에, 상기 MR 디바이스의 제2 자세를 결정하는 것;상기 제2 자세에 기초하여, 제2 프레임을 생성하는 것;상기 제2 자세에서의 상기 MR 디바이스에 대한 상기 물리적 객체의 위치, 및 상기 물리적 객체에 대한 상기 미리 정의된 포지션에 기초하여 상기 제2 프레임에서 증강 영역을 식별하는 것;상기 제2 프레임에서 상기 증강 영역으로 제한된 깊이 정보를 결정하는 것; 및상기 제2 프레임에서, 상기 깊이 정보에 기초하여 상기 가상 객체를 렌더링하는 것 - 상기 가상 객체는 상기 미리 정의된 위치에서 상기 물리적 객체에 결합된 것처럼 보임 -을 포함하는 동작들을 수행하게 하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 캘리포니아주 산타 모니카 써티퍼스트 스트리트 ****</address><code>520140253774</code><country>미국</country><engName>SNAP INC.</engName><name>스냅 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>그리스</country><engName>EVANGELIDIS, Georgios</engName><name>에반젤리디스, 게오르기오스</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>슬로바키아</country><engName>MICUSIK, Branislav</engName><name>미쿠식, 브라니슬라프</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>이스라엘</country><engName>KATZ, Sagi</engName><name>카츠, 사기</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920010003368</code><country>대한민국</country><engName>Kim Yeon Song</engName><name>김연송</name></agentInfo><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.11.18</priorityApplicationDate><priorityApplicationNumber>17/529,527</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.05.18</priorityApplicationDate><priorityApplicationNumber>63/189,980</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Divisional Application(International Application)] Patent Application</documentEngName><documentName>[분할출원(국제출원)]특허출원서</documentName><receiptDate>2025.03.14</receiptDate><receiptNumber>1-1-2025-0290579-47</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257008460.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c935c1c50069204212cbb24cb9f6b18fa1c15999560ff3d81869076f118437e382c35ba29906bdbdd4a0813ff0f43f5415ca947dc26d81c031b</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf8f9910ee3a8688e52a87b17d913f0a0a17fc482242257ac5ffa2e6f68de4a0fc95b604fbc3c15d4bfbe50f7c7fd8a4006fd0a2e864ef1d36</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>