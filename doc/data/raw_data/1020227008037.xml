<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:55:58.5558</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2020.06.25</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-7008037</applicationNumber><claimCount>16</claimCount><examinerName> </examinerName><finalDisposal>거절결정 후 재심사중</finalDisposal><inventionTitle>신경망의 아날로그 하드웨어 구현</inventionTitle><inventionTitleEng>ANALOG HARDWARE REALIZATION OF NEURAL NETWORKS</inventionTitleEng><openDate>2022.06.28</openDate><openNumber>10-2022-0088845</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2022.03.11</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2022.03.10</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/065</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/044</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/048</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 17/16</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 신경망의 아날로그 하드웨어 구현을 위한 시스템 및 방법이 제공된다. 방법은 신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하는 단계를 포함한다. 방법은 또한 신경망 토폴로지를 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계를 포함한다. 방법은 또한 훈련된 신경망의 가중치들에 기초하여 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하는 단계를 포함한다. 가중치 행렬의 각 요소는 등가 아날로그 네트워크의 아날로그 컴포넌트들 간 각각의 연결을 나타낸다. 방법은 또한 아날로그 컴포넌트들에 대한 컴포넌트 값들을 선택하는 것을 포함하여, 가중치 행렬에 기초하여 등가 아날로그 네트워크를 구현하기 위한 개략적 모델을 생성하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2021.12.30</internationOpenDate><internationOpenNumber>WO2021262023</internationOpenNumber><internationalApplicationDate>2020.06.25</internationalApplicationDate><internationalApplicationNumber>PCT/RU2020/000306</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 신경망의 하드웨어 구현 방법에 있어서, 신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하는 단계;상기 신경망 토폴로지를 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하는 단계로서, 상기 가중치 행렬의 각 요소는 상기 등가 아날로그 네트워크의 아날로그 컴포넌트들 간 각각의 연결을 나타내는, 상기 계산하는 단계; 및상기 아날로그 컴포넌트들에 대한 컴포넌트 값들을 선택하는 것을 포함하여, 상기 가중치 행렬에 기초하여 상기 등가 아날로그 네트워크를 구현하기 위한 개략적 모델을 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 개략적 모델을 생성하는 단계는 상기 가중치 행렬에 대한 저항 행렬을 생성하는 단계를 포함하며, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하며 저항 값을 나타내는, 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 훈련된 신경망에 대한 새로운 가중치들을 획득하는 단계;상기 새로운 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 새로운 가중치 행렬을 계산하는 단계; 및상기 새로운 가중치 행렬에 대한 새로운 저항 행렬을 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항에 있어서, 상기 신경망 토폴로지는 하나 이상의 뉴런 계층들을 포함하되, 각 뉴런 계층은 각각의 수학적 함수에 기초하여 각각의 출력들을 계산하며,상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계는, 상기 하나 이상의 뉴런 계층들의 각 계층에 대해,  상기 각각의 계층에 대해, 상기 각각의 수학적 함수에 기초하여, 하나 이상의 함수 블록들을 식별하는 단계로서, 각 함수 블록은 각각의 수학적 함수의 출력들에 부합하는 블록 출력들을 갖는 각각의 개략적인 구현을 갖는, 상기 식별하는 단계; 및  상기 하나 이상의 함수 블록들을 배열하는 것에 기초하여 각각의 아날로그 뉴런들의 다층 네트워크를 생성하는 단계로서, 각 아날로그 뉴런은 상기 하나 이상의 함수 블록들의 각각의 함수를 구현하고, 상기 다층 네트워크의 제1 계층의 각 아날로그 뉴런은 상기 다층 네트워크의 제2 계층의 하나 이상의 아날로그 뉴런들에 연결되는, 상기 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서, 상기 하나 이상의 함수 블록들은,블록 출력 을 갖는 가중 합산 블록으로서, ReLU는 정류된 선형 유닛(ReLU) 활성화 함수 또는 유사한 활성화 함수이고, Vi는 i번째 입력을 나타내며, wi는 i번째 입력에 대응하는 가중치를 나타내고, bias는 바이어스 값을 나타내며, ∑는 합산 연산자인, 상기 가중 합산 블록;블록 출력 를 갖는 신호 곱셈기 블록으로서, Vi는 i번째 입력을 나타내고, Vj는 j번째 입력을 나타내며, coeff는 미리 결정된 계수인, 상기 신호 곱셈기 블록; 블록 출력 을 갖는 시그모이드 활성화 블록으로서, V는 입력을 나타내고, A 및 B는 상기 시그모이드 활성화 블록의 미리 결정된 계수 값들인, 상기 시그모이드 활성화 블록;블록 출력 을 갖는 하이퍼볼록 탄젠트 활성화 블록으로서, Vin는 입력을 나타내고, A 및 B는 미리 결정된 계수 값들인, 상기 하이퍼볼록 탄젠트 활성화 블록; 및블록 출력 U(t) = V(t-dt)을 갖는 신호 지연 블록으로서, t는 현재 시간 기간을 나타내고, V(t-1)는 이전 시간 기간(t-1) 동안 상기 신호 지연의 출력을 나타내며, dt는 지연 값인, 상기 신호 지연 블록으로 구성된 그룹으로부터 선택된 하나 이상의 기본 함수 블록들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제4항에 있어서, 상기 하나 이상의 함수 블록들을 식별하는 단계는 상기 각각의 계층의 유형에 기초하여 상기 하나 이상의 함수 블록들을 선택하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서, 상기 신경망 토폴로지는 하나 이상의 뉴런 계층들을 포함하되, 각 뉴런 계층은 각각의 수학적 함수에 기초하여 각각의 출력들을 계산하며, 상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계는,하나 이상의 중간 수학적 함수들을 획득하기 위해 상기 제1 계층에 대응하는 수학적 함수를 분해하는 것을 포함하여, 상기 신경망 토폴로지의 제1 계층을 복수의 서브 계층들로 분해하는 단계로서, 각 서브 계층은 중간 수학적 함수를 구현하는, 상기 분해하는 단계; 및상기 신경망 토폴로지의 상기 제1 계층의 각 서브 계층에 대해, 상기 각각의 서브 계층에 대해, 각각의 중간 수학적 함수에 기초하여, 하나 이상의 서브 함수 블록들을 선택하는 단계; 및 상기 하나 이상의 서브 함수 블록들을 배열하는 것에 기초하여 각각의 아날로그 뉴런들의 다층 아날로그 서브 네트워크를 생성하는 단계로서, 각 아날로그 뉴런은 상기 하나 이상의 서브 함수 블록들의 각각의 함수를 구현하고, 상기 다층 아날로그 서브 네트워크의 제1 계층의 각 아날로그 뉴런은 상기 다층 아날로그 서브 네트워크의 제2 계층의 하나 이상의 아날로그 뉴런들에 연결되는, 상기 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서, 상기 제1 계층에 대응하는 상기 수학적 함수는 하나 이상의 가중치들을 포함하며, 상기 수학적 함수를 분해하는 단계는 상기 하나 이상의 중간 함수들을 결합하여 상기 수학적 함수를 생성하도록 상기 하나 이상의 가중치들을 조정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 신경망 토폴로지의 하나 이상의 출력 계층들에 대한 디지털 컴포넌트들의 등가 디지털 네트워크를 생성하는 단계; 및상기 등가 아날로그 네트워크의 하나 이상의 계층들의 출력을 상기 디지털 컴포넌트들의 등가 디지털 네트워크에 연결하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서, 상기 아날로그 컴포넌트들은 복수의 연산 증폭기들 및 복수의 저항기들을 포함하며, 각 연산 증폭기는 상기 등가 아날로그 네트워크의 아날로그 뉴런을 나타내고, 각 저항기는 두 개의 아날로그 뉴런들 간 연결을 나타내는, 방법.</claim></claimInfo><claimInfo><claim>11. 제10항에 있어서, 상기 아날로그 컴포넌트들의 컴포넌트 값들을 선택하는 단계는 상기 복수의 저항기들에 대한 가능한 저항 값들을 식별하기 위해 경사 하강법을 수행하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 제1항에 있어서, 상기 신경망 토폴로지는 하나 이상의 GRU 또는 LSTM 뉴런들을 포함하며, 상기 신경망 토폴로지를 변환하는 단계는 상기 하나 이상의 GRU 또는 LSTM 뉴런들의 각 순환 연결에 대해 하나 이상의 신호 지연 블록들을 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서, 상기 하나 이상의 신호 지연 블록들은 상기 신경망 토폴로지에 대해 미리 결정된 입력 신호 주파수와 일치하는 주파수에서 활성화되는, 방법.</claim></claimInfo><claimInfo><claim>14. 제1항에 있어서, 상기 신경망 토폴로지는 무제한 활성화 함수들을 수행하는 하나 이상의 뉴런 계층들을 포함하며, 상기 신경망 토폴로지를 변환하는 단계는,상기 무제한 활성화 함수들을 제한 활성화 함수로 대체하는 단계; 및미리 결정된 하나 이상의 입력들에 대해, 상기 훈련된 신경망과 상기 등가 아날로그 네트워크 간의 출력의 차이가 최소화되도록 상기 등가 아날로그 네트워크의 연결들 또는 가중치들을 조정하는 단계로 구성되는 그룹으로부터 선택된 하나 이상의 변환들을 적용하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>15. 제2항에 있어서,상기 저항 행렬에 기초하여 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크를 구현하는 회로를 제조하기 위한 하나 이상의 리소그래픽 마스크들을 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서,상기 훈련된 신경망에 대한 새로운 가중치들을 획득하는 단계;상기 새로운 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 새로운 가중치 행렬을 계산하는 단계;상기 새로운 가중치 행렬에 대한 새로운 저항 행렬을 생성하는 단계; 및상기 새로운 저항 행렬에 기초하여 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크를 구현하는 상기 회로를 제조하기 위한 새로운 리소그래픽 마스크를 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>17. 제1항에 있어서, 상기 훈련된 신경망은 상기 가중치들을 생성하기 위해 소프트웨어 시뮬레이션들을 사용하여 훈련되는, 방법.</claim></claimInfo><claimInfo><claim>18. 신경망의 하드웨어 구현을 위한 시스템에 있어서, 하나 이상의 프로세서들;메모리를 포함하며,상기 메모리는 상기 하나 이상의 프로세서들에 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하며, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 상기 등가 아날로그 네트워크의 아날로그 컴포넌트들 간 각각의 연결을 나타내고;상기 아날로그 컴포넌트들에 대한 컴포넌트 값들을 선택하는 것을 포함하여, 상기 가중치 행렬에 기초하여 상기 등가 아날로그 네트워크를 구현하기 위한 개략적 모델을 생성하기 위한 인스트럭션들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>19. 하나 이상의 프로세서들을 가진 컴퓨터 시스템 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 상기 등가 아날로그 네트워크의 아날로그 컴포넌트들 간 각각의 연결을 나타내고;상기 아날로그 컴포넌트들에 대한 컴포넌트 값들을 선택하는 것을 포함하여, 상기 가중치 행렬에 기초하여 상기 등가 아날로그 네트워크를 구현하기 위한 개략적 모델을 생성하기 위한 인스트럭션들을 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>20. 신경망의 하드웨어 구현 방법에 있어서, 신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하는 단계;아날로그 집적 회로(IC) 설계 제약들에 기초하여 하나 이상의 연결 제약들을 계산하는 단계;상기 신경망 토폴로지를 상기 하나 이상의 연결 제약들을 충족하는 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계; 및상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가의 희소 연결 네트워크에 대한 가중치 행렬을 계산하는 단계로서, 상기 가중치 행렬의 각 요소는 상기 등가의 희소 연결 네트워크의 아날로그 컴포넌트들 간 각각의 연결을 나타내는, 상기 계산하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>21. 제20항에 있어서, 상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는,상기 하나 이상의 연결 제약들에 따라 가능한 입력 연결 정도(Ni) 및 출력 연결 정도(No)를 도출하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>22. 제21항에 있어서, 상기 신경망 토폴로지는 K개의 입력들과 L개의 출력들과 가중치 행렬(U)을 갖는 적어도 하나의 조밀 연결 계층을 포함하며,상기 적어도 하나의 조밀 연결 계층을 변환하는 단계는, K개의 입력들, L개의 출력들 및 개의 계층들을 갖되, 입력 연결 정도가 Ni를 초과하지 않고 출력 연결 정도가 No를 초과하지 않도록, 상기 등가의 희소 연결 네트워크를 구성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>23. 제21항에 있어서, 상기 신경망 토폴로지는 K개의 입력들과 L개의 출력들과 가중치 행렬(U)을 갖는 적어도 하나의 조밀 연결 계층을 포함하며,상기 적어도 하나의 조밀 연결 계층을 변환하는 단계는, K개의 입력들, L개의 출력들 및 개의 계층들을 갖되, 각 계층(m)은 대응되는 가중치 행렬(Um)로 표현되며, 여기서 연결들이 없는 경우(absent connections)는 0으로 표현되고, 입력 연결 정도가 Ni를 초과하지 않고, 출력 연결 정도가 No를 초과하지 않도록 하며, 방정식 는 미리 결정된 정밀도로 충족되는, 상기 등가의 희소 연결 네트워크를 구성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 제21항에 있어서, 상기 신경망 토폴로지는 K개의 입력들과 L개의 출력들, 최대 입력 연결 정도(Pi), 최대 출력 연결 정도(Po) 및 가중치 행렬(U)을 가지며, 연결들이 없는 경우는 0으로 표현되는, 단일의 희소 연결 계층을 포함하며,상기 단일의 희소 연결 계층을 변환하는 단계는, K개의 입력들, L개의 출력들, 및 개의 계층들을 갖되, 각 계층(m)은 대응되는 가중치 행렬(Um)로 표현되고, 여기서 연결들이 없는 경우는 0으로 표현되며, 입력 연결 정도가 Ni를 초과하지 않고, 출력 연결 정도가 No를 초과하지 않도록 하며, 방정식 는 미리 결정된 정밀도로 충족되는, 상기 등가의 희소 연결 네트워크를 구성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>25. 제21항에 있어서, 상기 신경망 토폴로지는 K개의 입력들과 L개의 출력들을 갖는 콘볼루션 계층을 포함하며,상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 콘볼루션 계층을 K개의 입력들, L개의 출력들, 최대 입력 연결 정도(Pi) 및 최대 출력 연결 정도(Po)를 갖되, Pi ≤ Ni 및 Po ≤ No인, 단일의 희소 연결 계층으로 분해하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>26. 제20항에 있어서, 상기 가중치 행렬을 사용하여 상기 등가의 희소 연결 네트워크를 구현하기 위한 개략적 모델을 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>27. 제20항에 있어서, 상기 신경망 토폴로지는 순환 신경 계층을 포함하며,상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 순환 신경 계층을 신호 지연 연결들을 갖는 하나 이상의 조밀 또는 희소 연결 계층들로 변환하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>28. 제20항에 있어서, 상기 신경망 토폴로지는 순환 신경 계층을 포함하며, 상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 순환 신경 계층을 여러 개의 계층들로 분해하는 단계로서, 상기 계층들 중 적어도 하나는 K개의 입력들과 L개의 출력들과 가중치 행렬(U)을 가지며, 여기서 연결들이 없는 경우 0으로 표현되는, 조밀 또는 희소 연결 계층과 동일한, 상기 분해하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>29. 제20항에 있어서, 상기 신경망 토폴로지는 K개의 입력들, 가중치 벡터(U∈RK) 및 활성화 함수(F)를 갖는 계산 뉴런이 있는 단일 계층 퍼셉트론을 포함하며, 상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 하나 이상의 연결 제약들에 따라 상기 등가의 희소 연결 네트워크에 대한 연결 정도(N)를 도출하는 단계; 방적식 을 사용하여 상기 등가의 희소 연결 네트워크에 대한 계층들의 수(m)를 계산하는 단계; 및 K 개의 입력들, m개의 계층들 및 연결 정도(N)를 갖는 상기 등가의 희소 연결 네트워크를 구성하는 단계로서, 상기 등가의 희소 연결 네트워크는 상기 m개의 계층들의 각 계층에 각각의 하나 이상의 아날로그 뉴런들을 포함하고, 제1 m-1개의 계층들의 각 아날로그 뉴런은 항등 변환(identity transform)을 구현하며, 마지막 계층의 아날로그 뉴런은 상기 단일 계층 퍼셉트론의 상기 계산 뉴런의 상기 활성화 함수(F)를 구현하는, 상기 구성하는 단계를 포함하며,상기 등가의 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계는, 상기 가중치 벡터(U)에 기초하여 방정식 시스템을 풀어서 상기 등가의 희소 연결 네트워크의 연결들에 대한 가중치 벡터(W)를 계산하는 단계로서, 상기 방정식 시스템은 S개의 변수들을 갖는 K개의 방정식들을 포함하며, S개는 방정식 을 사용하여 계산되는, 상기 계산하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>30. 제20항에 있어서, 상기 신경망 토폴로지는 K개의 입력들, L개의 계산 뉴런들이 있는 단일 계층 퍼셉트론, 및 상기 L개의 계산 뉴런들의 각 계산 뉴런에 대한 가중치 행(row)을 포함하는 가중치 행렬(V)을 포함하며,상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 하나 이상의 연결 제약들에 따라 상기 등가의 희소 연결 네트워크에 대한 연결 정도(N)를 도출하는 단계; 방정식 을 사용하여 상기 등가의 희소 연결 네트워크에 대한 계층들의 수(m)를 계산하는 단계; 상기 단일 계층 퍼셉트론을 L개의 단일 계층 퍼셉트론 네트워크들로 분해하는 단계로서, 각 단일 계층 퍼셉트론 네트워크는 상기 L개의 계산 뉴런들의 각각의 계산 뉴런을 포함하는, 상기 분해하는 단계;    상기 L개의 단일 계층 퍼셉트론 네트워크들의 각 단일 계층 퍼셉트론 네트워크에 대해,  상기 K개의 입력들, 상기 m개의 계층들 및 상기 연결 정도(N)를 갖는 상기 각각의 단일 계층 퍼셉트론 네트워크에 대한 각각의 등가의 피라미드형 서브 네트워크를 구성하는 단계로서, 상기 등가의 피라미드형 서브 네트워크는 상기 m개의 계층들의 각 계층에 하나 이상의 각각의 아날로그 뉴런들을 포함하고, 제1 m-1개의 계층들의 각 아날로그 뉴런은 항등 변환을 구현하며, 마지막 계층의 아날로그 뉴런은 상기 각각의 단일 계층 퍼셉트론에 대응하는 상기 각각의 계산 뉴런의 상기 활성화 함수를 구현하는, 상기 구성하는 단계; 및 L*K개의 입력들을 갖는 입력 벡터를 형성하기 위해 상기 L개의 단일 계층 퍼셉트론 네트워크들에 대한 각 등가의 피라미드형 서브 네트워크의 입력을 연결하는 것을 포함하여 각 등가의 피라미드형 서브 네트워크를 연결함으로써 상기 등가의 희소 연결 네트워크를 구성하는 단계를 포함하며,상기 등가의 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계는, 상기 L개의 단일 계층 퍼셉트론 네트워크들의 각 단일 계층 퍼셉트론 네트워크에 대해,  가중치 벡터(U = Vi)를 설정하는 단계로서, 상기 가중치 행렬(V)의 i번째 행은 상기 각각의 단일 계층 퍼셉트론 네트워크에 대응하는 상기 각각의 계산 뉴런에 대응하는, 상기 설정하는 단계; 및   상기 가중치 벡터(U)에 기초하여 방정식 시스템을 풀어서 상기 각각의 등가의 피라미드형 서브 네트워크의 연결들에 대한 가중치 벡터(Wi)를 계산하는 단계로서, 상기 방정식 시스템은 S개의 변수들을 갖는 K개의 방정식들을 포함하고, S는 방정식 을 사용하여 계산되는, 상기 계산하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>31. 제20항에 있어서, 상기 신경망 토폴로지는 K개의 입력들, S개의 계층들이 있는 다층 퍼셉트론을 포함하며, 상기 S개의 계층들의 각 계층(i)은 대응되는 계산 뉴런 세트(Li) 및 상기 Li개의 계산 뉴런들의 각 계산 뉴런에 대한 가중치 행을 포함하는 대응되는 가중치 행렬들(Vi)을 포함하며,상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 하나 이상의 연결 제약들에 따라 상기 등가의 희소 연결 네트워크에 대한 연결 정도(N)를 도출하는 단계; 상기 다층 퍼셉트론을 개의 단일 계층 퍼셉트론 네트워크들로 분해하는 단계로서, 각 단일 계층 퍼셉트론은 상기 Q개의 계산 뉴런들의 각각의 계산 뉴런을 포함하며, 상기 다층 퍼셉트론을 분해하는 단계는 상기 Q개의 계산 뉴런들에 의해 공유되는 상기 K개의 입력들 중 하나 이상을 복제하는 단계를 포함하는, 상기 분해하는 단계; 상기 Q개의 단일 계층 퍼셉트론 네트워크들의 각 단일 계층 퍼셉트론 네트워크에 대해,  방정식 을 사용하여 각각의 등가의 피라미드형 서브 네트워크에 대한 계층들의 수(m)를 계산하는 단계로서, Ki, j는 상기 다층 퍼셉트론의 상기 각각의 계산 뉴런에 대한 입력들의 수인, 상기 계산하는 단계; 및  상기 Ki, j개의 입력들, 상기 m개의 계층들 및 상기 연결 정도(N)를 갖는 상기 각각의 단일 계층 퍼셉트론 네트워크에 대한 상기 각각의 등가의 피라미드형 서브 네트워크를 구성하는 단계로서, 상기 등가의 피라미드형 서브 네트워크는 상기 m개의 계층들의 각 계층에 하나 이상의 각각의 아날로그 뉴런들을 포함하고, 제1 m-1개의 계층들의 각 아날로그 뉴런은 항등 변환을 구현하며, 마지막 계층의 아날로그 뉴런은 상기 각각의 단일 계층 퍼셉트론 네트워크에 대응하는 상기 각각의 계산 뉴런의 상기 활성화 함수를 구현하는, 상기 구성하는 단계; 및 Q*Ki, j개의 입력들을 갖는 입력 벡터를 형성하기 위해 상기 Q개의 단일 계층 퍼셉트론 네트워크들에 대한 각 등가의 피라미드형 서브 네트워크의 입력을 연결하는 것을 포함하여 각 등가의 피라미드형 서브 네트워크를 연결함으로써 상기 등가의 희소 연결 네트워크를 구성하는 단계를 포함하며,상기 등가의 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계는, 상기 Q개의 단일 계층 퍼셉트론 네트워크들의 각 단일 계층 퍼셉트론 네트워크에 대해,  가중치 벡터()를 설정하는 단계로서, 상기 가중치 행렬(V)의 i번째 행은 상기 각각의 단일 계층 퍼셉트론 네트워크에 대응되는 상기 각각의 계산 뉴런에 대응되며, j는 상기 다층 퍼셉트론의 상기 각각의 계산 뉴런의 상기 대응되는 계층인, 상기 설정하는 단계; 및  상기 가중치 벡터(U)에 기초하여 방정식 시스템을 풀어서 상기 각각의 등가의 피라미드형 서브 네트워크의 연결들에 대한 가중치 벡터(Wi)를 계산하는 단계로서, 상기 방정식 시스템은 S개의 변수들을 갖는 Ki, j개의 방정식들을 포함하며, S개는 방정식 을 사용하여 계산되는, 상기 계산하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>32. 제20항에 있어서, 상기 신경망 토폴로지는 K개의 입력들, S개의 계층들이 있는 콘볼루션 신경망(CNN)을 포함하며, 상기 S개의 계층들의 각 계층(i)은 대응되는 계산 뉴런 세트(Li) 및 상기 Li개의 계산 뉴런들의 각 계산 뉴런에 대한 가중치 행을 포함하는 대응되는 가중치 행렬들(Vi)을 포함하며, 상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 하나 이상의 연결 제약들에 따라 상기 등가의 희소 연결 네트워크에 대한 연결 정도(N)를 도출하는 단계; 상기 CNN을  개의 단일 계층 퍼셉트론 네트워크들로 분해하는 단계로서, 각 단일 계층 퍼셉트론 네트워크는 상기 Q개의 각각의 계산 뉴런들을 포함하며, 상기 CNN을 분해하는 단계는 Q개의 계산 뉴런들에 의해 공유되는 상기 K개의 입력들의 하나 이상의 입력을 복제하는 단계를 포함하는, 상기 분해하는 단계; 상기 Q개의 단일 계층 퍼셉트론 네트워크들의 각 단일 계층 퍼셉트론 네트워크에 대해,  방정식 을 사용하여 각각의 등가의 피라미드형 서브 네트워크에 대한 계층들의 수(m)를 계산하는 단계로서, j는 상기 CNN의 상기 각각의 계산 뉴런의 상기 대응되는 계층이고, Ki, j는 상기 CNN의 상기 각각의 계산 뉴런에 대한 입력들의 수인, 상기 계산하는 단계; 및  상기 Ki, j 개의 입력들, 상기 m개의 계층들 및 상기 연결 정도(N)를 갖는 상기 각각의 단일 계층 퍼셉트론 네트워크에 대한 상기 각각의 등가의 피라미드형 서브 네트워크를 구성하는 단계로서, 상기 등가의 피라미드형 서브 네트워크는 상기 m개의 계층들의 각 계층에 하나 이상의 각각의 아날로그 뉴런들을 포함하고, 제1 m-1개의 계층들의 각 아날로그 뉴런은 항등 변환을 구현하며, 마지막 계층의 아날로그 뉴런은 상기 각각의 단일 계층 퍼셉트론 네트워크에 대응하는 상기 각각의 계산 뉴런의 상기 활성화 함수를 구현하는, 상기 구성하는 단계; 및 Q*Ki, j개의 입력들을 갖는 입력 벡터를 형성하기 위해 상기 Q개의 단일 계층 퍼셉트론 네트워크들에 대한 각 등가의 피라미드형 서브 네트워크의 입력을 연결하는 것을 포함하여 각 등가의 피라미드형 서브 네트워크를 연결함으로써 상기 등가의 희소 연결 네트워크를 구성하는 단계를 포함하며,상기 등가의 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계는, 상기 Q개의 단일 계층 퍼셉트론 네트워크들의 각 단일 계층 퍼셉트론 네트워크에 대해,  가중치 벡터()를 설정하는 단계로서, 상기 가중치 행렬(V)의 상기 i번째 행은 상기 각각의 단일 계층 퍼셉트론 네트워크에 대응하는 상기 각각의 계산 뉴런에 대응하며, j는 상기 CNN의 상기 각각의 계산 뉴런의 상기 대응하는 계층인, 상기 설정하는 단계; 및  상기 가중치 벡터(U)에 기초하여 방정식 시스템을 풀어서 상기 각각의 등가의 피라미드형 서브 네트워크의 연결들에 대한 가중치 벡터(Wi)를 계산하는 단계로서, 상기 방정식 시스템은 S개의 변수들을 갖는 Ki, j개의 방정식들을 포함하며, S는 방정식 을 사용하여 계산되는, 상기 계산하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>33. 제20항에 있어서, 상기 신경망 토폴로지는 K개의 입력들, K개의 뉴런들을 갖는 계층(Lp), L개의 뉴런들을 갖는 계층(Ln), 및 가중치 행렬(W ∈ RL×K)을 포함하며, R은 실수 세트이고, 상기 계층(Lp)의 각 뉴런은 상기 계층(Ln)의 각 뉴런에 연결되며, 상기 계층(Ln)의 각 뉴런은 활성화 함수(F)를 수행하여, 상기 계층(Ln)의 출력이 입력(x)에 대한 방정식 을 사용하여 계산되도록 하며,  상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 하나 이상의 연결 제약들에 따라, 가능한 입력 연결 정도(NI 003e# 1) 및 가능한 출력 연결 정도(NO 003e# 1)를 도출하는 단계; 이라는 결정에 따라, 항등 활성화 함수를 수행하는 K개의 아날로그 뉴런들을 갖는 계층(LAp), 항등 활성화 함수를 수행하는  개의 아날로그 뉴런들을 갖는 계층(LAh) 및 상기 활성화 함수(F)를 수행하는 L개의 아날로그 뉴런들을 갖는 계층(LAo)을 포함하여, 상기 계층(LAp)의 각 아날로그 뉴런이 NO 개의 출력들을 갖고, 상기 계층(LAh)의 각 아날로그 뉴런은 NI개 미만의 입력들 및 NO개 미만의 출력들을 가지며, 상기 계층(LAo)의 각 아날로그 뉴런이 NI 개의 입력들을 갖도록 하는, 3-계층 아날로그 네트워크를 구성하는 단계를 포함하며,상기 등가의 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계는, 상기 계층(LAo)의 총 출력이 방정식 을 사용하여 계산되도록,  개의 변수들의  개의 방정식들을 포함하는 행렬 방정식  을 풀어서 희소 가중치 행렬들(Wo 및 Wh)을 생성하는 단계로서, 상기 희소 가중치 행렬(Wo ∈ RKХM)은 상기 계층(LAp)과 계층(LAh) 간 연결들을 나타내고, 상기 희소 가중치 행렬(Wh ∈ RMХL)은 상기 계층(LAh)과 계층(LAo) 간 연결들을 나타내는, 상기 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>34. 제33항에 있어서, 상기 사다리꼴 변환을 수행하는 단계는, 이라는 결정에 따라,  이도록 K'개의 뉴런들을 갖는 서브 계층(Lp1) 및 (K - K')개의 뉴런들을 갖는 서브 계층(Lp2)을 획득하기 위해 상기 계층(Lp)을 분할하는 단계; K'개의 뉴런들을 갖는 상기 서브 계층(Lp1)에 대해, 상기 구성하는 단계 및 생성하는 단계를 수행하는 단계; 및 K - K'개를 갖는 상기 서브 계층(Lp1)에 대해, 상기 분할하는 단계, 구성하는 단계 및 생성하는 단계를 반복적으로 수행하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>35. 제34항에 있어서, 상기 신경망 토폴로지는 다층 퍼셉트론 네트워크를 포함하며, 상기 방법은, 상기 다층 퍼셉트론 네트워크의 연속 계층 쌍에 대해, 반복적으로 상기 사다리꼴 변환을 수행하고 상기 등가의 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>36. 제34항에 있어서, 상기 신경망 토폴로지는 (i) 두 개의 완전 연결 계층들에 대한 선형 조합의 계산, (ii) 요소별 가산, 및 (iii) 비선형 함수 계산을 포함하는 순환 신경망(RNN)을 포함하며, 상기 방법은, (i) 상기 두 개의 완전 연결 계층들, 및 (ii) 상기 비선형 함수 계산에 대해, 상기 사다리꼴 변환을 수행하고 상기 등가 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>37. 제34항에 있어서, 상기 신경망 토폴로지는 (i) 복수의 완전 연결 계층들에 대한 선형 조합의 계산, (ii) 요소별 가산, (iii) 아다마르 곱, 및 (iv) 복수의 비선형 함수 계산들을 포함하는 장단기 메모리(LSTM) 네트워크 또는 게이트 순환 유닛(GRU) 네트워크를 포함하며, 상기 방법은, (i) 상기 복수의 완전 연결 계층들, 및 (ii) 상기 복수의 비선형 함수 계산들에 대해, 상기 사다리꼴 변환을 수행하고 상기 등가 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>38. 제34항에 있어서, 상기 신경망 토폴로지는 (i) 복수의 부분 연결 계층들 및 (ii) 하나 이상의 완전 연결 계층들()을 포함하는 콘볼루션 신경망(CNN)을 포함하며, 상기 방법은,가중치가 0인 누락 연결들을 삽입함으로써 상기 복수의 부분 연결 계층들을 등가의 완전 연결 계층들로 변환하는 단계; 상기 등가의 완전 연결 계층들 및 상기 하나 이상의 완전 연결 계층들의 각 연속 계층 쌍에 대해, 반복적으로 상기 사다리꼴 변환을 수행하고 상기 등가 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>39. 제20항에 있어서, 상기 신경망 토폴로지는 K개의 입력들, L개의 출력 뉴런들 및 가중치 행렬(U ∈ RL×K)을 포함하며, R은 실수 세트이고, 각 출력 뉴런은 활성화 함수(F)를 수행하며, 상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는,  하나 이상의 연결 제약들에 따라, 가능한 입력 연결 정도(NI 003e# 1) 및 가능한 출력 연결 정도(NO 003e# 1)를 도출하는 단계; 상기 세트 set 로부터 파라미터(p)를 선택하는 단계;P 003e# 0이라는 결정에 따라, 피라미드 신경망이 그 출력 계층에  개의 뉴런들을 갖도록 상기 등가의 희소 연결 네트워크의 제1 p개의 계층들을 형성하는 상기 피라미드 신경망을 구성하는 단계로서, 상기 피라미드 신경망의 각 뉴런은 항등 함수를 수행하는, 상기 구성하는 단계;Np개의 입력들과 L개의 출력들을 갖는 사다리꼴 신경망을 구성하는 단계로서, 상기 사다리꼴 신경망의 마지막 계층의 각 뉴런은 상기 활성화 함수(F)를 수행하고 모든 다른 뉴런들은 항등 함수를 수행하는, 상기 구성하는 단계를 포함하며,상기 등가의 희소 연결 네트워크에 대한 상기 가중치 행렬을 계산하는 단계는, (i) 다음의 규칙들 (a) , 여기서 C는 비제로 상수이고 인, 상기 규칙; 및 (b) ki를 제외한 상기 뉴런의 모든 가중치들 j에 대해,  인 규칙에 따라 상기 피라미드 신경망의 상기 제1 계층의 모든 뉴런(i)의 가중치들을 설정하는 단계; 및 (ii) 상기 피라미드 신경망의 모든 다른 가중치들을 1로 설정하는 단계를 포함하는 상기 피라미드 신경망에 대한 가중치들을 생성하는 단계; 및 (i) 방정식 에 따라 상기 사다리꼴 신경망의 상기 제1 계층의 각 뉴런(i)의 가중치들을 설정하는 단계; 및 (ii) 상기 사다리꼴 신경망의 다른 가중치들을 1로 설정하는 단계를 포함하는 상기 사다리꼴 신경망에 대한 가중치들을 생성하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>40. 제39항에 있어서, 상기 신경망 토폴로지는 K개의 입력들, S개의 계층들 및 i번째 계층의 Li=1,S개의 계산 뉴런들을 갖는 다층 퍼셉트론, 및 i번째 계층에 대한 가중치 행렬()을 포함하되, L0 = K이며, 상기 신경망 토폴로지를 상기 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하는 단계는, 상기 다층 퍼셉트론의 상기 S개의 계층들의 각 계층(j)에 대해,   Lj-1개의 입력들, Lj개의 출력 뉴런들 및 가중치 행렬(Uj)로 구성된 각각의 단일 계층 퍼셉트론에 대한 근사화 변환을 수행하여 각각의 피라미드-사다리꼴 네트워크(PTNNXj)를 구성하는 단계; 각 피라미드-사다리꼴 네트워크를 적층함으로써 상기 등가의 희소 연결 네트워크를 구성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>41. 신경망의 하드웨어 구현을 위한 시스템에 있어서, 하나 이상의 프로세서들;메모리를 포함하며,상기 메모리는 상기 하나 이상의 프로세서들에 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하며, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;아날로그 집적 회로(IC) 설계 제약들에 기초하여 하나 이상의 연결 제약들을 계산하고;상기 신경망 토폴로지를 상기 하나 이상의 연결 제약들을 충족하는 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가의 희소 연결 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 상기 등가의 희소 연결 네트워크의 아날로그 컴포넌트들 간 각각의 연결을 나타내기 위한 인스트럭션들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>42. 하나 이상의 프로세서들을 가진 컴퓨터 시스템 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;아날로그 집적 회로(IC) 설계 제약들에 기초하여 하나 이상의 연결 제약들을 계산하고;상기 신경망 토폴로지를 상기 하나 이상의 연결 제약들을 충족하는 아날로그 컴포넌트들의 등가의 희소 연결 네트워크로 변환하고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가의 희소 연결 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 상기 등가의 희소 연결 네트워크의 아날로그 컴포넌트들 간 각각의 연결을 나타내기 위한 인스트럭션들을 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>43. 신경망의 하드웨어 구현 방법에 있어서, 신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하는 단계;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계로서, 각 연산 증폭기는 상기 등가 아날로그 네트워크의 아날로그 뉴런을 나타내고, 각 저항기는 두 개의 아날로그 뉴런들 간 연결을 나타내는, 상기 변환하는 단계;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하는 단계로서, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내는, 상기 계산하는 단계; 및상기 가중치 행렬에 대한 저항 행렬을 생성하는 단계로서, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하며 저항 값을 나타내는, 상기 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>44. 제43항에 있어서, 상기 가중치 행렬에 대한 상기 저항 행렬을 생성하는 단계는,가능한 저항 값들의 미리 결정된 범위{Rmin, Rmax}를 획득하고 상기 미리 설명된 범위 내에서 초기 기본 저항 값(Rbase)을 선택하는 단계;상기 미리 결정된 범위 내에서, 저항 값들의 제한된 길이 세트를 선택하는 단계로서, 상기 저항 값들의 상기 제한된 길이 세트 내에서 {Ri, Rj}의 모든 조합들을 위한 범위 [-Rbase, Rbase] 내에서 가능한 가중치들 의 가장 균일한 분포를 제공하는, 상기 선택하는 단계; = 이 에 가장 가까운 저항기 세트 값이 되도록, 상기 등가 아날로그 네트워크의 각 뉴런 또는 각 계층의 인입 연결들의 최대 가중치 및 바이어스(wmax)에 기초하여, 상기 등가 아날로그 네트워크의 각 아날로그 뉴런에 대해 또는 각 계층에 대해, 상기 저항 값들의 제한된 길이 세트로부터, 저항 값  = 을 선택하는 단계; 및상기 가중치 행렬의 각 요소에 대해, 상기 미리 결정된 가능한 저항 값들의 범위 내에서 R1 및 R2의 모든 가능한 값들에 대해 방정식 에 따라 오차를 최소화하는 각각의 제1 저항 값(R1) 및 각각의 제2 저항 값(R2)을 선택하는 단계로서, w는 상기 가중치 행렬의 상기 각각의 요소이고, rerr는 저항들에 대한 미리 결정된 상대 허용 오차 값인, 상기 선택하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>45. 제44항에 있어서, 상기 가능한 저항 값들의 미리 결정된 범위는 100 K옴 내지 1 M옴 범위의 공칭 시리즈 E24에 따른 저항들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>46. 제44항에 있어서, 상기 등가 아날로그 네트워크의 각 계층에 대해 R+ 및 R-가 독립적으로 선택되는, 방법.</claim></claimInfo><claimInfo><claim>47. 제44항에 있어서, 상기 등가 아날로그 네트워크의 각 아날로그 뉴런에 대해 R+ 및 R-가 독립적으로 선택되는, 방법.</claim></claimInfo><claimInfo><claim>48. 제43항에 있어서, 상기 가중치 행렬의 제1 하나 이상의 가중치들 및 제1 하나 이상의 입력들은 상기 등가 아날로그 네트워크의 제1 연산 증폭기에 대한 하나 이상의 연결들을 나타내며, 상기 방법은,상기 저항 행렬을 생성하기 전에, 제1 값에 의해 상기 제1 하나 이상의 가중치들을 수정하는 단계; 및  활성화 함수를 수행하기 전에, 상기 제1 하나 이상의 가중치들과 상기 제1 하나 이상의 입력들의 선형 조합에 상기 제1 값을 곱하기 위한 상기 제1 연산 증폭기를 구성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>49. 제43항에 있어서,미리 결정된 가중치 범위를 획득하는 단계; 및상기 등가 아날로그 네트워크가 동일한 입력에 대해 상기 훈련된 신경망과 유사한 출력을 생성하도록 상기 미리 결정된 가중치 범위에 따라 상기 가중치 행렬을 업데이트하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>50. 제43항에 있어서, 상기 훈련된 신경망은 상기 신경망 토폴로지의 각 계층이 양자화된 가중치들을 갖도록 훈련되는, 방법.</claim></claimInfo><claimInfo><claim>51. 제43항에 있어서,상기 등가 아날로그 네트워크가 상기 훈련된 신경망과 비교하여 상이한 출력을 생성하도록 하는 상기 가중치들 또는 상기 저항 값들의 오차에 대한 민감도를 줄이기 위해 상기 훈련된 신경망을 재훈련시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>52. 제43항에 있어서,해당 계층에 대한 평균 절대 가중치보다 미리 결정된 임계값 이상만큼 더 큰 임의의 계층의 가중치를 최소화하기 위해 상기 훈련된 신경망을 재훈련시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>53. 신경망의 하드웨어 구현을 위한 시스템에 있어서, 하나 이상의 프로세서들;메모리를 포함하며,상기 메모리는 상기 하나 이상의 프로세서들에 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하며, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하되, 각 연산 증폭기는 상기 등가 아날로그 네트워크의 아날로그 뉴런을 나타내고, 각 저항기는 두 개의 아날로그 뉴런들 간 연결을 나타내고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내고;상기 가중치 행렬에 대한 저항 행렬을 생성하되, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하며 저항 값을 나타내기 위한 인스트럭션들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>54. 하나 이상의 프로세서들을 가진 컴퓨터 시스템 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하되, 각 연산 증폭기는 상기 등가 아날로그 네트워크의 아날로그 뉴런을 나타내고, 각 저항기는 두 개의 아날로그 뉴런들 간 연결을 나타내고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내고;상기 가중치 행렬에 대한 저항 행렬을 생성하되, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하며 저항 값을 나타내기 위한 인스트럭션들을 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>55. 신경망의 하드웨어 구현 방법에 있어서, 신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하는 단계;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계로서, 각 연산 증폭기는 상기 등가 아날로그 네트워크의 아날로그 뉴런을 나타내고, 각 저항기는 두 개의 아날로그 뉴런들 간 연결을 나타내는, 상기 변환하는 단계;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하는 단계로서, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내는, 상기 계산하는 단계;상기 가중치 행렬에 대한 저항 행렬을 생성하는 단계로서, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하는, 상기 생성하는 단계; 및아날로그 컴포넌트들의 최적화된 아날로그 네트워크를 획득하기 위해, 상기 저항 행렬에 기초하여, 상기 복수의 연산 증폭기들 또는 상기 복수의 저항기들의 수를 줄이도록 상기 등가 아날로그 네트워크를 프루닝하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>56. 제55항에 있어서, 상기 등가 아날로그 네트워크를 프루닝하는 단계는,저항 값들이 미리 결정된 최소 임계 저항 값 미만인 상기 저항 행렬의 하나 이상의 요소들에 대응되는 저항기들을 컨덕터들로 대체하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>57. 제55항에 있어서, 상기 등가 아날로그 네트워크를 프루닝하는 단계는,미리 결정된 최대 임계 저항 값을 초과하는 상기 저항 행렬의 하나 이상의 요소들에 대응하는 상기 등가 아날로그 네트워크의 하나 이상의 연결들을 제거하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>58. 제55항에 있어서, 상기 등가 아날로그 네트워크를 프루닝하는 단계는,대략 0인 상기 가중치 행렬의 하나 이상의 요소들에 대응하는 상기 등가 아날로그 네트워크의 하나 이상의 연결들을 제거하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>59. 제58항에 있어서, 상기 등가 아날로그 네트워크를 프루닝하는 단계는,어떠한 입력 연결들도 없이 상기 등가 아날로그 네트워크의 하나 이상의 아날로그 뉴런들을 제거하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>60. 제55항에 있어서, 상기 등가 아날로그 네트워크를 프루닝하는 단계는,하나 이상의 데이터 세트들에 대한 계산을 수행할 때 상기 아날로그 뉴런들의 사용을 검출하는 것에 기초하여 상기 등가 아날로그 네트워크의 아날로그 뉴런들을 랭크시키는 단계; 상기 랭킹에 기초하여 상기 등가 아날로그 네트워크의 하나 이상의 아날로그 뉴런들을 선택하는 단계; 및상기 등가 아날로그 네트워크로부터 상기 하나 이상의 아날로그 뉴런들을 제거하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>61. 제60항에 있어서, 상기 아날로그 뉴런들의 사용을 검출하는 단계는,모델링 소프트웨어를 사용하여 상기 등가 아날로그 네트워크의 모델을 구축하는 단계; 및상기 모델을 사용하여 아날로그 신호들의 전파를 측정하여 상기 하나 이상의 데이터 세트들에 대한 계산을 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>62. 제60항에 있어서, 상기 아날로그 뉴런들의 사용을 검출하는 단계는,모델링 소프트웨어를 사용하여 상기 등가 아날로그 네트워크의 모델을 구축하는 단계; 및상기 모델을 사용하여 상기 모델의 출력 신호들을 측정하여 상기 하나 이상의 데이터 세트들에 대한 계산을 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>63. 제60항에 있어서, 상기 아날로그 뉴런들의 사용을 검출하는 단계는,모델링 소프트웨어를 사용하여 상기 등가 아날로그 네트워크의 모델을 구축하는 단계; 및상기 모델을 사용하여 상기 아날로그 뉴런들에 의해 소모된 전력을 측정하여 상기 하나 이상의 데이터 세트들에 대한 계산을 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>64. 제55항에 있어서,상기 등가 아날로그 네트워크를 프루닝하는 단계 이후에 및 상기 등가 아날로그 네트워크를 구현하는 회로를 제조하기 위한 하나 이상의 리소그래픽 마스크들을 생성하기 전에, 상기 등가 아날로그 네트워크에 대한 상기 가중치 행렬을 재계산하고 상기 재계산된 가중치 행렬에 기초하여 상기 저항 행렬을 업데이트하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>65. 제55항에 있어서,상기 등가 아날로그 네트워크의 각 아날로그 뉴런에 대해, 상기 가중치 행렬을 계산하는 동안, 상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 각각의 아날로그 뉴런에 대한 각각의 바이어스 값을 계산하는 단계; 상기 각각의 바이어스 값이 미리 결정된 최대 바이어스 임계값을 초과한다는 결정에 따라, 상기 등가 아날로그 네트워크로부터 상기 각각의 아날로그 뉴런을 제거하는 단계; 및 상기 각각의 바이어스 값이 미리 결정된 최소 바이어스 임계값 미만이라는 결정에 따라, 상기 각각의 아날로그 뉴런을 상기 등가 아날로그 네트워크의 선형 접합으로 교체하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>66. 제55항에 있어서, 상기 가중치 행렬을 생성하기 전에, 상기 등가 아날로그 네트워크의 하나 이상의 아날로그 뉴런들로부터 연결들의 수를 증가시킴으로써, 상기 등가 아날로그 네트워크의 뉴런들의 수를 감소시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>67. 제55항에 있어서,상기 신경망 토폴로지를 변환하기 전에, 상기 등가 아날로그 네트워크가 아날로그 컴포넌트들의 미리 결정된 수 미만을 포함하도록, 신경망에 대한 프루닝 기술들을 사용하여, 상기 신경망 토폴로지 및 상기 훈련된 신경망의 상기 가중치들을 업데이트하도록 상기 훈련된 신경망을 프루닝하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>68. 제67항에 있어서, 상기 프루닝하는 단계는 상기 훈련된 신경망과 상기 등가 아날로그 네트워크 간의 출력의 일치 레벨 또는 정확도를 고려하여 반복적으로 수행되는, 방법.</claim></claimInfo><claimInfo><claim>69. 제55항에 있어서,상기 신경망 토폴로지를 상기 등가 아날로그 네트워크로 변환하기 전에, 네트워크 지식 추출을 수행하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>70. 신경망의 하드웨어 구현을 위한 시스템에 있어서, 하나 이상의 프로세서들;메모리를 포함하며,상기 메모리는 상기 하나 이상의 프로세서들에 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하며, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하되, 각 연산 증폭기는 상기 등가 아날로그 네트워크의 아날로그 뉴런을 나타내고, 각 저항기는 두 개의 아날로그 뉴런들 간 연결을 나타내고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내고;상기 가중치 행렬에 대한 저항 행렬을 생성하되, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하고;아날로그 컴포넌트들의 최적화된 아날로그 네트워크를 획득하기 위해, 상기 저항 행렬에 기초하여, 상기 복수의 연산 증폭기들 또는 상기 복수의 저항기들의 수를 감소시키도록 상기 등가 아날로그 네트워크를 프루닝하기 위한 인스트럭션들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>71. 하나 이상의 프로세서들을 가진 컴퓨터 시스템 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하되, 각 연산 증폭기는 상기 등가 아날로그 네트워크의 아날로그 뉴런을 나타내고, 각 저항기는 두 개의 아날로그 뉴런들 간 연결을 나타내고;상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내고; 상기 가중치 행렬에 대한 저항 행렬을 생성하되, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하고;아날로그 컴포넌트들의 최적화된 아날로그 네트워크를 획득하기 위해, 상기 저항 행렬에 기초하여, 상기 복수의 연산 증폭기들 또는 상기 복수의 저항기들의 수를 감소시키도록 상기 등가 아날로그 네트워크를 프루닝하기 위한 인스트럭션들을 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>72. 집적 회로에 있어서,아날로그 컴포넌트들의 아날로그 네트워크로서, 신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하는 단계; 상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계로서, 각 연산 증폭기는 각각의 아날로그 뉴런을 나타내고, 각 저항기는 각각의 제1 아날로그 뉴런과 각각의 제2 아날로그 뉴런 간 각각의 연결을 나타내는, 상기 변환하는 단계;  상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하는 단계로서, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내는, 상기 계산하는 단계; 상기 가중치 행렬에 대한 저항 행렬을 생성하는 단계로서, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하는, 상기 생성하는 단계; 상기 저항 행렬에 기초하여 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크를 구현하는 회로를 제조하기 위한 하나 이상의 리소그래픽 마스크들을 생성하는 단계; 및 리소그래픽 프로세스를 사용하여 상기 하나 이상의 리소그래픽 마스크들에 기초하여 상기 회로를 제조하는 단계를 포함하는 방법에 의해 제조되는, 상기 아날로그 컴포넌트들의 아날로그 네트워크를 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>73. 제72항에 있어서, 하나 이상의 디지털 신호들에 기초하여 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크에 대한 아날로그 입력을 생성하도록 구성된 하나 이상의 디지털-아날로그 컨버터들을 더 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>74. 제72항에 있어서, 상기 집적 회로의 추론들의 수에 기반한 샘플링 주파수로 1차원 또는 2차원 아날로그 입력들을 처리하도록 구성된 아날로그 신호 샘플링 모듈을 더 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>75. 제72항에 있어서, 상기 복수의 연산 증폭기들의 동작 범위와 일치하도록 아날로그 신호들을 스케일 다운 또는 스케일 업시키는 전압 컨버터 모듈을 더 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>76. 제72항에 있어서, CCD 카메라로부터 획득된 하나 이상의 프레임들을 처리하도록 구성된 택트(tact) 신호 처리 모듈을 더 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>77. 제72항에 있어서, 상기 훈련된 신경망은 장단기 메모리(LSTM) 네트워크이며, 상기 집적 회로는 신호 택트들을 동기화하고 시계열 처리를 허용하는 하나 이상의 클록 모듈들을 더 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>78. 제72항에 있어서, 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크의 출력에 기초하여 디지털 신호를 생성하도록 구성된 하나 이상의 아날로그-디지털 컨버터들을 더 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>79. 제72항에 있어서, 상기 회로는 에지 애플리케이션들로부터 획득된 1차원 또는 2차원 아날로그 신호들을 처리하도록 구성된 하나 이상의 신호 처리 모듈들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>80. 제72항에 있어서,상기 훈련된 신경망은 미리 결정된 양의 검출될 가스들을 포함하는 가스 혼합물에서 상이한 가스들을 선택적으로 감지하기 위해, 상이한 가스 혼합물에 대한 가스 센서 어레이의 신호들을 포함한 훈련 데이터세트들을 사용하여 훈련되고, 상기 신경망 토폴로지는 16개의 가스 센서들에 의한 측정을 기반으로 3개의 이원 가스(binary gas) 성분들을 감지하도록 설계된 1차원 딥 콘볼루션 신경망이며, 16개의 센서별 1차원 콘볼루션 블록들, 3개의 공유 또는 공통 1차원 콘볼루션 블록들 및 3개의 조밀 계층들을 포함하며,상기 등가 아날로그 네트워크는, (i) 아날로그 뉴런당 최대 100개의 입력 및 출력 연결들, (ii) 임의 개수의 시간 단계들만큼 지연을 생성하기 위한 지연 블록들, (iii) 5의 신호 제한, (iv) 15개의 계층들, (v) 대략 100,000개의 아날로그 뉴런들, 및 (vi) 대략 4,900,000개의 연결들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>81. 제72항에 있어서,상기 훈련된 신경망은 MOSFET 장치의 잔여 유효 수명(RUL)을 예측하는 다른 MOSFET에 대한 열 노화 시계열 데이터를 포함하는 훈련 데이터세트들을 사용하여 훈련되고,상기 신경망 토폴로지는 각 계층에 64개의 뉴런들을 갖는 4개의 LSTM 계층들과, 그에 이어지는 각각 64개의 뉴런들과 1개의 뉴런이 있는 두 개의 조밀 계층들을 포함하며,상기 등가 아날로그 네트워크는, (i) 아날로그 뉴런당 최대 100개의 입력 및 출력 연결들, (ii) 5의 신호 제한, (iii) 18개의 계층들, (iv) 3,000개 내지 3,200개의 아날로그 뉴런들, 및 (v) 123,000개 내지 124,000개의 연결들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>82. 제72항에 있어서,상기 훈련된 신경망은, 배터리 관리 시스템(BMS)에서 사용할 리튬 이온 배터리들의 수명 상태(SOH) 및 충전 상태(SOC)를 모니터링하기 위해, 서로 다른 상업적으로 이용 가능한 리튬-이온 배터리들의 연속 사용 동안 방전 및 온도 데이터를 포함하는 시계열 데이터를 포함한 훈련 데이터세트들을 사용하여 훈련되고,상기 신경망 토폴로지는 입력 계층, 각 계층에 64개의 뉴런들이 있는 2개의 LSTM 계층들, 그에 이어지는 SOC 및 SOH 값들을 생성하기 위한 2개의 뉴런들이 있는 출력 조밀 계층을 포함하며,상기 등가 아날로그 네트워크는, (i) 아날로그 뉴런당 최대 100개의 입력 및 출력 연결들, (ii) 5의 신호 제한, (iii) 9개의 계층들, (iv) 1,200개 내지 1,300개의 아날로그 뉴런들, 및 (v) 51,000개 내지 52,000개의 연결들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>83. 제72항에 있어서,상기 훈련된 신경망은, 배터리 관리 시스템(BMS)에서 사용할 리튬 이온 배터리들의 수명 상태(SOH) 및 충전 상태(SOC)를 모니터링하기 위해, 서로 다른 상업적으로 이용 가능한 리튬-이온 배터리들의 연속 사용 동안 방전 및 온도 데이터를 포함하는 시계열 데이터를 포함한 훈련 데이터세트들을 사용하여 훈련되고,상기 신경망 토폴로지는 18개의 뉴런들이 있는 입력 계층, 100개의 뉴런들이 있는 단순 순환 계층, 및 1개의 뉴런이 있는 조밀 계층을 포함하며,상기 등가 아날로그 네트워크는, (i) 아날로그 뉴런당 최대 100개의 입력 및 출력 연결들, (ii) 5의 신호 제한, (iii) 4개의 계층들, (iv) 200개 내지 300개의 아날로그 뉴런들, 및 (v) 2,200개 내지 2,400개의 연결들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>84. 제72항에 있어서,상기 훈련된 신경망은 음성 명령들을 식별하기 위한 스피치 명령들을 포함하는 훈련 데이터세트들을 사용하여 훈련되고,상기 신경망 토폴로지는 1개의 뉴런이 있는 깊이별 분리 가능 콘볼루션 신경망(DS-CNN)이며,상기 등가 아날로그 네트워크는, (i) 아날로그 뉴런당 최대 100개의 입력 및 출력 연결들, (ii) 5의 신호 제한, (iii) 13개의 계층들, (iv) 대략 72,000개의 아날로그 뉴런들, 및 (v) 대략 260만 개의 연결들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>85. 제72항에 있어서,상기 훈련된 신경망은, PPG 센서 데이터 및 3축 가속도계 데이터에 기초하여 신체 운동 중 맥박수를 결정하기 위해, 미리 결정된 시간 동안 다양한 신체 활동들을 수행하는 다양한 개인들에 대한 광전용적맥파(PPG) 데이터, 가속도계 데이터, 온도 데이터 및 피부 전기반응 신호 데이터 및 ECG 센서로부터 획득된 기준 심박수 데이터를 포함한 훈련 데이터세트들을 사용하여 훈련되고,상기 신경망 토폴로지는 각각이 시계열 콘볼루션을 수행하는 16개의 필터들과 20개의 커널이 있는 2개의 Conv1D 계층들, 각각이 16개의 뉴런들이 있는 2개의 LSTM 계층들, 및 16개의 뉴런들과 1개의 뉴런이 있는 2개의 조밀 계층들을 각각 포함하며,상기 등가 아날로그 네트워크는, (i) 임의 개수의 시간 단계들을 생성하기 위한 지연 블록들, (ii) 아날로그 뉴런당 최대 100개의 입력 및 출력 연결들, (iii) 5의 신호 제한, (iv) 16개의 계층들, (v) 700개 내지 800개의 아날로그 뉴런들, 및 (vi) 12,000개 내지 12,500개의 연결들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>86. 제72항에 있어서,훈련된 신경망은 펄스 도플러 레이더 신호에 기초하여 서로 다른 객체들을 분류하도록 훈련되며, 상기 신경망 토폴로지는 멀티-스케일 LSTM 신경망을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>87. 제72항에 있어서,상기 훈련망은 인간 활동 유형 인식을 수행하도록 훈련되며,상기 신경망 토폴로지는 각각이 12개의 필터들과 64의 커널 차원이 있는 콘볼루션 계층, 및 그에 이어지는 각각이 최대 풀링 계층, 및 1024개의 뉴런들과 N개의 뉴런들로 구성된 두 개의 공통 조밀 계층들(여기서, N은 클래스 수임)을 포함하는 3개의 채널별 콘볼루션 네트워크들을 포함하고; 상기 등가 아날로그 네트워크는, (i) 임의 계수의 시간 단계들을 생성하기 위한 지연 블록들, (ii) 아날로그 뉴런당 최대 100개의 입력 및 출력 연결들, (iii) 10개의 아날로그 뉴런들의 출력 계층, (iv) 5의 신호 제한, (v) 10개의 계층들, (vi) 1,200개 내지 1,300개의 아날로그 뉴런들, 및 (vi) 20,000개 내지 21,000개의 연결들을 포함하는, 집적 회로.</claim></claimInfo><claimInfo><claim>88. 제72항에 있어서, 상기 훈련된 신경망은 콘볼루션 연산을 사용하여 심박수 데이터와 병합되는 가속도계 데이터에 기초하여 인간 활동의 비정상적인 패턴들을 검출하도록 더 훈련되는, 집적 회로.</claim></claimInfo><claimInfo><claim>89. 신경망의 하드웨어 구현을 위한 시스템에 있어서, 하나 이상의 프로세서들;메모리를 포함하며,상기 메모리는 상기 하나 이상의 프로세서들에 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하며, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하되, 각 연산 증폭기는 각각의 아날로그 뉴런을 나타내고, 각 저항기는 각각의 제1 아날로그 뉴런과 각각의 제2 아날로그 뉴런 간 각각의 연결을 나타내고; 상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내고;상기 가중치 행렬에 대한 저항 행렬을 생성하되, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하고;상기 저항 행렬에 기초하여 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크를 구현하는 회로를 제조하기 위한 하나 이상의 리소그래픽 마스크들을 생성하고;리소그래픽 프로세스를 사용하여 상기 하나 이상의 리소그래픽 마스크들에 기초하여 상기 회로를 제조하기 위한 인스트럭션들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>90. 하나 이상의 프로세서들을 가진 컴퓨터 시스템 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 하나 이상의 프로그램들은,신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하고;상기 신경망 토폴로지를 복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하되, 각 연산 증폭기는 각각의 아날로그 뉴런을 나타내고, 각 저항기는 각각의 제1 아날로그 뉴런과 각각의 제2 아날로그 뉴런 간 각각의 연결을 나타내고; 상기 훈련된 신경망의 상기 가중치들에 기초하여 상기 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하되, 상기 가중치 행렬의 각 요소는 각각의 연결을 나타내고;상기 가중치 행렬에 대한 저항 행렬을 생성하되, 상기 저항 행렬의 각 요소는 상기 가중치 행렬의 각각의 가중치에 대응하고;상기 저항 행렬에 기초하여 상기 아날로그 컴포넌트들의 등가 아날로그 네트워크를 구현하는 회로를 제조하기 위한 하나 이상의 리소그래픽 마스크들을 생성하고;리소그래픽 프로세스를 사용하여 상기 하나 이상의 리소그래픽 마스크들에 기초하여 상기 회로를 제조하기 위한 인스트럭션들을 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>91. 신경망의 하드웨어 구현을 위한 라이브러리 생성 방법에 있어서, 복수의 신경망 토폴로지들을 획득하는 단계로서, 각 신경망 토폴로지는 각각의 신경망에 대응하는, 상기 획득하는 단계;각 신경망 토폴로지를 각각의 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하는 단계; 및복수의 회로들을 제조하기 위한 복수의 리소그래픽 마스크들을 생성하는 단계로서, 각 회로는 아날로그 컴포넌트들의 각각의 등가 아날로그 네트워크를 구현하는, 상기 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>92. 제91항에 있어서,새로운 신경망 토폴로지 및 훈련된 신경망의 가중치들을 획득하는 단계;상기 새로운 신경망 토폴로지를 상기 복수의 신경망 토폴로지들과 비교하는 것에 기초하여 상기 복수의 리소그래픽 마스크들로부터 하나 이상의 리소그래픽 마스크들을 선택하는 단계;상기 가중치들에 기초하여 새로운 등가 아날로그 네트워크에 대한 가중치 행렬을 계산하는 단계;상기 가중치 행렬에 대한 저항 행렬을 생성하는 단계; 및상기 저항 행렬 및 상기 하나 이상의 리소그래픽 마스크들에 기초하여 상기 새로운 등가 아날로그 네트워크를 구현하는 회로를 제조하기 위한 새로운 리소그래픽 마스크를 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>93. 제92항에 있어서, 상기 새로운 신경망 토폴로지는 복수의 서브 네트워크 토폴로지들을 포함하며, 상기 하나 이상의 리소그래픽 마스크들을 선택하는 단계는 각 서브 네트워크 토폴로지를 상기 복수의 네트워크 토폴로지들의 각 네트워크 토폴로지와 비교하는 것에 더 기반하는, 방법.</claim></claimInfo><claimInfo><claim>94. 제93항에 있어서, 상기 복수의 서브 네트워크 토폴로지들 중 하나 이상의 서브 네트워크 토폴로지들은 상기 복수의 네트워크 토폴로지들 중 임의의 네트워크 토폴로지와 비교하는 데 실패하며, 상기 방법은,상기 하나 이상의 서브 네트워크 토폴로지들의 각 서브 네트워크 토폴로지를 아날로그 컴포넌트들의 각각의 등가 아날로그 서브 네트워크로 변환하는 단계; 및하나 이상의 회로들을 제조하기 위한 하나 이상의 리소그래픽 마스크들을 생성하는 단계로서, 상기 하나 이상의 회로들의 각 회로는 아날로그 컴포넌트들의 각각의 등가 아날로그 서브 네트워크를 구현하는, 상기 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>95. 제91항에 있어서, 각각의 네트워크 토폴로지를 각각의 등가 아날로그 네트워크로 변환하는 단계는,상기 각각의 네트워크 토폴로지를 복수의 서브 네트워크 토폴로지들로 분해하는 단계;각 서브 네트워크 토폴로지를 아날로그 컴포넌트들의 각각의 등가 아날로그 서브 네트워크로 변환하는 단계; 및상기 각각의 등가 아날로그 네트워크를 획득하기 위해 각 등가 아날로그 서브 네트워크를 구성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>96. 제95항에 있어서, 상기 각각의 네트워크 토폴로지를 분해하는 단계는 상기 각각의 네트워크 토폴로지의 하나 이상의 계층들을 상기 복수의 서브 네트워크 토폴로지들로 식별하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>97. 제91항에 있어서, 각 회로는,아날로그 컴포넌트들의 각각의 등가 아날로그 네트워크에 대한 개략도들을 생성하고; 상기 개략도들에 기초하여 각각의 회로 레이아웃 설계를 생성함으로써 획득되는, 방법.</claim></claimInfo><claimInfo><claim>98. 제97항에 있어서,상기 복수의 회로들을 제조하기 위한 상기 복수의 리소그래픽 마스크들을 생성하기 전에 하나 이상의 회로 레이아웃 설계들을 결합하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>99. 신경망의 하드웨어 구현을 위한 라이브러리를 생성하기 위한 시스템에 있어서, 하나 이상의 프로세서들;메모리를 포함하며,상기 메모리는 상기 하나 이상의 프로세서들에 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하며, 상기 하나 이상의 프로그램들은,복수의 신경망 토폴로지들을 획득하되, 각 신경망 토폴로지는 각각의 신경망에 대응하고;각 신경망 토폴로지를 각각의 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하고;복수의 회로들을 제조하기 위한 복수의 리소그래픽 마스크들을 생성하되, 각 회로는 아날로그 컴포넌트들의 각각의 등가 아날로그 네트워크를 구현하기 위한 인스트럭션들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>100. 하나 이상의 프로세서들을 가진 컴퓨터 시스템 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 하나 이상의 프로그램들은,복수의 신경망 토폴로지들을 획득하되, 각 신경망 토폴로지는 각각의 신경망에 대응하고;각 신경망 토폴로지를 각각의 아날로그 컴포넌트들의 등가 아날로그 네트워크로 변환하고;복수의 회로들을 제조하기 위한 복수의 리소그래픽 마스크들을 생성하되, 각 회로는 아날로그 컴포넌트들의 각각의 등가 아날로그 네트워크를 구현하기 위한 인스트럭션들을 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo><claimInfo><claim>101. 아날로그 뉴로모픽 회로들의 에너지 효율을 최적화하기 위한 방법에 있어서, 상기 방법은,복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 아날로그 네트워크를 구현하는 집적 회로를 획득하는 단계로서, 상기 아날로그 네트워크는 훈련된 신경망을 나타내고, 각 연산 증폭기는 각각의 아날로그 뉴런을 나타내고, 각 저항기는 각각의 제1 아날로그 뉴런과 각각의 제2 아날로그 뉴런 간 각각의 연결을 나타내는, 상기 획득하는 단계;상기 아날로그 네트워크의 하나의 계층에서 후속 계층으로 신호들을 동시에 전달하는 것을 포함하여, 복수의 테스트 입력들에 대한 상기 집적 회로를 사용하여 추론들을 생성하는 단계; 및상기 집적 회로를 사용하여 추론들을 생성하는 동안, 상기 복수의 연산 증폭기들이 평형을 이루는지를 결정하는 단계; 및 상기 신호 출력 레벨이 평형을 이룬다는 결정에 따라,  신호들의 전파를 위한 신호 형성에 영향을 미치는 상기 아날로그 네트워크의 활성 아날로그 뉴런 세트를 결정하는 단계; 및  미리 결정된 시간 기간 동안, 상기 활성 아날로그 뉴런 세트와 구별되는, 상기 아날로그 네트워크의 하나 이상의 아날로그 뉴런들에 대한 전력을 턴 오프시키는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>102. 제101항에 있어서, 상기 활성 아날로그 뉴런 세트를 결정하는 단계는 상기 아날로그 네트워크를 통한 신호 전파의 지연을 계산하는 것에 기반하는, 방법.</claim></claimInfo><claimInfo><claim>103. 제101항에 있어서, 상기 활성 아날로그 뉴런 세트를 결정하는 단계는 상기 아날로그 네트워크를 통한 상기 신호들의 전파를 검출하는 것에 기반하는, 방법.</claim></claimInfo><claimInfo><claim>104. 제101항에 있어서, 상기 훈련된 신경망은 피드포워드 신경망이며, 상기 활성 아날로그 뉴런 세트는 상기 아날로그 네트워크의 활성 계층에 속하며, 전력을 턴 오프시키는 단계는 상기 아날로그 네트워크의 상기 활성 계층 이전의 하나 이상의 계층들에 대한 전력을 턴 오프시키는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>105. 제101항에 있어서, 상기 미리 결정된 시간 기간은 신호 지연을 고려하여, 상기 아날로그 네트워크를 통한 신호들의 전파를 시뮬레이션하는 것에 기반하여 계산되는, 방법.</claim></claimInfo><claimInfo><claim>106. 제101항에 있어서, 상기 훈련된 신경망은 순환 신경망(RNN)이며, 상기 아날로그 네트워크는 상기 복수의 연산 증폭기들 및 상기 복수의 저항기 이외의 하나 이상의 아날로그 컴포넌트들을 더 포함하며, 상기 방법은,상기 신호 출력 레벨이 평형을 이룬다는 결정에 따라, 상기 미리 결정된 시간 기간 동안, 상기 하나 이상의 아날로그 컴포넌트들에 대한 전력을 턴 오프시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>107. 제101항에 있어서,상기 미리 결정된 시간 기간 이후 상기 아날로그 네트워크의 상기 하나 이상의 아날로그 뉴런들에 대한 전력을 턴 온시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>108. 제101항에 있어서, 상기 복수의 연산 증폭기들의 상기 신호 출력 레벨이 평형을 이루는지를 결정하는 단계는 상기 아날로그 네트워크의 하나 이상의 연산 증폭기들이 미리 결정된 임계 신호 레벨 이상 출력되고 있는 경우의 검출에 기반하는, 방법.</claim></claimInfo><claimInfo><claim>109. 제101항에 있어서,상기 추론을 생성하는 동안, 상기 미리 결정된 기간 동안 상기 활성 아날로그 뉴런 세트를 턴 오프시키고 상기 미리 결정된 기간 동안 상기 활성 아날로그 뉴런 세트를 턴 온시키는 단계를 반복하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>110. 제101항에 있어서,상기 신호 출력 레벨이 평형을 이룬다는 결정에 따라, 각 추론 사이클 동안,  신호들의 전파를 위한 신호 형성에 영향을 미치는 상기 아날로그 네트워크의 제1 아날로그 뉴런 계층을 결정하는 단계; 및  상기 미리 결정된 시간 기간 동안, 상기 제1 계층 이전에, 상기 아날로그 네트워크의 제1 하나 이상의 아날로그 뉴런들에 대한 전력을 턴 오프시키는 단계; 및 상기 제1 시간 간격 이후의 제2 시간 간격 동안, 상기 아날로그 네트워크의 상기 제1 하나 이상의 아날로그 뉴런들 및 상기 제1 아날로그 뉴런 계층을 포함하는 제2 하나 이상의 아날로그 뉴런들에 대한 전력을 상기 미리 결정된 시간 동안 턴 오프시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>111. 제101항에 있어서, 상기 하나 이상의 아날로그 뉴런들은 상기 아날로그 네트워크의 제1 하나 이상의 계층들의 아날로그 뉴런들로 구성되고, 상기 활성 아날로그 뉴런 세트는 상기 아날로그 네트워크의 제2 계층의 아날로그 뉴런들로 구성되며, 상기 아날로그 네트워크의 상기 제2 계층은 상기 제1 하나 이상의 계층들의 계층들과 구별되는, 방법.</claim></claimInfo><claimInfo><claim>112. 아날로그 뉴로모픽 회로들의 에너지 효율을 최적화하기 위한 시스템에 있어서, 하나 이상의 프로세서들;메모리를 포함하며,상기 메모리는 상기 하나 이상의 프로세서들에 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하며, 상기 하나 이상의 프로그램들은,복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 아날로그 네트워크를 구현하는 집적 회로를 획득하되, 상기 아날로그 네트워크는 훈련된 신경망을 나타내고, 각 연산 증폭기는 각각의 아날로그 뉴런을 나타내고, 각 저항기는 각각의 제1 아날로그 뉴런과 각각의 제2 아날로그 뉴런 간 각각의 연결을 나타내고;상기 아날로그 네트워크의 하나의 계층에서 후속 계층으로 신호들을 동시에 전달하는 것을 포함하여, 복수의 테스트 입력들에 대한 상기 집적 회로를 사용하여 추론들을 생성하고;상기 집적 회로를 사용하여 추론들을 생성하는 동안, 상기 복수의 연산 증폭기들이 평형을 이루는지를 결정하고; 상기 신호 출력 레벨이 평형을 이룬다는 결정에 따라,  신호들의 전파를 위한 신호 형성에 영향을 미치는 상기 아날로그 네트워크의 활성 아날로그 뉴런 세트를 결정하고;  미리 결정된 시간 기간 동안, 상기 활성 아날로그 뉴런 세트와 구별되는, 상기 아날로그 네트워크의 하나 이상의 아날로그 뉴런들에 대한 전력을 턴 오프시키기 위한 인스트럭션들을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>113. 하나 이상의 프로세서들을 가진 컴퓨터 시스템 의해 실행하도록 구성된 하나 이상의 프로그램들을 저장하는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 하나 이상의 프로그램들은,복수의 연산 증폭기들 및 복수의 저항기들을 포함하는 아날로그 컴포넌트들의 아날로그 네트워크를 구현하는 집적 회로를 획득하되, 상기 아날로그 네트워크는 훈련된 신경망을 나타내고, 각 연산 증폭기는 각각의 아날로그 뉴런을 나타내고, 각 저항기는 각각의 제1 아날로그 뉴런과 각각의 제2 아날로그 뉴런 간 각각의 연결을 나타내고;상기 아날로그 네트워크의 하나의 계층에서 후속 계층으로 신호들을 동시에 전달하는 것을 포함하여, 복수의 테스트 입력들에 대한 집적 회로를 사용하여 추론들을 생성하고;상기 집적 회로를 사용하여 추론들을 생성하는 동안, 상기 복수의 연산 증폭기들의 신호 출력 레벨이 평형을 이루는지를 결정하고; 상기 신호 출력 레벨이 평형을 이룬다는 결정에 따라,  신호들의 전파를 위한 신호 형성에 영향을 미치는 상기 아날로그 네트워크의 활성 아날로그 뉴런 세트를 결정하고;  미리 결정된 시간 기간 동안, 상기 활성 아날로그 뉴런 세트와 구별되는, 상기 아날로그 네트워크의 하나 이상의 아날로그 뉴런들에 대한 전력을 턴 오프시키기 위한 인스트럭션들을 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>영국 브리스톨 스테이션 어프로치 엔진 쉐드 (우: 비에스* *큐에이치)</address><code>520220160438</code><country>영국</country><engName>PolyN Technology Limited</engName><name>폴리앤 테크놀로지 리미티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>영국 더블유*더블유 *에이에프...</address><code> </code><country> </country><engName>TIMOFEJEVS, Aleksandrs</engName><name>티모페예프스, 알렉산드르스</name></inventorInfo><inventorInfo><address>영국 더블유*더블유 *에이에프...</address><code> </code><country> </country><engName>MASLOV, Boris</engName><name>마슬로프, 보리스</name></inventorInfo><inventorInfo><address>영국 더블유*더블유 *에이에프...</address><code> </code><country> </country><engName>GODOVSKIY, Dmitry Yulievich</engName><name>고도프스키, 드미트리 유리에비치</name></inventorInfo><inventorInfo><address>영국 더블유*더블유 *에이에프...</address><code> </code><country> </country><engName>KOVSHOV, Nikolai Vadimovich</engName><name>코브쇼프, 니콜라이 바디모비치</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 중구 서소문로**(서소문동, 정안빌딩*층)</address><code>920121001826</code><country>대한민국</country><engName>NAM &amp; NAM</engName><name>특허법인 남앤남</name></agentInfo><agentInfo><address>서울 중구 서소문로 **, *층(서소문동)</address><code>920241000417</code><country>대한민국</country><engName>NAM IP GROUP</engName><name>특허법인(유)남아이피그룹</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2022.03.10</receiptDate><receiptNumber>1-1-2022-0260140-67</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2022.03.11</receiptDate><receiptNumber>1-1-2022-0265528-39</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2022.03.11</receiptDate><receiptNumber>1-1-2022-0265529-85</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Request for Amendment</documentEngName><documentName>보정요구서</documentName><receiptDate>2022.03.21</receiptDate><receiptNumber>1-5-2022-0043067-35</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Amendment to Patent Application, etc.] Amendment</documentEngName><documentName>[출원서 등 보정]보정서</documentName><receiptDate>2022.05.23</receiptDate><receiptNumber>1-1-2022-0544295-18</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Request for Amendment</documentEngName><documentName>보정요구서</documentName><receiptDate>2022.05.24</receiptDate><receiptNumber>1-5-2022-0079242-17</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Amendment to Patent Application, etc.] Amendment</documentEngName><documentName>[출원서 등 보정]보정서</documentName><receiptDate>2022.05.30</receiptDate><receiptNumber>1-1-2022-0570673-17</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2022.06.09</receiptDate><receiptNumber>1-5-2022-0086095-55</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>특허고객번호 정보변경(경정)신고서·정정신고서</documentName><receiptDate>2024.01.18</receiptDate><receiptNumber>4-1-2024-5036920-75</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Appointment of Agent] Report on Agent (Representative)</documentEngName><documentName>[대리인선임]대리인(대표자)에 관한 신고서</documentName><receiptDate>2024.03.15</receiptDate><receiptNumber>1-1-2024-0295742-10</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for refusal</documentEngName><documentName>의견제출통지서</documentName><receiptDate>2024.03.28</receiptDate><receiptNumber>9-5-2024-0270886-03</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[지정기간연장]기간 연장신청서·기간 단축신청서·기간 경과 구제신청서·절차 계속신청서</documentName><receiptDate>2024.05.28</receiptDate><receiptNumber>1-1-2024-0574662-88</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2024.06.28</receiptDate><receiptNumber>1-1-2024-0705333-12</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[거절이유 등 통지에 따른 의견]의견서·답변서·소명서</documentName><receiptDate>2024.06.28</receiptDate><receiptNumber>1-1-2024-0705332-66</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Report of Deferment (Postponement) of Processing of Examination</documentEngName><documentName>심사처리보류(연기)보고서</documentName><receiptDate>2025.01.24</receiptDate><receiptNumber>9-6-2025-0018743-41</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Decision to Refuse a Patent</documentEngName><documentName>거절결정서</documentName><receiptDate>2025.03.24</receiptDate><receiptNumber>9-5-2025-0288103-83</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Amendment to Description, etc(Reexamination)</documentEngName><documentName>[명세서등 보정]보정서(재심사)</documentName><receiptDate>2025.04.30</receiptDate><receiptNumber>1-1-2025-0490190-26</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[거절이유 등 통지에 따른 의견]의견서·답변서·소명서</documentName><receiptDate>2025.04.30</receiptDate><receiptNumber>1-1-2025-0490189-80</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020227008037.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93a4c5ae95f13d7fe71a05fb15848df9e0ed923da9bd2c4824dfaf71e2eb1cc7cc019e7de3182ed1dc6e6aeb18b34a749db125aae60378b5eb</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf3cba01d213c05eb288fe3c4d827039d817ecf5815e6cd09540cb3e4f497ecae5aa9553045d7a1a06f5823aa194c40dffaac65ae0acb18039</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>