<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:30:04.304</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.02.16</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-7033621</applicationNumber><claimCount>30</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>확장 현실을 사용한 로봇 조작을 위한 시스템들 및 방법들</inventionTitle><inventionTitleEng>SYSTEMS AND METHODS FOR ROBOTIC MANIPULATION USING EXTENDED REALITY</inventionTitleEng><openDate>2024.11.20</openDate><openNumber>10-2024-0164785</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.10.08</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 13/02</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 13/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 15/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 19/02</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B62D 57/032</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2013.01.01)</ipcDate><ipcNumber>G06F 3/0346</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06F 3/01</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 로봇을 제어하는 방법은, 컴퓨팅 디바이스에 의해 하나 이상의 센서들로부터, 로봇의 환경을 반영하는 센서 데이터를 수신하는 단계 — 하나 이상의 센서들은 로봇의 지상 평면에 대해 적어도 150도에 걸쳐 있는 시야를 갖도록 구성됨 —; 컴퓨팅 디바이스에 의해, 로봇의 오퍼레이터에 의해 사용가능한 XR(extended reality) 디스플레이에 비디오 출력을 제공하는 단계 — 비디오 출력은 로봇의 환경을 반영함 —; 컴퓨팅 디바이스에 의해, 로봇의 오퍼레이터에 의한 이동을 반영하는 이동 정보를 수신하는 단계; 및 컴퓨팅 디바이스에 의해, 이동 정보에 기반하여 이동되도록 로봇을 제어하는 단계를 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2023.09.14</internationOpenDate><internationOpenNumber>WO2023172385</internationOpenNumber><internationalApplicationDate>2023.02.16</internationalApplicationDate><internationalApplicationNumber>PCT/US2023/013195</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 로봇으로서,상기 로봇의 지상 평면에 대해 적어도 150도에 걸쳐 있는 시야를 갖도록 구성된 하나 이상의 카메라 센서들; 및컴퓨팅 디바이스를 포함하며,상기 컴퓨팅 디바이스는, 상기 하나 이상의 카메라 센서들로부터, 상기 로봇의 환경을 반영하는 이미지 데이터를 수신하고; 상기 로봇의 오퍼레이터에 의해 사용가능한 XR(extended reality) 디스플레이에 비디오 출력을 제공하고 — 상기 비디오 출력은 상기 로봇의 환경을 반영하는 이미지 데이터에 기초한 정보를 포함함 —; 상기 로봇의 상기 오퍼레이터에 의한 이동을 반영하는 이동 정보를 수신하고; 그리고상기 이동 정보에 기반하여 이동되도록 상기 로봇을 제어하도록구성되는, 로봇.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 컴퓨팅 디바이스는, 제1 시간 간격으로 상기 XR 디스플레이에 상기 비디오 출력을 제공하고, 그리고 제2 시간 간격으로 이동되도록 상기 로봇을 제어하도록 구성되며, 상기 제1 시간 간격 및 상기 제2 시간 간격은 계획 기간에 의해 분리되는, 로봇.</claim></claimInfo><claimInfo><claim>3. 제1항에 있어서,조작기를 더 포함하며, 상기 이동되도록 상기 로봇을 제어하는 것은 오브젝트의 위치를 특정함으로써 상기 로봇의 환경에서 상기 오브젝트를 파지하도록 상기 로봇을 제어하는 것을 포함하고, 상기 로봇은 상기 오브젝트를 파지하기 위해 상기 로봇에 의한 로코모션(locomotion)과 상기 로봇의 상기 조작기에 의한 이동의 적합한 조합을 결정하는, 로봇.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서,상기 조작기는 팔 부분 및 관절 부분을 포함하는, 로봇.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서,상기 이동되도록 상기 로봇을 제어하는 것은,상기 이동 정보에 기반하여 상기 오퍼레이터의 모션의 관절 중심을 식별하는 것; 및상기 오퍼레이터의 상기 모션의 관절 중심에 대응하는 상기 조작기 상의 포인트에 대해 이동되도록 상기 조작기를 제어하는 것을 포함하는, 로봇.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서,조작기를 더 포함하며, 상기 이동되도록 상기 로봇을 제어하는 것은 상기 오퍼레이터의 작업공간을 상기 조작기의 작업공간에 맵핑하는 것을 포함하는, 로봇.</claim></claimInfo><claimInfo><claim>7. 제6항에 있어서,상기 이동되도록 상기 로봇을 제어하는 것은 달성될 태스크-레벨 결과에 기반하여 상기 조작기의 상기 작업공간에서 이동 계획을 생성하는 것을 포함하며, 상기 이동 계획은 상기 이동 정보에서 반영된 것과 상이한 모션의 양상을 반영하는, 로봇.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,상기 로봇은 제1 로봇이고;상기 컴퓨팅 디바이스는 상기 제1 로봇 및 제2 로봇과 전자 통신하고; 그리고상기 컴퓨팅 디바이스는 협력하여 이동되도록 상기 제1 로봇 및 상기 제2 로봇을 제어하도록 구성되는, 로봇.</claim></claimInfo><claimInfo><claim>9. 제1항에 있어서,상기 이동되도록 상기 로봇을 제어하는 것은 상기 이동 정보에 기반하여 조작 계획을 생성하는 것 및 상기 조작 계획에 기반하여 로코모션 계획을 생성하는 것을 포함하는, 로봇.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,조작기를 더 포함하며, 상기 이동되도록 상기 로봇을 제어하는 것은,오브젝트가 상기 로봇의 상기 조작기와 접촉되는 것으로 검출되면, 힘 제어 모드를 이용하는 것; 및어떠한 오브젝트도 상기 조작기와 접촉되는 것으로 검출되지 않으면, 낮은-힘 모드 또는 힘-없음 모드를 이용하는 것을 포함하는, 로봇.</claim></claimInfo><claimInfo><claim>11. 로봇을 제어하는 방법으로서,하나 이상의 카메라 센서들로부터 컴퓨팅 디바이스에 의해, 상기 로봇의 환경을 반영하는 이미지 데이터를 수신하는 단계 — 상기 하나 이상의 카메라 센서들은 상기 로봇의 지상 평면에 대해 적어도 150도에 걸쳐 있는 시야를 갖도록 구성됨 —;상기 컴퓨팅 디바이스에 의해, 상기 로봇의 오퍼레이터에 의해 사용가능한 XR(extended reality) 디스플레이에 비디오 출력을 제공하는 단계 — 상기 비디오 출력은 상기 로봇의 환경을 반영하는 이미지 데이터에 기초한 정보를 포함함 —;상기 컴퓨팅 디바이스에 의해, 상기 로봇의 상기 오퍼레이터에 의한 이동을 반영하는 이동 정보를 수신하는 단계; 및상기 컴퓨팅 디바이스에 의해, 상기 이동 정보에 기반하여 이동되도록 상기 로봇을 제어하는 단계를 포함하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 비디오 출력은 제1 시간 간격으로 제공되고, 상기 제어하는 단계는 제2 시간 간격으로 수행되며, 상기 제1 시간 간격 및 상기 제2 시간 간격은 계획 기간에 의해 분리되는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>13. 제11항에 있어서,상기 이동되도록 상기 로봇을 제어하는 단계는 오브젝트의 위치를 특정함으로써 상기 오브젝트를 파지하도록 상기 로봇을 제어하는 단계를 포함하며, 상기 로봇은 상기 오브젝트를 파지하기 위해 상기 로봇에 의한 로코모션과 상기 로봇의 조작기에 의한 이동의 적합한 조합을 결정하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 조작기는 팔 부분 및 관절 부분을 포함하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>15. 제14항에 있어서,상기 이동되도록 상기 로봇을 제어하는 단계는,상기 이동 정보에 기반하여 상기 오퍼레이터의 모션의 관절 중심을 식별하는 단계; 및상기 오퍼레이터의 상기 모션의 관절 중심에 대응하는 상기 조작기 상의 포인트에 대해 이동되도록 상기 조작기를 제어하는 단계를 포함하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>16. 제11항에 있어서,상기 이동되도록 상기 로봇을 제어하는 단계는 상기 오퍼레이터의 작업공간을 상기 로봇의 조작기의 작업공간에 맵핑하는 단계를 포함하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서,상기 이동되도록 상기 로봇을 제어하는 단계는 달성될 태스크-레벨 결과에 기반하여 상기 조작기의 상기 작업공간에서 이동 계획을 생성하는 단계를 포함하며, 상기 이동 계획은 상기 이동 정보에서 반영된 것과 상이한 모션의 양상을 반영하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>18. 제11항에 있어서,상기 로봇은 제1 로봇이고; 상기 컴퓨팅 디바이스는 제2 로봇과 전자 통신하고; 그리고상기 컴퓨팅 디바이스는 협력하여 이동되도록 상기 제1 로봇 및 상기 제2 로봇을 제어하도록 구성되는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>19. 제11항에 있어서,상기 이동되도록 상기 로봇을 제어하는 단계는 상기 이동 정보에 기반하여 조작 계획을 생성하는 단계 및 상기 조작 계획에 기반하여 로코모션 계획을 생성하는 단계를 포함하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>20. 제11항에 있어서,상기 이동되도록 상기 로봇을 제어하는 단계는,오브젝트가 상기 로봇의 조작기와 접촉되는 것으로 검출되면, 힘 제어 모드를 이용하는 단계; 및어떠한 오브젝트도 상기 조작기와 접촉되는 것으로 검출되지 않으면, 낮은-힘 모드 또는 힘-없음 모드를 이용하는 단계를 포함하는, 로봇을 제어하는 방법.</claim></claimInfo><claimInfo><claim>21. 시스템으로서,로봇;상기 로봇의 지상 평면에 대해 적어도 150도에 걸쳐 있는 시야를 갖도록 구성된 하나 이상의 카메라 센서들;XR(extended reality) 디스플레이 및 적어도 하나의 XR 제어기를 포함하는 XR 시스템; 및컴퓨팅 디바이스를 포함하며,상기 컴퓨팅 디바이스는, 상기 하나 이상의 카메라 센서들로부터, 상기 로봇의 환경을 반영하는 이미지 데이터를 수신하고; 상기 로봇의 오퍼레이터에 의해 사용가능한 상기 XR 디스플레이에 비디오 출력을 제공하고 — 상기 비디오 출력은 상기 로봇의 환경을 반영하는 이미지 데이터에 기초한 정보를 포함함 —; 상기 적어도 하나의 XR 제어기로부터, 상기 로봇의 상기 오퍼레이터에 의한 이동을 반영하는 이동 정보를 수신하고; 그리고 상기 이동 정보에 기반하여 이동되도록 상기 로봇을 제어하도록구성되는, 시스템.</claim></claimInfo><claimInfo><claim>22. 제21항에 있어서,상기 컴퓨팅 디바이스는, 제1 시간 간격으로 상기 XR 디스플레이에 상기 비디오 출력을 제공하고, 그리고 제2 시간 간격으로 이동되도록 상기 로봇을 제어하도록 구성되며, 상기 제1 시간 간격 및 상기 제2 시간 간격은 계획 기간에 의해 분리되는, 시스템.</claim></claimInfo><claimInfo><claim>23. 제21항에 있어서,상기 로봇은 조작기를 포함하며, 상기 이동되도록 상기 로봇을 제어하는 것은 오브젝트의 위치를 특정함으로써 상기 로봇의 환경에서 상기 오브젝트를 파지하도록 상기 로봇을 제어하는 것을 포함하고, 상기 로봇은 상기 오브젝트를 파지하기 위해 상기 로봇에 의한 로코모션과 상기 로봇의 상기 조작기에 의한 이동의 적합한 조합을 결정하는, 시스템.</claim></claimInfo><claimInfo><claim>24. 제23항에 있어서,상기 조작기는 팔 부분 및 관절 부분을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>25. 제24항에 있어서,상기 이동되도록 상기 로봇을 제어하는 것은,상기 이동 정보에 기반하여 상기 오퍼레이터의 모션의 관절 중심을 식별하는 것; 및상기 오퍼레이터의 상기 모션의 관절 중심에 대응하는 상기 조작기 상의 포인트에 대해 이동되도록 상기 조작기를 제어하는 것을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>26. 제21항에 있어서,상기 로봇은 조작기를 포함하며, 상기 이동되도록 상기 로봇을 제어하는 것은 상기 오퍼레이터의 작업공간을 상기 조작기의 작업공간에 맵핑하는 것을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>27. 제26항에 있어서,상기 이동되도록 상기 로봇을 제어하는 것은 달성될 태스크-레벨 결과에 기반하여 상기 조작기의 상기 작업공간에서 이동 계획을 생성하는 것을 포함하며, 상기 이동 계획은 상기 이동 정보에서 반영된 것과 상이한 모션의 양상을 반영하는, 시스템.</claim></claimInfo><claimInfo><claim>28. 제21항에 있어서,상기 로봇은 제1 로봇이고;상기 시스템은 제2 로봇을 더 포함하고;상기 컴퓨팅 디바이스는 상기 제1 로봇 및 상기 제2 로봇과 전자 통신하고; 그리고상기 컴퓨팅 디바이스는 협력하여 이동되도록 상기 제1 로봇 및 상기 제2 로봇을 제어하도록 구성되는, 시스템.</claim></claimInfo><claimInfo><claim>29. 제21항에 있어서,상기 이동되도록 상기 로봇을 제어하는 것은 상기 이동 정보에 기반하여 조작 계획을 생성하는 것 및 상기 조작 계획에 기반하여 로코모션 계획을 생성하는 것을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>30. 제21항에 있어서,상기 로봇은 조작기를 포함하며, 상기 이동되도록 상기 로봇을 제어하는 것은,오브젝트가 상기 로봇의 상기 조작기와 접촉되는 것으로 검출되면, 힘 제어 모드를 이용하는 것; 및어떠한 오브젝트도 상기 조작기와 접촉되는 것으로 검출되지 않으면, 낮은-힘 모드 또는 힘-없음 모드를 이용하는 것을 포함하는, 시스템.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 매사추세츠주 월섬 스미스 스트리트 *** 유닛 ****</address><code>520200430545</code><country>미국</country><engName>Boston Dynamics, Inc.</engName><name>보스턴 다이나믹스, 인크.</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 매사추세츠 ...</address><code> </code><country> </country><engName>DELLON, Brian Todd</engName><name>델론, 브라이언 토드</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 서소문로 **, *층(서소문동)</address><code>920241000417</code><country>대한민국</country><engName>NAM IP GROUP</engName><name>특허법인(유)남아이피그룹</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.03.11</priorityApplicationDate><priorityApplicationNumber>17/693,019</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2024.10.08</receiptDate><receiptNumber>1-1-2024-1094400-31</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2024.10.18</receiptDate><receiptNumber>1-5-2024-0167782-16</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247033621.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93878c971b8065682abd5220ae382c0cc318fe7de2f30a7d004c014fd67606d4a995805a8b7ae6f3747b3237c5b13e5ed781b248dddcbf5cad</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf21caa215be650f617f7da5a16a1830a9a7d9e3a7bb87ed91fee278de5b088d02883101b888095a036e56ae1fa6a2baa25a92230fcc8d8570</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>