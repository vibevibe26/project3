<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:41:42.4142</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2020.12.10</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2020-0172465</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>깊이 추정 방법 및 장치</inventionTitle><inventionTitleEng>METHOD AND APPARATUS FOR ESTIMATING DEPTH</inventionTitleEng><openDate>2021.07.26</openDate><openNumber>10-2021-0092669</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.11.28</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/55</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0464</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 깊이 추정 방법 및 장치가 개시된다. 일 실시예에 따른 깊이 추정 장치는 복수의 상위 픽셀을 포함하고 각 상위 픽셀은 N개의 서브 픽셀을 포함하는 이미지 센서 및 하나 이상의 프로세서를 포함한다. 이미지 센서는 원본 이미지를 획득하고, 프로세서는 원본 이미지로부터 상기 N개의 서브 픽셀의 각각에 대응하는 N개의 서브 이미지를 획득하고, 제1 뉴럴 네트워크를 이용하여 N개의 서브 이미지 간의 시점 차이를 획득하고, 제2 뉴럴 네트워크를 이용하여 시점 차이를 기초로 원본 이미지의 깊이 맵을 획득할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 복수의 상위 픽셀을 포함하고 각 상위 픽셀은 N개의 서브 픽셀을 포함하는 이미지 센서로부터 원본 이미지를 획득하는 단계;상기 원본 이미지로부터 상기 N개의 서브 픽셀의 각각에 대응하는 N개의 서브 이미지를 획득하는 단계;제1 뉴럴 네트워크를 이용하여 상기 N개의 서브 이미지 간의 시점 차이를 획득하는 단계; 및제2 뉴럴 네트워크를 이용하여 상기 시점 차이를 기초로 상기 원본 이미지의 깊이 맵(map)을 획득하는 단계를 포함하는, 깊이 추정 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 시점 차이를 획득하는 단계는,상기 N개의 서브 이미지를 2개씩 페어링(paring)하는 단계; 및상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계를 포함하는,깊이 추정 방법.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서,상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계는,상기 페어링된 서브 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하는 단계; 및상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계를 포함하는,깊이 추정 방법.</claim></claimInfo><claimInfo><claim>4. 제2항에 있어서,상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계는,상기 페어링된 서브 이미지 각각에 컨벌루션(convolution) 연산을 수행하는 단계;상기 컨벌루션 연산의 결과 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하는 단계; 및상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계를 포함하는,깊이 추정 방법.</claim></claimInfo><claimInfo><claim>5. 제2항에 있어서,상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계는,상기 페어링된 서브 이미지 각각에 변위 연산을 수행하는 단계;상기 변위 연산의 결과 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하는 단계; 및상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계를 포함하는,깊이 추정 방법.</claim></claimInfo><claimInfo><claim>6. 제2항에 있어서,상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계는,상기 페어링된 서브 이미지 각각에 변위 연산을 수행하는 단계;상기 변위 연산의 결과 이미지 각각에 컨벌루션 연산을 수행하는 단계;상기 컨벌루션 연산의 결과 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하는 단계; 및상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는 단계를 포함하는,깊이 추정 방법.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서,제3 뉴럴 네트워크를 이용하여 상기 원본 이미지의 깊이맵을 기초로 에지 검출 다이어그램을 획득하는 단계를 더 포함하는, 깊이 추정 방법.</claim></claimInfo><claimInfo><claim>8. 제1항에 있어서,제4 뉴럴 네트워크를 이용하여 상기 원본 이미지의 깊이맵을 기초로 이미지 분할 다이어그램을 획득하는 단계를 더 포함하는, 깊이 추정 방법.</claim></claimInfo><claimInfo><claim>9. 제5항에 있어서,상기 변위 연산은 상기 페어링된 서브 이미지 각각에 동일한 방향의 변위값 또는 상이한 방향의 변위값을 적용하는,깊이 추정 방법.</claim></claimInfo><claimInfo><claim>10. 제1항에 있어서,상기 N은 4이고, 상기 상위 픽셀에 포함된 4개의 서브 픽셀은 각각 상기 상위 픽셀의 좌상단, 좌하단, 우상단 및 우하단에 배치되고,상기 N개의 서브 이미지는 좌상단의 서브 픽셀에 대응하는 서브 이미지, 좌하단의 서브 픽셀에 대응하는 서브 이미지, 우상단의 서브 픽셀에 대응하는 서브 이미지 및 우하단의 서브 픽셀에 대응하는 서브 이미지를 포함하는,깊이 추정 방법.</claim></claimInfo><claimInfo><claim>11. 하드웨어와 결합되어 제1항의 방법을 실행시키기 위하여 컴퓨터 판독 가능한 기록매체에 저장된 컴퓨터 프로그램.</claim></claimInfo><claimInfo><claim>12. 복수의 상위 픽셀을 포함하고 각 상위 픽셀은 N개의 서브 픽셀을 포함하는 이미지 센서; 및하나 이상의 프로세서를 포함하고,상기 이미지 센서는 원본 이미지를 획득하고,상기 프로세서는,상기 원본 이미지로부터 상기 N개의 서브 픽셀의 각각에 대응하는 N개의 서브 이미지를 획득하고,제1 뉴럴 네트워크를 이용하여 상기 N개의 서브 이미지 간의 시점 차이를 획득하고,제2 뉴럴 네트워크를 이용하여 상기 시점 차이를 기초로 상기 원본 이미지의 깊이 맵을 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서,상기 프로세서는,상기 N개의 서브 이미지를 2개씩 페어링(paring)하고,상기 페어링된 서브 이미지 간의 시점 차이를 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서,상기 프로세서는,상기 페어링된 서브 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하고,상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>15. 제13항에 있어서,상기 프로세서는,상기 페어링된 서브 이미지 각각에 컨벌루션(convolution) 연산을 수행하고,상기 컨벌루션 연산의 결과 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하고,상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>16. 제13항에 있어서,상기 프로세서는,상기 페어링된 서브 이미지 각각에 변위 연산을 수행하고,상기 변위 연산의 결과 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하고,상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>17. 제13항에 있어서,상기 프로세서는,상기 페어링된 서브 이미지 각각에 변위 연산을 수행하고,상기 변위 연산의 결과 이미지 각각에 컨벌루션 연산을 수행하고,상기 컨벌루션 연산의 결과 이미지 간의 대응 위치의 서브 픽셀의 차이값을 획득하고,상기 차이값을 기초로 상기 페어링된 서브 이미지 간의 시점 차이를 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>18. 제12항에 있어서,상기 프로세서는,제3 뉴럴 네트워크를 이용하여 상기 원본 이미지의 깊이맵을 기초로 에지 검출 다이어그램을 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>19. 제12항에 있어서,상기 프로세서는,제4 뉴럴 네트워크를 이용하여 상기 원본 이미지의 깊이맵을 기초로 이미지 분할 다이어그램을 획득하는,깊이 추정 장치.</claim></claimInfo><claimInfo><claim>20. 제16항에 있어서,상기 변위 연산은 상기 페어링된 서브 이미지 각각에 동일한 방향의 변위값 또는 상이한 방향의 변위값을 적용하는,깊이 추정 장치.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>중국 베이징시 차오양구...</address><code> </code><country> </country><engName>Jiaqian YU</engName><name>지아키안 유</name></inventorInfo><inventorInfo><address>중국 베이징시 차오양구...</address><code> </code><country> </country><engName>Jingtao XU</engName><name>징타오 수</name></inventorInfo><inventorInfo><address>중국 베이징시 차오양구...</address><code> </code><country> </country><engName>Yiwei CHEN</engName><name>이웨이 첸 </name></inventorInfo><inventorInfo><address>서울특별시 강남구...</address><code>420170743681</code><country>대한민국</country><engName>YOO BYUNG IN</engName><name>유병인</name></inventorInfo><inventorInfo><address>경기도 성남시 분당구...</address><code>420170475655</code><country>대한민국</country><engName>CHOI, Chang Kyu</engName><name>최창규</name></inventorInfo><inventorInfo><address>경기도 수원시 권선구...</address><code>420180507481</code><country>대한민국</country><engName>LEE, Hana</engName><name>이한아</name></inventorInfo><inventorInfo><address>서울특별시 서초구...</address><code>420170731631</code><country>대한민국</country><engName>Han, Jae Joon</engName><name>한재준</name></inventorInfo><inventorInfo><address>중국 베이징시 차오양구...</address><code> </code><country> </country><engName>Qiang WANG</engName><name>퀴앙 왕</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 언주로 ***, *층(역삼동,화물재단빌딩)</address><code>920071000614</code><country>대한민국</country><engName>MUHANN PATENT &amp; LAW FIRM</engName><name>특허법인무한</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>중국</priorityApplicationCountry><priorityApplicationDate>2020.01.16</priorityApplicationDate><priorityApplicationNumber>202010046284.3</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2020.12.10</receiptDate><receiptNumber>1-1-2020-1342173-18</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>우선권주장증명서류제출서(CN)</documentName><receiptDate>2020.12.14</receiptDate><receiptNumber>9-1-2020-9011987-65</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2023.11.28</receiptDate><receiptNumber>1-1-2023-1327629-43</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020200172465.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9373b27e60fad37edc65b27f583728e6eea7bff7935f806d9aad0e145e837b4c4ad36bcd2e1b49f1ea5e45c2af7754b29ba5b2a3a036ec16f5</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf0a6fea1a58e91aa86d8c032ac881751c60ce3cbd669f16700108401390566303ba72ac2a70598956bf36babeb0b22f848bb193eecf27d24e</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>