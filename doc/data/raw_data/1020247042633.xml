<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:38:27.3827</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2019.09.04</applicationDate><applicationFlag>특허</applicationFlag><applicationNumber>10-2024-7042633</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>스테레오 추출을 사용하는 3차원 장면 인페인팅</inventionTitle><inventionTitleEng>THREE DIMENSIONAL SCENE INPAINTING USING STEREO EXTRACTION</inventionTitleEng><openDate>2025.01.09</openDate><openNumber>10-2025-0005536</openNumber><originalApplicationDate>2019.09.04</originalApplicationDate><originalApplicationKind>국제출원/분할</originalApplicationKind><originalApplicationNumber>10-2021-7012080</originalApplicationNumber><originalExaminationRequestDate>2024.12.23</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.12.23</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 15/10</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/136</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/77</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo><familyApplicationNumber>1020217012080</familyApplicationNumber></familyInfo></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 발명은, 상이한 관점들을 갖는 한 쌍의 2차원(2D) 이미지들로부터 개선된 시각적 특징들을 갖는 3차원 장면들을 렌더링하기 위한 시스템들 및 방법들에 관한 것이다. 3D 장면은, 제1 관점으로부터 장면 객체의 제1의 2차원(2D) 이미지를 획득하고, 제1 관점과 상이한 제2 관점으로부터 장면 객체의 제2의 2D 이미지를 획득하고, 제1의 2D 이미지 및 제2의 2D 이미지로부터 깊이 맵을 생성하고, 깊이 맵 및 제1의 2D 이미지 및 제2의 2D 이미지로부터 3D 장면을 생성하고, 불완전한 이미지 정보를 갖는 초기 3D 장면의 구역들을 검출하고, 3D 장면의 검출된 구역들을 재구성하고, 교체 정보를 결정하고 재구성된 구역들을 수정하고, 그리고 복수의 관점들로부터 수정된 재구성된 구역들을 갖는 3D 장면을 렌더링함으로써 생성된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2020.04.02</internationOpenDate><internationOpenNumber>WO2020068383</internationOpenNumber><internationalApplicationDate>2019.09.04</internationalApplicationDate><internationalApplicationNumber>PCT/US2019/049455</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 3차원(3D) 장면을 생성하기 위한 시스템으로서,제1 이미저(imager) 및 상기 제1 이미저로부터 이격된 제2 이미저를 포함하는 아이웨어(eyewear) — 상기 제1 이미저는 상기 제1 관점으로부터 장면 객체(scene object)의 제1의 2차원(2D) 이미지를 획득하도록 구성되고, 상기 제2 이미저는 상기 제1 관점과 상이한 제2 관점으로부터 상기 장면 객체의 제2의 2D 이미지를 획득하도록 구성됨 — ;상기 아이웨어에 커플링된 프로세싱 시스템을 포함하고, 상기 프로세싱 시스템은: 상기 제1의 2D 이미지 및 상기 제2의 2D 이미지를 획득하고; 상기 제1의 2D 이미지 및 상기 제2의 2D 이미지로부터 깊이 맵(depth map)을 생성하고; 상기 깊이 맵 및 상기 제1의 2D 이미지 및 상기 제2의 2D 이미지로부터 3D 장면을 생성하고; 불완전한 이미지 정보로 갖는 초기 3D 장면의 구역들을 검출하고; 상기 3D 장면의 상기 검출된 구역들을 재구성하고; 교체 정보(replacement information)를 결정하고, 상기 재구성된 구역들을 수정하고; 그리고 복수의 관점들로부터 상기 수정된 재구성된 구역들을 갖는 상기 3D 장면을 렌더링하도록 구성되는, 3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서, 상기 프로세싱 시스템은 추가로:하나 이상의 관점들로부터 상기 렌더링된 3D 장면에서 구멍들(holes)을 식별하고; 그리고상기 구멍들을 채우기 위해 상기 렌더링된 3D 장면을 개선(refine)하도록 구성되는, 3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>3. 제1 항에 있어서, 상기 아이웨어는 제1 템플(temple) 및 제2 템플을 포함하고, 상기 제1 이미저는 상기 제1 템플에 인접하고, 상기 제2 이미저는 상기 제2 템플에 인접한, 3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>4. 제1 항에 있어서, 불완전한 이미지 정보를 갖는 상기 검출된 구역들에 대한 교체 정보를 결정하기 위해, 상기 프로세싱 시스템은:각각의 검출된 구역을 둘러싸는 경계(boundary)를 식별하고; 경계들에서 배경 정보(background information)를 식별하고;상기 경계들에서 전경 정보(foreground information)를 식별하고; 그리고전경 경계 정보보다 더 높은 가중치를 배경 경계 정보에 부여하여, 개개의 구역들을 통해 상기 배경 경계 정보 및 상기 전경 경계 정보를 블렌딩(blend)하도록 구성되는,3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>5. 제4 항에 있어서, 상기 배경 경계 정보 및 상기 전경 경계 정보를 블렌딩하기 위해, 상기 프로세싱 시스템은:상기 개개의 구역들을 통해 상기 배경 경계 정보로부터 누락 정보를 상기 전경 경계 정보 내로 확산(diffuse)시키도록 구성되는,3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>6. 제1 항에 있어서, 상기 깊이 맵은 픽셀 꼭짓점들(pixel vertices) 및 상기 제1의 2D 이미지 및 상기 제2의 2D 이미지로부터의 대응하는 이미지 정보를 포함하고, 상기 3D 장면을 생성하기 위해, 상기 프로세싱 시스템은 상기 꼭짓점들을 연결하여 제1 면들(faces)을 형성하도록 구성되고, 그리고 상기 검출된 구역들을 재구성하기 위해, 상기 프로세싱 시스템은 상기 경계 구역들의 꼭짓점들을 연결하여 제2 면들을 형성하도록 구성되고, 상기 제2 면들은 상기 제1 면들과 상이한, 3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>7. 제6 항에 있어서, 상기 제1 면들 및 제2 면들은 삼각형 면들 또는 사각형 면들 중 적어도 하나를 포함하는,3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>8. 제1 항에 있어서, 상기 깊이 맵은 픽셀 꼭짓점들 및 각각의 꼭짓점에 대응하는 신뢰도 값들(confidence values)을 포함하고, 상기 3D 장면을 생성하기 위해, 상기 프로세싱 시스템은 상기 꼭짓점들을 연결하여 면들을 형성하도록 구성되고, 그리고 불완전한 정보를 갖는 상기 3D 장면의 구역들을 검출하기 위해, 상기 프로세싱 시스템은 변질된 면들(degenerated faces) 또는 낮은 신뢰도 면들 중 적어도 하나를 포함하는 연속적인 면들(contiguous faces)을 식별하도록 구성되는,3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>9. 제8 항에 있어서, 상기 변질된 면들 각각은 임계값 미만인 적어도 하나의 각도를 갖는,3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>10. 제8 항에 있어서, 각각의 낮은 신뢰도 면은 상기 제1의 2D 이미지와 상기 제2의 2D 이미지 사이의 불일치 값들(inconsistent values)로 생성된 적어도 하나의 꼭짓점을 포함하는,3D 장면을 생성하기 위한 시스템.</claim></claimInfo><claimInfo><claim>11. 3차원(3D) 장면을 생성하기 위한 방법으로서,제1 관점으로부터 장면 객체의 제1의 2차원(2D) 이미지를 획득하는 단계;상기 제1 관점과 상이한 제2 관점으로부터 상기 장면 객체의 제2의 2D 이미지를 획득하는 단계;상기 제1의 2D 이미지 및 상기 제2의 2D 이미지로부터 깊이 맵을 생성하는 단계;상기 깊이 맵 및 상기 제1의 2D 이미지 및 상기 제2의 2D 이미지로부터 3D 장면을 생성하는 단계;불완전한 이미지 정보를 갖는 초기 3D 장면의 구역들을 검출하는 단계;상기 3D 장면의 상기 검출된 구역들을 재구성하는 단계;교체 정보를 결정하고, 상기 재구성된 구역들을 수정하는 단계; 및복수의 관점들로부터 상기 수정된 재구성된 구역들을 갖는 상기 3D 장면을 렌더링하는 단계를 포함하는, 3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>12. 제11 항에 있어서,하나 이상의 관점들로부터 상기 렌더링된 3D 장면에서 구멍들을 식별하는 단계; 및상기 구멍들을 채우기 위해 상기 렌더링된 3D 장면을 개선하는 단계를 더 포함하는, 3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>13. 제11 항에 있어서, 상기 제1의 2D 이미지는 아이웨어의 제1 템플에 인접한 제1 이미저로부터 획득되고, 그리고 상기 제2의 2D 이미지는 상기 아이웨어의 제2 템플에 인접한 제2 이미저로부터 획득되는, 3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>14. 제11 항에 있어서, 상기 결정하는 단계는:각각의 검출된 구역을 둘러싸는 경계를 식별하는 단계; 경계들에서 배경 정보를 식별하는 단계;상기 경계들에서 전경 정보를 식별하는 단계; 및전경 경계 정보보다 더 높은 가중치를 배경 경계 정보에 부여하여, 개개의 구역들을 통해 상기 배경 경계 정보 및 상기 전경 경계 정보를 블렌딩하는 단계를 포함하는,3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>15. 제14 항에 있어서, 상기 블렌딩하는 단계는:상기 개개의 구역들을 통해 상기 배경 경계 정보로부터 누락 정보를 상기 전경 경계 정보 내로 확산시키는 단계를 포함하는,3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>16. 제11 항에 있어서, 상기 3D 장면을 생성하는 단계는, 픽셀 꼭짓점들 및 상기 제1의 2D 이미지 및 상기 제2의 2D 이미지로부터의 대응하는 이미지 정보를 포함하는 깊이 맵을 생성하는 단계, 및 상기 꼭짓점들을 연결하여 제1 면들을 형성하는 단계를 포함하고, 그리고 상기 재구성하는 단계는:상기 경계 구역들의 꼭지점들을 연결하여 제2 면들을 형성하는 단계를 더 포함하고, 상기 제2 면들은 상기 제1 면들과 상이한, 3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>17. 제16 항에 있어서, 상기 제1 면들 및 제2 면들은 삼각형 면들 또는 사각형 면들 중 적어도 하나를 포함하는,3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>18. 제11 항에 있어서, 상기 깊이 맵은 픽셀 꼭짓점들 및 상기 제1의 2D 이미지 및 상기 제2의 2D 이미지로부터의 대응하는 이미지 정보를 포함하고, 상기 3D 장면을 생성하는 단계는 상기 꼭짓점들을 연결하여 다중-각도 면들을 형성하는 단계를 포함하고, 상기 검출하는 단계는:변질된 면들 또는 낮은 신뢰도 면들 중 적어도 하나를 포함하는 연속적인 면들을 식별하는 단계를 포함하는,3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>19. 제18 항에 있어서, 상기 변질된 면들 각각은 임계치 미만인 적어도 하나의 각도를 갖는,3D 장면을 생성하기 위한 방법.</claim></claimInfo><claimInfo><claim>20. 제18 항에 있어서, 각각의 낮은 신뢰도 면은, 상기 제1의 2D 이미지와 상기 제2의 2D 이미지 사이의 임계값을 초과하는 불일치 값들로 생성된 적어도 하나의 꼭짓점을 포함하는,3D 장면을 생성하기 위한 방법.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 캘리포니아주 산타 모니카 써티퍼스트 스트리트 ****</address><code>520140253774</code><country>미국</country><engName>SNAP INC.</engName><name>스냅 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>이스라엘</country><engName>DAUBE, Nir</engName><name>다우베, 니르</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country>이스라엘</country><engName>KARNI, Zachi</engName><name>카르니, 자치</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 서소문로 **, *층(서소문동)</address><code>920241000417</code><country>대한민국</country><engName>NAM IP GROUP</engName><name>특허법인(유)남아이피그룹</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2018.09.27</priorityApplicationDate><priorityApplicationNumber>62/737,280</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Divisional Application(International Application)] Patent Application</documentEngName><documentName>[분할출원(국제출원)]특허출원서</documentName><receiptDate>2024.12.23</receiptDate><receiptNumber>1-1-2024-1429531-72</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2024.12.24</receiptDate><receiptNumber>1-1-2024-1432298-09</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247042633.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93816f0a5464987b153abbb3fef236ebb684b7ddd05127094681d49271d94f80a4bc3c9fcdf58da60d5b2b08c0ef39c75ff198ca2ef56bbe6f</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf376d170fe022674c0d8af29e877eab21ecac979a87797068c48e59cf992e6dd84cd646b07dda2f14b9f867e796c483de49d83e8bc8a0703f</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>