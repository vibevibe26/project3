<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:06:17.617</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.04.06</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0045503</applicationNumber><claimCount>21</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>비디오 인페인팅을 수행하는 전자 장치 및 그 동작 방법</inventionTitle><inventionTitleEng>Electronic device performing video inpainting  and method of operating the same</inventionTitleEng><openDate>2024.05.08</openDate><openNumber>10-2024-0062080</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/77</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 5/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2024.01.01)</ipcDate><ipcNumber>G06T 3/02</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/045</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0455</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 전자 장치가 비디오 인페인팅(video inpainting)을 수행하는 방법이 개시된다. 본 비디오 인페인팅을 수행하는 방법은 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 인페인팅인 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈들 및 타겟 동영상의 복수의 프레임의 이미지들 사이에서 획득되는 특징에 기초한 인페인팅인 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함하는 인페인팅 모델에 적용함으로써, 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는 단계를 포함하고, 인페인팅 모델은, 제1 훈련 전의 복수의 제1 모듈들 및 단일 이미지에 대해 공간적 어텐션을 수행하는 제1 훈련 전의 복수의 제2 모듈들에 대하여 제1 훈련이 수행되고, 제1 훈련이 수행된 복수의 제1 모듈들 및 제1 훈련이 수행된 복수의 제2 모듈들에 대하여 제2 훈련이 수행된 인공 지능 모델이고, 제1 훈련은, 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지를 획득하기 위한 훈련이며, 제2 훈련은, 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 제2 복원 대상 영역이 복원된 훈련용 동영상을 획득하기 위한 훈련이다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 전자 장치가 비디오 인페인팅(video inpainting)을 수행하는 방법에 있어서,타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 상기 타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈들 및 상기 타겟 동영상의 복수의 프레임들의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함하는 인페인팅 모델에 적용함으로써, 복원된 상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는 단계;를 포함하고,상기 인페인팅 모델은,제1 훈련(300) 전의 상기 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 상기 제1 훈련(300) 전의 상기 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)에 대하여 상기 제1 훈련(300)이 수행되고,상기 제1 훈련(300)이 수행된 상기 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 상기 제1 훈련(300)이 수행된 상기 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)에 대하여 제2 훈련(350)이 수행된 인공 지능 모델이고,상기 제1 훈련(300)은,제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 상기 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지를 획득하기 위한 훈련이며,상기 제2 훈련(350)은,제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 상기 제2 복원 대상 영역이 복원된 훈련용 동영상을 획득하기 위한 훈련인, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서상기 타겟 동영상을 획득하는 단계는,상기 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상에 기초하여 복수의 프레임들의 제1 토큰들을 획득하는 단계;상기 복수의 프레임들의 제1 토큰들을 상기 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 상기 시-공간적 어텐션이 수행된 상기 복수의 프레임들의 제2 토큰들을 획득하는 단계;상기 복수의 프레임들 각각의 제2 토큰들을 상기 복수의 제1 모듈들 중 적어도 하나에 적용함으로써, 상기 공간적 어텐션이 수행된 상기 복수의 프레임들의 각각의 제3 토큰들을 획득하는 단계;상기 복수의 프레임들의 제3 토큰들에 기초하여 상기 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는 단계;를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항 내지 제2항 중 어느 한 항에 있어서, 상기 인페인팅 모델은,상기 복수의 프레임들 중 서로 인접한 프레임들의 이미지 사이에서 획득되는 특징을 추출하기 위한 복수의 제3 모듈들을 더 포함하고,상기 방법은,상기 복수의 프레임들의 제2 토큰들을 상기 복수의 제3 모듈들 중 적어도 하나에 적용함으로써, 상기 인접한 프레임들의 이미지 사이에서 존재하는 움직임(motion)에 대한 정보를 포함하는 상기 복수의 프레임들의 제1 피처맵들을 획득하는 단계; 를 더 포함하고,상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계는,상기 복수의 프레임들 각각의 제2 토큰들 및 상기 복수의 프레임들 각각의 제1 피처맵을 상기 복수의 제1 모듈들 중 적어도 하나에 적용함으로써, 상기 복수의 프레임들 각각의 제3 토큰을 획득하는 단계;를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제1항 내지 제3항 중 어느 한 항에 있어서,상기 복수의 프레임들의 제1 피처맵들을 획득하는 단계는,상기 복수의 프레임들의 제2 토큰들 중 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 다음 프레임인 제2 프레임의 제2 토큰들에 기초하여 상기 복수의 프레임들의 제1 피처맵들 중 상기 제1 프레임의 제1 피처맵을 획득하는 단계;를 포함하고,상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계는,상기 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 제1 피처맵 중 적어도 하나에 기초하여 상기 복수의 프레임들의 제3 토큰들 중 상기 제1 프레임의 제3 토큰들을 획득하는 단계;를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항 내지 제4항 중 어느 한 항에 있어서,상기 제1 프레임의 제3 토큰들을 획득하는 단계는,상기 제1 프레임의 제1 피처맵을 토큰화(tokenization)하여 상기 제1 프레임의 제4 토큰들을 획득하는 단계; 및상기 제1 프레임의 제4 토큰들 및 상기 제1 프레임의 제2 토큰들에 기초하여 상기 제1 프레임의 제3 토큰들을 획득하는 단계;를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5항 중 어느 한 항에 있어서,상기 제1 프레임의 제3 토큰들을 획득하는 단계는,상기 제1 프레임의 제2 토큰들을 디토큰화(de-tokenization)하여 상기 제1 프레임의 제2 피처맵을 획득하는 단계;상기 제1 프레임의 제1 피처맵 및 상기 제1 프레임의 제2 피처맵에 기초하여 상기 제1 프레임의 제3 피처맵을 획득하는 단계; 및 상기 제1 프레임의 제3 피처맵을 토큰화하여 상기 제1 프레임의 제3 토큰들을 획득하는 단계;를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항 내지 제6항 중 어느 한 항에 있어서,상기 인페인팅 모델은,상기 제1 훈련 전의 상기 복수의 제1 모듈들 중 하나 이상 및 상기 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 상기 제1 훈련 전의 상기 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 상기 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여 상기 제1 훈련이 수행되고,상기 제1 훈련이 수행된 상기 복수의 제1 모듈들 중 하나 이상 및 상기 제1 훈련이 수행된 상기 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여 상기 제2 훈련이 수행된 인공 지능 모델인, 방법.</claim></claimInfo><claimInfo><claim>8. 제1항 내지 제7항 중 어느 한 항에 있어서,상기 인페인팅 모델은,제1 손실 함수 및 제2 손실 함수 중 적어도 하나에 기초하여 제2 훈련이 수행된 인공 지능 모델이고,상기 제1 손실 함수는,상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이의 옵티컬 플로우(optical flow)에 기초하여 계산되고,상기 제2 손실 함수는,상기 제2 복원 대상 영역을 포함하는 훈련용 동영상 및 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 아핀 변환(affine transformation)을 수행하는 것에 기초하여 계산되는, 방법.</claim></claimInfo><claimInfo><claim>9. 제1항 내지 제8항 중 어느 한 항에 있어서,상기 옵티컬 플로우는,상기 제2 훈련 전의 상기 복수의 제3 모듈들로부터 획득되는 상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보에 기초하여 획득되는, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항 내지 제9항 중 어느 한 항에 있어서,상기 방법은,상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라의 움직임의 크기, 상기 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도 하나를 식별하는 단계;를 더 포함하고,상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계는,상기 카메라의 움직임의 크기, 상기 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도 하나에 기초하여 제1 추론 모드 및 제2 추론 모드 중 하나를 선택하여 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계;를 포함하고,상기 제1 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰들 및 상기 복수의 프레임들 각각의 제1 피처맵에 기초하여 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 모드이고,상기 제2 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰들에 기초하여 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 모드인, 방법.</claim></claimInfo><claimInfo><claim>11. 비디오 인페인팅(video inpainting)을 수행하는 전자 장치에 있어서,하나 이상의 명령어들(instructions)을 저장하는 메모리; 및상기 메모리에 저장된 하나 이상의 명령어들을 실행하는 적어도 하나의 프로세서를 포함하며,상기 적어도 하나의 프로세서는,타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 상기 타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈들 및 상기 타겟 동영상의 복수의 프레임의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함하는 인페인팅 모델에 적용함으로써, 복원된 상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하고,상기 인페인팅 모델은,제1 훈련(300) 전의 상기 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및, 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 상기 제1 훈련(300) 전의 상기 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)에 대하여 상기 제1 훈련(300)이 수행되고,상기 제1 훈련(300)이 수행된 상기 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 상기 제1 훈련(300)이 수행된 상기 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)에 대하여 제2 훈련(350)이 수행된 인공 지능 모델이고,상기 제1 훈련(300)은,제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 상기 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지를 획득하기 위한 훈련이며,상기 제2 훈련(350)은,제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 상기 제2 복원 대상 영역이 복원된 훈련용 동영상을 획득하기 위한 훈련인, 전자 장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서상기 적어도 하나의 프로세서는,상기 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상에 기초하여 복수의 프레임들의 제1 토큰들을 획득하고,상기 복수의 프레임들의 제1 토큰들을 상기 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 상기 시-공간적 어텐션이 수행된 상기 복수의 프레임들의 제2 토큰들을 획득하고,상기 복수의 프레임들 각각의 제2 토큰들을 상기 복수의 제1 모듈들 중 적어도 하나에 적용함으로써, 상기 공간적 어텐션이 수행된 상기 복수의 프레임들 각각의 제3 토큰들을 획득하고,상기 복수의 프레임들의 제3 토큰들에 기초하여 상기 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>13. 제11항 내지 제12항 중 어느 한 항에 있어서, 상기 인페인팅 모델은,상기 복수의 프레임들 중 서로 인접한 프레임들의 이미지 사이에서 획득되는 특징을 추출하기 위한 복수의 제3 모듈들을 더 포함하고,상기 적어도 하나의 프로세서는,상기 복수의 프레임들의 상기 제2 토큰들을 상기 복수의 제3 모듈들 중 적어도 하나에 적용함으로써, 상기 인접한 프레임들의 이미지 사이에서 존재하는 움직임(motion)에 대한 정보를 포함하는 상기 복수의 프레임들의 제1 피처맵들을 획득하고,상기 복수의 프레임들 각각의 제2 토큰들 및 상기 복수의 프레임들 각각의 제1 피처맵을 상기 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>14. 제11항 내지 제13항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 복수의 프레임들의 제2 토큰들 중 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 다음 프레임인 제2 프레임의 제2 토큰들에 기초하여 상기 복수의 프레임들의 제1 피처맵들 중 상기 제1 프레임의 제1 피처맵을 획득하고,상기 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 제1 피처맵 중 적어도 하나에 기초하여 상기 복수의 프레임들의 제3 토큰들 중 상기 제1 프레임의 제3 토큰들을 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>15. 제11항 내지 제14항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 제1 프레임의 제1 피처맵을 토큰화(tokenization)하여 상기 제1 프레임의 제4 토큰들을 획득하고,상기 제1 프레임의 제4 토큰들 및 상기 제1 프레임의 제2 토큰들에 기초하여 상기 제1 프레임의 제3 토큰들을 획득하는, 전자 장치. </claim></claimInfo><claimInfo><claim>16. 제11항 내지 제15항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 제1 프레임의 제2 토큰들을 디토큰화(de-tokenization)하여 상기 제1 프레임의 제2 피처맵을 획득하고,상기 제1 프레임의 제1 피처맵 및 상기 제1 프레임의 제2 피처맵에 기초하여 상기 제1 프레임의 제3 피처맵을 획득하고,상기 제1 프레임의 제3 피처맵을 토큰화하여 상기 제1 프레임의 제3 토큰들을 획득하는, 전자 장치.</claim></claimInfo><claimInfo><claim>17. 제11항 내지 제16항 중 어느 한 항에 있어서,상기 인페인팅 모델은,상기 제1 훈련 전의 상기 복수의 제1 모듈들 중 하나 이상 및 상기 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 상기 제1 훈련 전의 상기 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 상기 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여 상기 제1 훈련이 수행되고,상기 제1 훈련이 수행된 복수의 제1 모듈들 중 하나 이상 및 상기 제1 훈련이 수행된 상기 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여 상기 제2 훈련이 수행된 인공 지능 모델인, 전자 장치.</claim></claimInfo><claimInfo><claim>18. 제11항 내지 제17항 중 어느 한 항에 있어서,상기 인페인팅 모델은,제1 손실 함수 및 제2 손실 함수 중 적어도 하나에 기초하여 제2 훈련이 수행된 인공 지능 모델이고,상기 제1 손실 함수는,상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이의 옵티컬 플로우(optical flow)에 기초하여 계산되고,상기 제2 손실 함수는,상기 제2 복원 대상 영역을 포함하는 훈련용 동영상 및 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 아핀 변환(affine transformation)을 수행하는 것에 기초하여 계산되는, 전자 장치.</claim></claimInfo><claimInfo><claim>19. 제11항 내지 제18항 중 어느 한 항에 있어서,상기 옵티컬 플로우는,상기 제2 훈련 전의 상기 복수의 제3 모듈들로부터 획득되는 상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보에 기초하여 획득되는, 전자 장치.</claim></claimInfo><claimInfo><claim>20. 제11항 내지 제19항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라의 움직임의 크기, 상기 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도 하나를 식별하고,상기 카메라의 움직임의 크기, 상기 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도 하나에 기초하여 제1 추론 모드 및 제2 추론 모드 중 하나를 선택하여 상기 복수의 프레임들 각각의 제3 토큰을 획득하며, 상기 제1 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰 및 상기 복수의 프레임들 각각의 제1 피처맵에 기초하여 상기 복수의 프레임들 각각의 제3 토큰을 획득하는 모드이고,상기 제2 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰에 기초하여 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 모드인, 전자 장치.</claim></claimInfo><claimInfo><claim>21. 제1항 내지 제10항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>KONG, Nae Jin</engName><name>공내진</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>GOKA, Harshith</engName><name>고카 하쉿</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>CHO, Ho Chul</engName><name>조호철</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.11.01</priorityApplicationDate><priorityApplicationNumber>1020220143949</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.04.06</receiptDate><receiptNumber>1-1-2023-0389852-26</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230045503.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93a4f079e4cfcbe8d250c5386a4e24d2eeadf9a9c9aa43de7c3d8b365d3577cfb634a7cd53c9c48f2775867c767b32df363ad8b04c489c52da</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf7d5da1c645db27287f544e332b7775cd1b60ee35d388d3f261e502c885c6bb8949cab1f8e0181f5f9e3718da8b2edfc74962f6e71bca7154</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>