<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:40:36.4036</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.01.27</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7030157</applicationNumber><claimCount>52</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>P-프레임 코딩 시스템을 이용한 학습된 B-프레임 코딩</inventionTitle><inventionTitleEng>LEARNED B-FRAME CODING USING P-FRAME CODING SYSTEM</inventionTitleEng><openDate>2023.11.07</openDate><openNumber>10-2023-0154022</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.01.10</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.09.04</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/577</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/587</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/573</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/105</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/172</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2014.01.01)</ipcDate><ipcNumber>H04N 19/139</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0455</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/0464</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 단방향 코딩 시스템 및 보간된 참조 프레임을 사용하여 학습된 양방향 코딩을 수행하는 것 등에 의해, 비디오 데이터를 프로세싱하기 위한 기법들이 설명된다. 예를 들어, 프로세스는 제 1 참조 프레임 및 제 2 참조 프레임을 획득하는 단계를 포함할 수 있다. 그 프로세스는, 적어도 부분적으로, 제 1 참조 프레임과 제 2 참조 프레임 사이의 보간을 수행함으로써, 제 3 참조 프레임을 생성하는 단계를 포함할 수 있다. 그 프로세스는, 예컨대 입력 프레임과 제 3 참조 프레임 사이의 모션을 추정함으로써, 제 3 참조 프레임에 기초하여 입력 프레임에 대해 단방향 인터 예측을 수행하는 단계, 및, 적어도 부분적으로, 추정된 모션에 기초하여 제 3 참조 프레임의 하나 이상의 픽셀들을 워핑함으로써, 워핑된 프레임을 생성하는 단계를 포함할 수 있다. 그 프로세스는 워핑된 프레임 및 예측된 잔차에 기초하여, 입력 프레임을 나타내는 재구성된 프레임을 생성하는 단계를 포함할 수 있고, 그 재구성된 프레임은 양방향-예측된 프레임을 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.09.15</internationOpenDate><internationOpenNumber>WO2022191933</internationOpenNumber><internationalApplicationDate>2022.01.27</internationalApplicationDate><internationalApplicationNumber>PCT/US2022/014143</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 비디오 데이터를 프로세싱하기 위한 시스템으로서,상기 시스템은:보간 엔진 및 단방향 코딩 엔진을 포함하는 양방향 코딩 엔진을 포함하고,상기 보간 엔진은: 제 1 참조 프레임 및 제 2 참조 프레임을 획득하고; 그리고 적어도 부분적으로, 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 보간을 수행하는 것에 의해, 제 3 참조 프레임을 생성하도록구성되고; 그리고상기 단방향 코딩 엔진은: 상기 제 3 참조 프레임에 기초하여 입력 프레임에 대한 단방향 인터 예측을 수행하도록구성되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>2. 제 1 항에 있어서,상기 단방향 코딩 엔진은 모션 엔진 및 워핑 엔진을 포함하고, 상기 모션 엔진은 상기 입력 프레임과 상기 제 3 참조 프레임 사이의 모션을 추정하도록 구성되고; 그리고상기 워핑 엔진은, 적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 3 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 워핑된 프레임을 생성하도록 구성되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>3. 제 2 항에 있어서,상기 단방향 코딩 엔진은 잔차 엔진을 더 포함하고, 상기 잔차 엔진은: 적어도 부분적으로, 상기 입력 프레임과 상기 워핑된 프레임 사이의 차이를 결정하는 것에 의해, 잔차를 결정하고; 그리고 상기 잔차를 이용하여 예측된 잔차를 생성하도록구성되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>4. 제 3 항에 있어서,상기 시스템은, 메모리 및 상기 메모리에 커플링된 적어도 하나의 프로세서를 더 포함하고, 상기 단방향 코딩 엔진은: 상기 워핑된 프레임 및 상기 예측된 잔차에 기초하여, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하는 것으로서, 상기 재구성된 프레임은 양방향 예측된 프레임을 포함하는, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하는 것을 행하도록 구성되고; 그리고 상기 적어도 하나의 프로세서는 상기 재구성된 프레임이 상기 메모리에 저장되게 하도록 구성되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>5. 제 4 항에 있어서,상기 적어도 하나의 프로세서는 뉴럴 프로세싱 유닛 (NPU) 을 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>6. 제 3 항에 있어서,상기 모션 엔진은 제 1 뉴럴 네트워크를 포함하고, 상기 잔차 엔진은 제 2 뉴럴 네트워크를 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>7. 제 6 항에 있어서,상기 제 1 뉴럴 네트워크는 제 1 오토인코더를 포함하고, 상기 제 2 뉴럴 네트워크는 제 2 오토인코더를 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>8. 제 1 항에 있어서,상기 단방향 코딩 엔진은: 제 4 참조 프레임을 획득하고; 추가 입력 프레임과 상기 제 4 참조 프레임 사이의 모션을 추정하며; 적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 4 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 추가 워핑된 프레임을 생성하고; 그리고 상기 추가 워핑된 프레임 및 추가 예측된 잔차에 기초하여, 상기 추가 입력 프레임을 나타내는 재구성된 프레임을 생성하도록구성되고,상기 추가 입력 프레임을 나타내는 상기 재구성된 프레임은 단방향 예측된 프레임을 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>9. 제 1 항에 있어서,적어도 부분적으로, 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 보간을 수행하는 것에 의해, 상기 제 3 참조 프레임을 생성하기 위해, 상기 보간 엔진은: 상기 제 1 참조 프레임으로부터 상기 제 3 참조 프레임으로의 픽셀 모션을 나타내는 모션 정보의 제 1 세트를 결정하고; 상기 제 2 참조 프레임으로부터 상기 제 3 참조 프레임으로의 픽셀 모션을 나타내는 모션 정보의 제 2 세트를 결정하며; 적어도 부분적으로, 상기 모션 정보의 제 1 세트를 사용하여 상기 제 1 참조 프레임에 대해 워핑 함수를 수행하는 것에 의해, 제 1 워핑 정보를 생성하고; 적어도 부분적으로, 상기 모션 정보의 제 2 세트를 사용하여 상기 제 2 참조 프레임에 대해 상기 워핑 함수를 수행하는 것에 의해, 제 2 워핑 정보를 생성하고; 그리고 상기 제 1 워핑 정보 및 상기 제 2 워핑 정보에 기초하여 상기 제 3 참조 프레임을 생성하도록구성되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>10. 제 9 항에 있어서,상기 모션 정보의 제 1 세트 및 상기 모션 정보의 제 2 세트는 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 픽셀 모션에 기초하여 결정되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>11. 제 9 항에 있어서,상기 모션 정보의 제 1 세트는 제 1 광학 플로우 맵을 포함하고, 상기 모션 정보의 제 2 세트는 제 2 광학 플로우 맵을 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>12. 제 9 항에 있어서,상기 워핑 함수는 이중 선형 보간 함수를 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>13. 제 2 항에 있어서,상기 입력 프레임과 상기 제 3 참조 프레임 사이의 상기 추정된 모션은 광학 플로우 데이터를 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>14. 제 2 항에 있어서,상기 입력 프레임과 상기 제 3 참조 프레임 사이의 상기 추정된 모션은 동적 컨볼루션 데이터를 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>15. 제 2 항에 있어서,상기 입력 프레임과 상기 제 3 참조 프레임 사이의 상기 추정된 모션은 블록 기반 모션 데이터를 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>16. 제 1 항에 있어서,상기 시스템은 모바일 디바이스, 확장 현실 디바이스, 또는 텔레비전 중 하나인, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>17. 제 1 항에 있어서,하나 이상의 비디오 프레임들을 캡처하도록 구성된 카메라 및 디스플레이 중 적어도 하나를 더 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>18. 비디오 데이터를 프로세싱하기 위한 방법으로서,상기 방법은:제 1 참조 프레임 및 제 2 참조 프레임을 획득하는 단계;적어도 부분적으로, 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 보간을 수행하는 것에 의해, 제 3 참조 프레임을 생성하는 단계; 및상기 제 3 참조 프레임에 기초하여 입력 프레임에 대한 단방향 인터 예측을 수행하는 단계를 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>19. 제 18 항에 있어서,상기 입력 프레임과 상기 제 3 참조 프레임 사이의 모션을 추정하는 단계; 및적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 3 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 워핑된 프레임을 생성하는 단계를 더 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>20. 제 19 항에 있어서,적어도 부분적으로, 상기 입력 프레임과 상기 워핑된 프레임 사이의 차이를 결정하는 것에 의해, 잔차를 결정하는 단계; 및상기 잔차를 이용하여 예측된 잔차를 생성하는 단계를 더 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>21. 제 20 항에 있어서,상기 워핑된 프레임 및 상기 예측된 잔차에 기초하여, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하는 단계로서, 상기 재구성된 프레임은 양방향 예측된 프레임을 포함하는, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하는 단계; 및상기 재구성된 프레임이 메모리에 저장되게 하는 단계를 더 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>22. 제 20 항에 있어서,상기 모션은 제 1 오토인코더를 사용하여 추정되고, 상기 예측된 잔차는 제 2 오토인코더를 사용하여 생성되는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>23. 제 18 항에 있어서,제 4 참조 프레임을 획득하는 단계;추가 입력 프레임과 상기 제 4 참조 프레임 사이의 모션을 추정하는 단계;적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 4 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 추가 워핑된 프레임을 생성하는 단계; 및상기 추가 워핑된 프레임 및 추가 예측된 잔차에 기초하여, 상기 추가 입력 프레임을 나타내는 재구성된 프레임을 생성하는 단계를 더 포함하고,상기 추가 입력 프레임을 나타내는 상기 재구성된 프레임은 단방향 예측된 프레임을 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>24. 제 18 항에 있어서,상기 제 1 참조 프레임으로부터 상기 제 3 참조 프레임으로의 픽셀 모션을 나타내는 모션 정보의 제 1 세트를 결정하는 단계;상기 제 2 참조 프레임으로부터 상기 제 3 참조 프레임으로의 픽셀 모션을 나타내는 모션 정보의 제 2 세트를 결정하는 단계;적어도 부분적으로, 상기 모션 정보의 제 1 세트를 사용하여 상기 제 1 참조 프레임에 대해 워핑 함수를 수행하는 것에 의해, 제 1 워핑 정보를 생성하는 단계;적어도 부분적으로, 상기 모션 정보의 제 2 세트를 사용하여 상기 제 2 참조 프레임에 대해 상기 워핑 함수를 수행하는 것에 의해, 제 2 워핑 정보를 생성하는 단계; 및상기 제 1 워핑 정보 및 상기 제 2 워핑 정보에 기초하여 상기 제 3 참조 프레임을 생성하는 단계를 더 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>25. 제 24 항에 있어서,상기 모션 정보의 제 1 세트 및 상기 모션 정보의 제 2 세트는 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 픽셀 모션에 기초하여 결정되는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>26. 제 24 항에 있어서,상기 모션 정보의 제 1 세트는 제 1 광학 플로우 맵을 포함하고, 상기 모션 정보의 제 2 세트는 제 2 광학 플로우 맵을 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>27. 제 24 항에 있어서,상기 워핑 함수는 이중 선형 보간 함수를 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>28. 제 19 항에 있어서,상기 입력 프레임과 상기 제 3 참조 프레임 사이의 상기 추정된 모션은 광학 플로우 데이터를 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>29. 제 19 항에 있어서,상기 입력 프레임과 상기 제 3 참조 프레임 사이의 상기 추정된 모션은 동적 컨볼루션 데이터를 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>30. 제 19 항에 있어서,상기 입력 프레임과 상기 제 3 참조 프레임 사이의 상기 추정된 모션은 블록 기반 모션 데이터를 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>31. 명령들을 저장한 비일시적 컴퓨터 판독가능 저장 매체로서,상기 명령들은, 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금:제 1 참조 프레임 및 제 2 참조 프레임을 획득하게 하고;적어도 부분적으로, 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 보간을 수행하는 것에 의해, 제 3 참조 프레임을 생성하게 하며; 그리고상기 제 3 참조 프레임에 기초하여 입력 프레임에 대한 단방향 인터 예측을 수행하게 하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>32. 제 31 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 입력 프레임과 상기 제 3 참조 프레임 사이의 모션을 추정하게 하고; 그리고 적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 3 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 워핑된 프레임을 생성하게 하는명령들을 더 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>33. 제 32 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 적어도 부분적으로, 상기 입력 프레임과 상기 워핑된 프레임 사이의 차이를 결정하는 것에 의해, 잔차를 결정하게 하고; 그리고 상기 잔차를 이용하여 예측된 잔차를 생성하게 하는명령들을 더 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>34. 제 33 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 상기 워핑된 프레임 및 상기 예측된 잔차에 기초하여, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하는 것으로서, 상기 재구성된 프레임은 양방향 예측된 프레임을 포함하는, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하는 것을 행하게 하고; 그리고 상기 재구성된 프레임이 메모리에 저장되게 하는명령들을 더 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>35. 제 33 항에 있어서,상기 모션은 제 1 오토인코더를 사용하여 추정되고, 상기 예측된 잔차는 제 2 오토인코더를 사용하여 생성되는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>36. 제 31 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 제 4 참조 프레임을 획득하게 하고; 추가 입력 프레임과 상기 제 4 참조 프레임 사이의 모션을 추정하게 하며; 적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 4 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 추가 워핑된 프레임을 생성하게 하고; 그리고 상기 추가 워핑된 프레임 및 추가 예측된 잔차에 기초하여, 상기 추가 입력 프레임을 나타내는 재구성된 프레임을 생성하게 하는명령들을 더 포함하고,상기 추가 입력 프레임을 나타내는 상기 재구성된 프레임은 단방향 예측된 프레임을 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>37. 제 31 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 상기 제 1 참조 프레임으로부터 상기 제 3 참조 프레임으로의 픽셀 모션을 나타내는 모션 정보의 제 1 세트를 결정하게 하고; 상기 제 2 참조 프레임으로부터 상기 제 3 참조 프레임으로의 픽셀 모션을 나타내는 모션 정보의 제 2 세트를 결정하게 하며;적어도 부분적으로, 상기 모션 정보의 제 1 세트를 사용하여 상기 제 1 참조 프레임에 대해 워핑 함수를 수행하는 것에 의해, 제 1 워핑 정보를 생성하게 하고; 적어도 부분적으로, 상기 모션 정보의 제 2 세트를 사용하여 상기 제 2 참조 프레임에 대해 상기 워핑 함수를 수행하는 것에 의해, 제 2 워핑 정보를 생성하게 하고; 그리고 상기 제 1 워핑 정보 및 상기 제 2 워핑 정보에 기초하여 상기 제 3 참조 프레임을 생성하게 하는명령들을 더 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>38. 비디오 데이터를 프로세싱하기 위한 시스템으로서,상기 시스템은:메모리; 및상기 메모리에 커플링된 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는: 보간된 참조 프레임 및 입력 프레임과 상기 보간된 참조 프레임 사이의 모션을 나타내는 모션 정보를 획득하고; 상기 모션 정보에 기초하여, 상기 입력 프레임과 상기 보간된 참조 프레임 사이의 모션을 추정하며; 적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 보간된 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 워핑된 프레임을 생성하고; 그리고 상기 워핑된 프레임 및 예측된 잔차에 기초하여, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하도록구성되고, 상기 재구성된 프레임은 양방향 예측된 프레임을 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>39. 제 38 항에 있어서,상기 적어도 하나의 프로세서는: 제 1 참조 프레임 및 제 2 참조 프레임을 획득하고; 그리고 적어도 부분적으로, 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 보간을 수행하는 것에 의해, 상기 보간된 참조 프레임을 생성하도록구성되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>40. 제 38 항에 있어서,상기 적어도 하나의 프로세서는: 비트스트림으로부터 잔차를 획득하고; 그리고 획득된 상기 잔차에 기초하여 예측된 잔차를 생성하도록구성되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>41. 제 40 항에 있어서,상기 모션 정보는 상기 비트스트림으로부터 획득되는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>42. 제 38 항에 있어서,상기 적어도 하나의 프로세서는: 제 4 참조 프레임을 획득하고; 추가 입력 프레임과 상기 제 4 참조 프레임 사이의 모션을 추정하며; 적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 4 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 추가 워핑된 프레임을 생성하고; 그리고 상기 추가 워핑된 프레임 및 추가 예측된 잔차에 기초하여, 상기 추가 입력 프레임을 나타내는 재구성된 프레임을 생성하도록구성되고, 상기 추가 입력 프레임을 나타내는 상기 재구성된 프레임은 단방향 예측된 프레임을 포함하는, 비디오 데이터를 프로세싱하기 위한 시스템.</claim></claimInfo><claimInfo><claim>43. 비디오 데이터를 프로세싱하기 위한 방법으로서,상기 방법은:보간된 참조 프레임 및 입력 프레임과 상기 보간된 참조 프레임 사이의 모션을 나타내는 모션 정보를 획득하는 단계;상기 모션 정보에 기초하여, 상기 입력 프레임과 상기 보간된 참조 프레임 사이의 모션을 추정하는 단계;적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 보간된 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 워핑된 프레임을 생성하는 단계; 및상기 워핑된 프레임 및 예측된 잔차에 기초하여, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하는 단계를 포함하고,상기 재구성된 프레임은 양방향 예측된 프레임을 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>44. 제 43 항에 있어서,제 1 참조 프레임 및 제 2 참조 프레임을 획득하는 단계; 및적어도 부분적으로, 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 보간을 수행하는 것에 의해, 상기 보간된 참조 프레임을 생성하는 단계를 더 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>45. 제 43 항에 있어서,비트스트림으로부터 잔차를 획득하는 단계; 및획득된 상기 잔차에 기초하여 예측된 잔차를 생성하는 단계를 더 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>46. 제 45 항에 있어서,상기 모션 정보는 상기 비트스트림으로부터 획득되는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>47. 제 43 항에 있어서,제 4 참조 프레임을 획득하는 단계;추가 입력 프레임과 상기 제 4 참조 프레임 사이의 모션을 추정하는 단계;적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 4 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 추가 워핑된 프레임을 생성하는 단계; 및상기 추가 워핑된 프레임 및 추가 예측된 잔차에 기초하여, 상기 추가 입력 프레임을 나타내는 재구성된 프레임을 생성하는 단계를 더 포함하고,상기 추가 입력 프레임을 나타내는 상기 재구성된 프레임은 단방향 예측된 프레임을 포함하는, 비디오 데이터를 프로세싱하기 위한 방법.</claim></claimInfo><claimInfo><claim>48. 명령들을 저장한 비일시적 컴퓨터 판독가능 저장 매체로서,상기 명령들은, 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금:보간된 참조 프레임 및 입력 프레임과 상기 보간된 참조 프레임 사이의 모션을 나타내는 모션 정보를 획득하게 하고;상기 모션 정보에 기초하여, 상기 입력 프레임과 상기 보간된 참조 프레임 사이의 모션을 추정하게 하며;적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 보간된 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 워핑된 프레임을 생성하게 하고; 그리고상기 워핑된 프레임 및 예측된 잔차에 기초하여, 상기 입력 프레임을 나타내는 재구성된 프레임을 생성하게 하며,상기 재구성된 프레임은 양방향 예측된 프레임을 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>49. 제 48 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 제 1 참조 프레임 및 제 2 참조 프레임을 획득하게 하고; 그리고 적어도 부분적으로, 상기 제 1 참조 프레임과 상기 제 2 참조 프레임 사이의 보간을 수행하는 것에 의해, 상기 보간된 참조 프레임을 생성하게 하는명령들을 더 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>50. 제 48 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 비트스트림으로부터 잔차를 획득하게 하고; 그리고 획득된 상기 잔차에 기초하여 예측된 잔차를 생성하게 하는명령들을 더 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>51. 제 50 항에 있어서,상기 모션 정보는 상기 비트스트림으로부터 획득되는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo><claimInfo><claim>52. 제 48 항에 있어서,상기 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 제 4 참조 프레임을 획득하게 하고; 추가 입력 프레임과 상기 제 4 참조 프레임 사이의 모션을 추정하게 하며; 적어도 부분적으로, 추정된 상기 모션에 기초하여 상기 제 4 참조 프레임의 하나 이상의 픽셀들을 워핑하는 것에 의해, 추가 워핑된 프레임을 생성하게 하고; 그리고 상기 추가 워핑된 프레임 및 추가 예측된 잔차에 기초하여, 상기 추가 입력 프레임을 나타내는 재구성된 프레임을 생성하게 하는명령들을 더 포함하고,  상기 추가 입력 프레임을 나타내는 상기 재구성된 프레임은 단방향 예측된 프레임을 포함하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 *****-**** 캘리포니아주 샌 디에고 모어하우스 드라이브 ****</address><code>519980804600</code><country>미국</country><engName>Qualcomm Incorporated</engName><name>퀄컴 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>POURREZA, REZA</engName><name>푸레자 레자</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>COHEN, TACO SEBASTIAAN</engName><name>코헌 타코 세바스티안</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 강남구 강남대로 **길 **(역삼동, 케이피빌딩)</address><code>920011000013</code><country>대한민국</country><engName>Koreana Patent Firm</engName><name>특허법인코리아나</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.03.11</priorityApplicationDate><priorityApplicationNumber>17/198,813</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.09.04</receiptDate><receiptNumber>1-1-2023-0977083-14</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.10.12</receiptDate><receiptNumber>1-5-2023-0161188-20</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2025.01.10</receiptDate><receiptNumber>1-1-2025-0039420-21</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사유예신청]결정 보류신청서·심사유예신청서</documentName><receiptDate>2025.01.10</receiptDate><receiptNumber>1-1-2025-0039421-77</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237030157.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93cb78c6fb991dee5a62a3f13726f65f1fc28a89e605546818bda1ef50eeac6d5b1184815769a464353c91ded8eaf0f91bc5ec71efb5e9975d</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfad43c600a131cc0c1feecf05ae1e1b1f4aa8b429227e32ee085e7deeb5779d45321fdf06a2507d7011259942e86d36c98dbe8779d0228575</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>