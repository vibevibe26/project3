<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:40:04.404</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.09.21</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0119212</applicationNumber><claimCount>10</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>단일 열화상 기반 단안 깊이 추정 방법 및 장치</inventionTitle><inventionTitleEng>Single infrared image-based monocular depth  estimation method and apparatus</inventionTitleEng><openDate>2024.03.28</openDate><openNumber>10-2024-0040337</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2022.09.21</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/55</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/90</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/30</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 발명은 단일 열화상 기반 단안 깊이 추정 방법 및 장치를 개시한다. 본 발명에 따르면, 프로세서 및 상기 프로세서에 연결된 메모리를 포함하되, 상기 메모리는, 스테레오 칼라영상 중 하나의 입력 칼라영상을 딥러닝 기반 제1 깊이 네트워크에 입력하여 제1 깊이 맵을 생성하고, 상기 입력 칼라영상에 대응되는 열화상을 딥러닝 기반 제2 깊이 네트워크에 입력하여 제2 깊이 맵을 생성하고, 상기 제1 깊이 맵을 이용하여 추정된 추정 칼라영상과 상기 입력 칼라영상을 비교하여 상기 제1 깊이 네트워크의 학습을 반복하고, 상기 제1 깊이 네트워크의 학습의 반복에 따른 제1 깊이 맵과 상기 제2 깊이 맵을 비교하여 상기 제2 깊이 네트워크의 학습을 반복하고, 학습된 제2 깊이 네트워크를 이용하여 새로 입력되는 열화상으로부터 제2 깊이 맵을 생성하도록, 상기 프로세서에 의해 실행되는 프로그램 명령어들을 저장한 단일 열화상 기반 단안 깊이 추정 장치가 제공된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 단일 열화상 기반 단안 깊이 추정 장치로서, 프로세서; 및상기 프로세서에 연결된 메모리를 포함하되, 상기 메모리는, 스테레오 칼라영상 중 하나의 입력 칼라영상을 딥러닝 기반 제1 깊이 네트워크에 입력하여 제1 깊이 맵을 생성하고, 상기 입력 칼라영상에 대응되는 열화상을 딥러닝 기반 제2 깊이 네트워크에 입력하여 제2 깊이 맵을 생성하고, 상기 제1 깊이 맵을 이용하여 추정된 추정 칼라영상과 상기 입력 칼라영상을 비교하여 상기 제1 깊이 네트워크의 학습을 반복하고, 상기 제1 깊이 네트워크의 학습의 반복에 따른 제1 깊이 맵과 상기 제2 깊이 맵을 비교하여 상기 제2 깊이 네트워크의 학습을 반복하고, 학습된 제2 깊이 네트워크를 이용하여 새로 입력되는 열화상으로부터 제2 깊이 맵을 생성하도록, 상기 프로세서에 의해 실행되는 프로그램 명령어들을 저장한 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 프로그램 명령어들은, 상기 제1 깊이 맵과 상기 제2 깊이 맵의 자기지도 손실(Self-Guided Loss)을 산출하여 상기 제2 깊이 네트워크의 학습을 반복하는 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>3. 제2항에 있어서, 상기 자기지도 손실은 상기 제1 깊이 맵과 상기 제2 깊이 맵의 SSIM(structural simulation index measure)과 L1 거리의 조합에 따른 제1 손실, 상기 제1 깊이 맵과 상기 제2 깊이 맵을 VGG 네트워크에 입력하여 획득한 특징을 이용하여 산출되는 제2 손실 및 상기 제1 깊이 맵과 상기 제2 깊이 맵에서 각각 추출되는 전역 기술자(global descriptor) 및 지역 기술자(local descriptor)를 이용하여 산출되는 제3 손실을 포함하는 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서, 상기 제3 손실은 Patch-NetVLAD 손실로 정의되는 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>5. 제3항에 있어서, 상기 제1 손실 및 상기 제3 손실 각각에는 미리 설정된 가중치가 적용되는 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>6. 제1항에 있어서, 상기 제1 깊이 맵은 상기 제1 깊이 네트워크의 학습이 반복됨에 따라 업데이트되는 의사 레이블로 정의되는 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>7. 제1항에 있어서, 상기 프로그램 명령어들은, 상기 추정 칼라영상과 상기 입력 칼라영상 사이의 외형 정합 손실(Appearance Matching Loss) 및 이미지 정합 손실(Image Matching Loss)을 산출하여 상기 제1 깊이 네트워크의 학습을 반복하는 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서, 상기 이미지 정합 손실은, 상기 추정 칼라영상과 상기 입력 칼라영상을 VGG 네트워크에 입력하여 획득한 특징을 이용하여 산출되는 지각 손실 및 상기 추정 칼라영상과 상기 입력 칼라영상에서 각각 추출되는 전역 기술자(global descriptor) 및 지역 기술자(local descriptor)를 이용하여 산출되는 Patch-NetVLAD 손실을 포함하는 단일 열화상 기반 단안 깊이 추정 장치.</claim></claimInfo><claimInfo><claim>9. 프로세서 및 메모리를 포함하는 장치에서 단일 열화상 기반으로 단안 깊이를 추정하는 방법으로서,  스테레오 칼라영상 중 하나의 입력 칼라영상을 딥러닝 기반 제1 깊이 네트워크에 입력하여 제1 깊이 맵을 생성하는 단계; 상기 입력 칼라영상에 대응되는 열화상을 딥러닝 기반 제2 깊이 네트워크에 입력하여 제2 깊이 맵을 생성하는 단계; 상기 제1 깊이 맵을 이용하여 추정된 추정 칼라영상과 상기 입력 칼라영상을 비교하여 상기 제1 깊이 네트워크의 학습을 반복하는 단계; 상기 제1 깊이 네트워크의 학습의 반복에 따른 제1 깊이 맵과 상기 제2 깊이 맵을 비교하여 상기 제2 깊이 네트워크의 학습을 반복하는 단계; 및학습된 제2 깊이 네트워크를 이용하여 새로 입력되는 열화상으로부터 제2 깊이 맵을 생성하는 단일 열화상 기반 단안 깊이 추정 방법.</claim></claimInfo><claimInfo><claim>10. 제9항에 따른 방법을 수행하는 컴퓨터 판독 가능한 기록매체에 저장된 컴퓨터 프로그램.  </claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>서울특별시 광진구 능동로 *** (군...</address><code>220050114702</code><country>대한민국</country><engName>INDUSTRY ACADEMY COOPERATION FOUNDATION OF SEJONG UNIVERSITY</engName><name>세종대학교산학협력단</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>서울특별시 광진구 능동로 ***, 세종대학교 대양 AI센터 ***호(군...</address><code> </code><country> </country><engName>CHOI, Yu Kyung</engName><name>최유경</name></inventorInfo><inventorInfo><address>서울특별시 광진구 광나루로**길 **-*, ***호(군...</address><code> </code><country> </country><engName>HAN, Dae Chan</engName><name>한대찬</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 강남대로**길 ** 동림빌딩 *층(아이피즈국제특허법률사무소)</address><code>920050008309</code><country>대한민국</country><engName>SONG INHO</engName><name>송인호</name></agentInfo><agentInfo><address>서울특별시 강남구 강남대로**길 ** (역삼동) 동림빌딩 *층(아이피즈국제특허법률사무소)</address><code>920040002237</code><country>대한민국</country><engName>choi kwan rak</engName><name>최관락</name></agentInfo></agentInfoArray><priorityInfoArray/><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.09.21</receiptDate><receiptNumber>1-1-2022-0992594-95</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Amendment to Patent Application, etc.] Amendment</documentEngName><documentName>[출원서 등 보정]보정서</documentName><receiptDate>2022.10.07</receiptDate><receiptNumber>1-1-2022-1055614-77</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Request for Prior Art Search</documentEngName><documentName>선행기술조사의뢰서</documentName><receiptDate>2025.03.12</receiptDate><receiptNumber>9-1-9999-9999999-89</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Report of Prior Art Search</documentEngName><documentName>선행기술조사보고서</documentName><receiptDate>2025.04.25</receiptDate><receiptNumber>9-6-2025-0175674-82</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for refusal</documentEngName><documentName>의견제출통지서</documentName><receiptDate>2025.09.25</receiptDate><receiptNumber>9-5-2025-0938761-18</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220119212.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93d60e70671cbb16ff71811fe6cbfd8af68df259952b5bd35f7db66ffb2bb04de895f475e2d36a3e5df48366e2303aeeb2ee2fc0ceb4084302</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfb9adb1260a72bc1aad2fc1ce3b8133895d292c9abf803f0a0407d82d17f49c8f963d4b1b0cb4116c00039d1f3ef51ce689dc15e88628561b</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>