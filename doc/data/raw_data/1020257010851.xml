<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:00:50.050</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.09.27</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2025-7010851</applicationNumber><claimCount>15</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>로봇, 학습 장치, 제어 방법 및 프로그램</inventionTitle><inventionTitleEng>ROBOT, LEARNING DEVICE, CONTROL METHOD, AND PROGRAM</inventionTitleEng><openDate>2025.05.07</openDate><openNumber>10-2025-0060902</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2025.04.03</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2025.04.03</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 13/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 9/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 13/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>B25J 19/02</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>A63H 11/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 40/10</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 적절한 타이밍에 사용자와의 접촉을 유발한다. 로봇은, 사용자를 찍은 촬영 화상과 상기 사용자의 생체 정보 중 적어도 한쪽을 취득하는 취득부와, 상기 촬영 화상 및 상기 생체 정보 중 적어도 한쪽에 근거하여, 상기 사용자의 상태에 따라 상기 사용자와의 접촉을 유발하는 소정의 행동의 실행을 지령하는 행동 제어부를 갖는다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2024.04.04</internationOpenDate><internationOpenNumber>WO2024071164</internationOpenNumber><internationalApplicationDate>2023.09.27</internationalApplicationDate><internationalApplicationNumber>PCT/JP2023/035067</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 사용자를 찍은 촬영 화상과 상기 사용자의 생체 정보 중 적어도 한쪽을 취득하는 취득부와,상기 촬영 화상 및 상기 생체 정보 중 적어도 한쪽에 근거하여, 상기 사용자의 상태에 따라 상기 사용자와의 접촉을 유발하는 소정의 행동의 실행을 지령하는 행동 제어부를 갖는 로봇.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서,상기 사용자의 상태에 적합한 상기 소정의 행동을 추정하는 추정부를 더 갖고,상기 추정부는,상기 촬영 화상 및 상기 생체 정보 중 적어도 한쪽에 근거하여 상기 사용자의 상태를 관측하는 상태 관측부와,상기 소정의 행동의 가치에 근거하여, 상기 사용자의 상태에 적합한 상기 소정의 행동을 결정하는 행동 결정부를 갖는 로봇.</claim></claimInfo><claimInfo><claim>3. 제1항 또는 제2항에 있어서,상기 촬영 화상은, 상기 사용자의 얼굴 화상 및 전신 화상 중 적어도 한쪽을 포함하고,상기 생체 정보는, 상기 사용자의 심박, 호흡, 혈압 및 체온 중 적어도 하나에 관한 정보를 포함하는 로봇.</claim></claimInfo><claimInfo><claim>4. 제1항 또는 제2항에 있어서,상기 사용자의 상태는, 상기 사용자의 얼굴 화상과, 상기 사용자의 심박, 호흡, 혈압 및 체온 중 적어도 하나에 관한 정보의 적어도 한쪽에 근거하여 분류되는 상기 사용자의 감정의 상태를 포함하는 로봇.</claim></claimInfo><claimInfo><claim>5. 제1항 또는 제2항에 있어서,상기 사용자의 상태는, 상기 사용자의 전신 화상으로부터 추정한 골격의 동작에 근거하여 분류되는 상기 사용자의 행동의 상태를 포함하는 로봇.</claim></claimInfo><claimInfo><claim>6. 제1항 또는 제2항에 있어서,상기 사용자의 상태는, 상기 촬영 화상 및 상기 생체 정보 중 적어도 한쪽으로부터 추정한 상기 사용자의 감정 및 행동의 조합에 의해 분류되는 소정의 상태인 로봇.</claim></claimInfo><claimInfo><claim>7. 제2항에 있어서,상기 추정부는, 입력을 상기 사용자의 상태로 하고, 출력을 상기 로봇의 상기 행동의 가치로 하는 학습 모델을 기계 학습에 의해 생성하는 학습부를 갖는 로봇.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서,상기 추정부는, 상기 로봇의 상기 행동의 결과로서, 상기 사용자와의 접촉의 성립 결과에 관한 정보를 취득하는 결과 취득부를 더 갖고,상기 학습부는, 상기 사용자와의 접촉의 성립 결과에 근거하여, 상기 학습 모델을 갱신하는 로봇.</claim></claimInfo><claimInfo><claim>9. 제7항에 있어서,상기 학습 모델은 행동 가치 테이블 또는 뉴럴 네트워크인 로봇.</claim></claimInfo><claimInfo><claim>10. 제8항에 있어서,상기 사용자와의 접촉의 성립 결과에 관한 정보는, 상기 사용자의 접근 유무와, 상기 사용자의 감정 레벨과, 상기 사용자와 접촉한 시간 길이를 포함하고,상기 학습부는,상기 사용자와의 접촉의 성립 결과에 근거하여, 상기 로봇의 상기 행동에 대한 보수를 취득하고,상기 보수에 근거하여 상기 사용자의 상태에 대한 상기 행동의 가치를 갱신하는 로봇.</claim></claimInfo><claimInfo><claim>11. 로봇에 통신 가능하게 접속하는 학습 장치로서,사용자를 찍은 촬영 화상과 상기 사용자의 생체 정보 중 적어도 한쪽에 근거하여, 상기 사용자의 상태를 관측하는 상태 관측부와,입력을 상기 사용자의 상태로 하고, 출력을 상기 로봇의 행동의 가치로 하는 학습 모델을 기계 학습에 의해 생성하는 학습부를 갖는 학습 장치.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서,상기 행동의 가치에 근거하여, 상기 사용자의 상태에 적합한 상기 로봇의 상기 행동을 결정하는 행동 결정부와,상기 행동의 실행의 지령을 상기 로봇에 송신하는 통신 제어부를 더 갖는 학습 장치.</claim></claimInfo><claimInfo><claim>13. 제11항 또는 제12항에 있어서,상기 로봇의 상기 행동의 결과로서, 상기 사용자와의 접촉의 성립 결과에 관한 정보를 취득하는 결과 취득부를 더 갖고,상기 학습부는, 상기 접촉의 성립 결과에 근거하여 상기 학습 모델을 갱신하는 학습 장치.</claim></claimInfo><claimInfo><claim>14. 로봇의 제어 방법으로서, 상기 로봇이,사용자를 찍은 촬영 화상과 상기 사용자의 생체 정보 중 적어도 한쪽을 취득하는 단계와,상기 촬영 화상 및 상기 생체 정보 중 적어도 한쪽에 근거하여, 상기 사용자의 상태에 따라 상기 사용자와의 접촉을 유발하는 소정의 행동을 실행하는 단계를 실행하는 제어 방법.</claim></claimInfo><claimInfo><claim>15. 로봇을 제어하는 컴퓨터에,사용자를 찍은 촬영 화상과 상기 사용자의 생체 정보 중 적어도 한쪽을 취득하는 단계와,상기 촬영 화상 및 상기 생체 정보 중 적어도 한쪽에 근거하여, 상기 사용자의 상태에 따라 상기 사용자와의 접촉을 유발하는 소정의 행동의 실행을 지령하는 단계를 실행시키는 프로그램.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>일본국 오사카후 이바라키시 시모호츠미 *-*-*</address><code>519980608477</code><country>일본</country><engName>Nitto Denko Corporation</engName><name>닛토덴코 가부시키가이샤</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>일본 오사카후 이바라키시 시모호즈...</address><code> </code><country>일본</country><engName>KIYOSHIMA, Keita</engName><name>기요시마 게이타</name></inventorInfo><inventorInfo><address>일본 오사카후 이바라키시 시모호즈...</address><code> </code><country>일본</country><engName>FUKUSHIMA, Rihito</engName><name>후쿠시마 리히토</name></inventorInfo><inventorInfo><address>일본 오사카후 이바라키시 시모호즈...</address><code> </code><country>일본</country><engName>YAMAUCHI, Kengo</engName><name>야마우치 겐고</name></inventorInfo><inventorInfo><address>일본 오사카후 이바라키시 시모호즈...</address><code> </code><country>일본</country><engName>SHIMIZU, Yusuke</engName><name>시미즈 유스케</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울특별시 서초구 마방로 ** (양재동, 동원F&amp;B빌딩)</address><code>920101000812</code><country>대한민국</country><engName>FirstLaw P.C.</engName><name>제일특허법인(유)</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>일본</priorityApplicationCountry><priorityApplicationDate>2022.09.29</priorityApplicationDate><priorityApplicationNumber>JP-P-2022-156758</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2025.04.03</receiptDate><receiptNumber>1-1-2025-0374347-10</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2025.04.07</receiptDate><receiptNumber>1-5-2025-0058035-39</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020257010851.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93d25b0acb2956c4dd25073866b1f94262ed735f37d0ded98e8ef80a0fcf3a9208c190976ba6654a7e5aba22ded8462b801474f7d5aa15882c</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf965da9994007687c07522a884139c774c498b507acbebcf283431c5a1862cd77e5320b327dda6216bf7d788b73a75521054c07b76207863f</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>