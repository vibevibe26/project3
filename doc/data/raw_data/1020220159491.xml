<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:06:19.619</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2022.11.24</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2022-0159491</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>카메라 캘리브레이션(camera calibration)을 수행하는 전자 장치 및 그 동작 방법</inventionTitle><inventionTitleEng>AN ELECTRONIC DEVICE FOR PERFORMING CAMERA  CALIBRATION AND A METHOD FOR OPERATING THE SAME</inventionTitleEng><openDate>2024.03.15</openDate><openNumber>10-2024-0035292</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/80</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/73</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/44</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/62</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/33</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 복수의 카메라 간의 캘리브레이션을 수행하는 전자 장치 및 그 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 전자 장치는 제1 카메라로부터 획득한 제1 이미지로부터 사용자의 관절들의 2차원 위치 좌표인 제1 관절 특징점을 추출하고, 제2 카메라로부터 획득된 제2 이미지로부터 관절들의 2차원 위치 좌표인 제2 관절 특징점을 추출하고, 추출된 제1 관절 특징점을 3차원 위치 좌표로 리프팅(lifting)하여 관절들의 3차원 관절 특징점을 획득하고, 3차원 관절 특징점을 제2 관절 특징점의 2차원 위치 좌표값으로 투영(projection)하기 위한 프로젝션 관계를 획득하고, 프로젝션 관계에 기초하여 제1 카메라와 제2 카메라 간의 위치 관계를 예측함으로써 카메라 캘리브레이션을 수행할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 카메라 캘리브레이션(camera calibration)을 수행하는 전자 장치(100)에 있어서,통신 인터페이스(110); 적어도 하나의 명령어들(instructions)를 저장하는 메모리(130); 및상기 적어도 하나의 명령어들을 실행하는 적어도 하나의 프로세서(120); 를 포함하고, 상기 적어도 하나의 프로세서(120)는, 상기 통신 인터페이스(110)를 통해 제1 카메라(210)로부터 사용자를 촬영한 제1 이미지를 획득하고, 제2 카메라(220)로부터 상기 사용자를 촬영한 제2 이미지를 획득하고,상기 제1 이미지로부터 상기 사용자의 관절들의 2차원 위치 좌표인 제1 관절 특징점을 추출하고, 상기 제2 이미지로부터 상기 관절들의 2차원 위치 좌표인 제2 관절 특징점을 추출하고,상기 추출된 제1 관절 특징점을 3차원 위치 좌표로 리프팅(lifting)하여 상기 관절들의 3차원 관절 특징점을 획득하고,상기 3차원 관절 특징점을 상기 제2 관절 특징점의 2차원 위치 좌표값으로 투영(projection)하기 위한 프로젝션 관계를 획득하고, 상기 획득된 프로젝션 관계에 기초하여 상기 제1 카메라(210)와 상기 제2 카메라(220) 간의 위치 관계를 예측함으로써 카메라 캘리브레이션을 수행하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 적어도 하나의 프로세서(120)는, 상기 3차원 관절 특징점이 상기 제2 관절 특징점의 2차원 위치 좌표값과 매칭되도록 상기 3차원 관절 특징점을 투영하기 위한 회전 방향 및 이동 거리값에 관한 정보를 획득하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>3. 제1 항 및 제2 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(120)는, 상기 3차원 관절 특징점에 포함되는 3차원 위치 좌표값들 중 Z축 방향에 따른 위치 좌표값들의 분포에 기초하여, 상기 3차원 관절 특징점으로 구성된 사용자의 자세(pose)가 카메라 캘리브레이션에 적용하기에 적합한지 여부를 판단하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>4. 제3 항에 있어서,디스플레이부;를 더 포함하고, 상기 적어도 하나의 프로세서(120)는, 상기 사용자의 자세가 카메라 캘리브레이션에 적용하기에 적절하지 않다고 판단된 경우 상기 사용자에게 기 설정된 자세를 취할 것을 요구하는 가이드 정보를 디스플레이하도록 상기 디스플레이부를 제어하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>5. 제3 항에 있어서, 상기 적어도 하나의 프로세서(120)는, 상기 제1 카메라(210)로부터 일정 시간 동안 촬영된 복수의 제1 이미지 프레임을 획득하고, 상기 제2 카메라(220)로부터 일정 시간 동안 촬영된 복수의 제2 이미지 프레임을 획득하고, 상기 복수의 제1 이미지 프레임 각각으로부터 추출된 복수의 제1 관절 특징점을 리프팅하여 복수의 3차원 관절 특징점을 획득하고, 상기 복수의 제1 이미지 프레임 중 상기 복수의 3차원 관절 특징점의 3차원 위치 좌표값들 중 Z축 방향에 따른 위치 좌표값들의 분포 정도가 최대인 이미지 프레임을 식별하고, 상기 복수의 제2 이미지 프레임 중 상기 식별된 이미지 프레임에 대응되는 상기 제2 이미지로부터 상기 제2 관절 특징점을 추출하고, 상기 식별된 이미지 프레임으로부터 획득된 3차원 관절 좌표값과 상기 제2 이미지로부터 추출된 제2 관절 특징점 간의 프로젝션 관계에 기초하여 카메라 캘리브레이션을 수행하는, 전자 장치(100).  </claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(120)는, 상기 제1 관절 특징점, 상기 제2 관절 특징점, 및 상기 제1 카메라(210)와 상기 제2 카메라(220) 간의 위치 관계에 기초하여 상기 사용자의 관절들의 3차원 위치 좌표값을 획득하고, 상기 획득된 관절들의 3차원 위치 좌표값에 기초하여 상기 사용자의 3차원 자세(pose)를 추정하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>7. 제6 항에 있어서, 상기 적어도 하나의 프로세서(120)는, 상기 제1 카메라(210)와 상기 제2 카메라(220) 간의 캘리브레이션 정보에 기초하여 상기 3차원 위치 좌표값을 2차원 위치 좌표값으로 재투영(re-projection)하여 제1 위치 좌표값 및 제2 위치 좌표값을 획득하고, 재투영 결과 획득된 상기 제1 위치 좌표값과 상기 제1 관절 특징점 간의 차이값 및 상기 제2 위치 좌표값과 상기 제2 관절 특징점 간의 차이값을 각각 산출하고,산출된 차이값들을 기 설정된 임계치와 비교한 결과에 따라 카메라 캘리브레이션의 정확도를 판단하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>8. 제6 항에 있어서, 상기 적어도 하나의 프로세서(120)는, 상기 3차원 자세로부터 관절들 사이의 뼈 길이를 측정하고, 상기 측정된 뼈 길이를 일반적인 사람의 뼈 길이와 비교하여 차이값을 산출하고,상기 산출된 차이값을 기 설정된 임계치와 비교한 결과에 기초하여 카메라 캘리브레이션의 재수행 여부를 결정하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>9. 제1 항 내지 제8 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(120)는, 상기 제1 이미지로부터 복수의 사용자의 관절들의 2차원 위치 좌표인 복수의 제1 관절 특징점을 추출하고, 상기 제2 이미지로부터 상기 복수의 사용자의 관절들의 2차원 위치 좌표인 복수의 제2 관절 특징점을 추출하고, 상기 복수의 제1 관절 특징점 및 상기 복수의 제2 관절 특징점을 3차원 위치 좌표값으로 리프팅하여, 복수의 제1 3차원 관절 특징점 및 복수의 제2 3차원 관절 특징점을 획득하며,상기 획득된 복수의 제1 3차원 관절 특징점으로 구성된 제1 3차원 자세와 상기 복수의 제2 3차원 관절 특징점으로 구성된 제2 3차원 자세를 매칭함으로써, 상기 제1 이미지 및 상기 제2 이미지에 포함된 상기 복수의 사용자를 구별하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>10. 제9 항에 있어서, 상기 적어도 하나의 프로세서(120)는,  상기 복수의 사용자의 구별 결과에 기초하여, 상기 복수의 제1 3차원 관절 특징점을 상기 복수의 제2 관절 특징점으로 각각 투영하기 위한 프로젝션 관계를 획득하는, 전자 장치(100). </claim></claimInfo><claimInfo><claim>11. 카메라 캘리브레이션(camera calibration)을 수행하는 방법에 있어서, 제1 카메라(210)로부터 사용자를 촬영한 제1 이미지를 획득하고, 제2 카메라(220)로부터 상기 사용자를 촬영한 제2 이미지를 획득하는 단계(S410); 상기 제1 이미지로부터 상기 사용자의 관절들의 2차원 위치 좌표인 제1 관절 특징점을 추출하고, 상기 제2 이미지로부터 상기 관절들의 2차원 위치 좌표인 제2 관절 특징점을 추출하는 단계(S420);상기 추출된 제1 관절 특징점을 3차원 위치 좌표로 리프팅(lifting)하여 상기 관절들의 3차원 관절 특징점을 획득하는 단계(S430); 상기 3차원 관절 특징점을 상기 제2 관절 특징점의 2차원 위치 좌표값으로 투영(projection)하기 위한 프로젝션 관계를 획득하는 단계(S440); 및상기 획득된 프로젝션 관계에 기초하여, 상기 제1 카메라(210)와 상기 제2 카메라(220) 간의 위치 관계를 예측함으로써 카메라 캘리브레이션을 수행하는 단계(S450);를 포함하는, 방법.  </claim></claimInfo><claimInfo><claim>12. 제11 항에 있어서,상기 프로젝션 관계를 획득하는 단계(S440)는,상기 3차원 관절 특징점이 상기 제2 관절 특징점의 2차원 위치 좌표값과 매칭되도록 상기 3차원 관절 특징점을 투영하기 위한 회전 방향 및 이동 거리값에 관한 정보를 획득하는 단계를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>13. 제11 항 및 제12 항 중 어느 하나의 항에 있어서,상기 3차원 관절 특징점에 포함되는 3차원 위치 좌표값들 중 Z축 방향에 따른 위치 좌표값들의 분포에 기초하여, 상기 3차원 관절 특징점으로 구성된 사용자의 자세(pose)가 카메라 캘리브레이션에 적용하기에 적합한지 여부를 판단하는 단계(S510);를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>14. 제13 항 중 어느 하나의 항에 있어서,상기 사용자의 자세가 카메라 캘리브레이션에 적용하기에 적절하지 않다고 판단된 경우 상기 사용자에게 기 설정된 자세를 취할 것을 요구하는 가이드 정보를 디스플레이하는 단계(S530);를 더 포함하는, 방법.   </claim></claimInfo><claimInfo><claim>15. 제11항 내지 제14 항 중 어느 하나의 항에 있어서, 상기 제1 관절 특징점, 상기 제2 관절 특징점, 및 상기 제1 카메라(210)와 상기 제2 카메라(220) 간의 위치 관계에 기초하여 상기 사용자의 관절들의 3차원 위치 좌표값을 획득하는 단계; 및 상기 획득된 관절들의 3차원 위치 좌표값에 기초하여 상기 사용자의 3차원 자세(pose)를 추정하는 단계;를 더 포함하는, 방법.  </claim></claimInfo><claimInfo><claim>16. 제15 항에 있어서, 상기 제1 카메라(210)와 상기 제2 카메라(220) 간의 캘리브레이션 정보에 기초하여 상기 3차원 위치 좌표값을 2차원 위치 좌표값으로 재투영(re-projection)하여 제1 위치 좌표값 및 제2 위치 좌표값을 획득하는 단계(S920); 재투영 결과 획득된 상기 제1 위치 좌표값과 상기 제1 관절 특징점 간의 차이값 및 상기 제2 위치 좌표값과 상기 제2 관절 특징점 간의 차이값을 각각 산출하는 단계(S930);산출된 차이값들을 기 설정된 임계치와 비교하는 단계(S940); 및비교 결과에 따라 카메라 캘리브레이션의 정확도를 판단하는 단계;를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>17. 제15 항에 있어서, 상기 3차원 자세로부터 관절들 사이의 뼈 길이를 측정하는 단계(S1120); 상기 측정된 뼈 길이를 일반적인 사람의 뼈 길이와 비교하여 차이값을 산출하는 단계(S1130); 및상기 산출된 차이값을 기 설정된 임계치와 비교한 결과에 기초하여 카메라 캘리브레이션의 재수행 여부를 결정하는 단계;를 더 포함하는, 방법. </claim></claimInfo><claimInfo><claim>18. 제11 항 내지 제17 항 중 어느 하나의 항에 있어서,상기 제1 관절 특징점 및 제2 관절 특징점을 추출하는 단계(S420)는, 상기 제1 이미지로부터 복수의 사용자의 관절들의 2차원 위치 좌표인 복수의 제1 관절 특징점을 추출하고, 상기 제2 이미지로부터 상기 복수의 사용자의 관절들의 2차원 위치 좌표인 복수의 제2 관절 특징점을 추출하는 단계(S1310)를 포함하고, 상기 3차원 관절 특징점을 획득하는 단계(S430)는, 상기 복수의 제1 관절 특징점 및 상기 복수의 제2 관절 특징점을 3차원 위치 좌표값으로 리프팅하여, 복수의 제1 3차원 관절 특징점 및 복수의 제2 3차원 관절 특징점을 획득하는 단계(S1320); 및 상기 획득된 복수의 제1 3차원 관절 특징점으로 구성된 제1 3차원 자세와 상기 복수의 제2 3차원 관절 특징점으로 구성된 제2 3차원 자세를 매칭함으로써, 상기 제1 이미지 및 상기 제2 이미지에 포함된 상기 복수의 사용자를 구별하는 단계(S1330);를 포함하는, 방법. </claim></claimInfo><claimInfo><claim>19. 제18 항에 있어서, 상기 프로젝션 관계를 획득하는 단계는, 상기 복수의 사용자의 구별 결과에 기초하여, 상기 복수의 제1 3차원 관절 특징점을 상기 복수의 제2 관절 특징점으로 각각 투영하기 위한 프로젝션 관계를 획득하는(S1340), 방법. </claim></claimInfo><claimInfo><claim>20. 제11 항 내지 제19 항 중 어느 하나의 항에 기재된 방법을 구현하기 위한 적어도 하나의 프로그램이 기록된 컴퓨터로 판독 가능한 기록 매체. </claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>LEE, Sang Hoon</engName><name>이상훈</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>LEE, Hong Pyo</engName><name>이홍표</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>PARK, Sung Gwan</engName><name>박성관</name></inventorInfo><inventorInfo><address>경기도 수원시 영통구...</address><code> </code><country> </country><engName>SEO, Chan Won</engName><name>서찬원</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>대한민국</priorityApplicationCountry><priorityApplicationDate>2022.09.08</priorityApplicationDate><priorityApplicationNumber>1020220114495</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2022.11.24</receiptDate><receiptNumber>1-1-2022-1259797-31</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020220159491.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9353750368cfa243cd384728bf8a13fbe67b6cac0528e2be4945a7974a42fc77a7692c451fa5336a2382f4b6e3463fc3fc7c60c60bfeb09c39</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cfe6c0a37e5419bd01e5f27e308f5bedd1081b29f547c958ee334a46bb141a68c52d33d02c3b5d17a9ad45418226c4bbd06d7d7cd1cb900797</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>