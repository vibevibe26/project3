<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 17:55:08.558</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2021.11.22</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-7007208</applicationNumber><claimCount>32</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>어시스턴트 디바이스(들)의 주변 감지에 기초한 어시스턴트 액션(들) 추론하기</inventionTitle><inventionTitleEng>INFERRING ASSISTANT ACTION (S) BASED ON AMBIENT SENSING BY ASSISTANT DEVICE (S)</inventionTitleEng><openDate>2023.04.07</openDate><openNumber>10-2023-0047434</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2023.02.28</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2023.02.28</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06N 20/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>H04L 51/02</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G10L 15/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G10L 15/18</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2018.01.01)</ipcDate><ipcNumber>G06F 3/16</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06F 3/0484</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2021.01.01)</ipcDate><ipcNumber>H04M 1/72454</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 구현예는 센서 데이터의 인스턴스에 기초하여 사용자의 상태 및/또는 사용자의 환경을 반영하는 주변 상태를 결정할 수 있다. 주변 상태는 주변 감지 기계 학습(ML) 모델을 사용하여 프로세싱되어 자동화된 어시스턴트에 의해 사용자를 대신하여 수행하도록 제안되는 제안 액션(들)을 생성할 수 있다. 일부 구현예에서, 제안된 액션(들)의 대응하는 표현은 사용자에게 제시하기 위해 제공될 수 있고, 제안된 액션(들)은 제안된 액션(들)의 사용자 선택에 응답하여 자동화된 어시스턴트에 의해 수행될 수 있다. 추가적 또는 대안적 구현예에서, 제안된 액션(들)은 자동화된 어시스턴트에 의해 자동으로 수행될 수 있다. 구현예는 자동화된 어시스턴트와의 인터렉션에 기초하여 주변 감지 ML 모델을 트레이닝하기 위한 트레이닝 인스턴스를 추가적으로 또는 대안적으로 생성할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2022.09.01</internationOpenDate><internationOpenNumber>WO2022182397</internationOpenNumber><internationalApplicationDate>2021.11.22</internationalApplicationDate><internationalApplicationNumber>PCT/US2021/060321</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 하나 이상의 프로세서에 의해 수행되는 방법으로서,센서 데이터의 인스턴스에 기초하여 주변 상태를 결정하는 단계, 상기 센서 데이터의 인스턴스는 사용자의 어시스턴트 디바이스의 하나 이상의 센서를 통해 검출되며, 상기 주변 상태는 상기 사용자의 상태 또는 상기 사용자의 환경을 반영하며;트레이닝된 주변 감지 기계 학습(ML) 모델을 사용하여, 상기 사용자를 대신하여, 상기 어시스턴트 디바이스 또는 상기 사용자의 추가적인 어시스턴트 디바이스에 의해 수행되도록 제안된 하나 이상의 제안된 액션을 생성하기 위해 상기 주변 상태를 프로세싱하는 단계;상기 하나 이상의 제안된 액션의 대응하는 표현이 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스를 통해 상기 사용자에게 제시하기 위해 제공되게 하는 단계; 및상기 하나 이상의 제안된 액션의 대응하는 표현에 대한 사용자 선택을 수신하는 것에 응답하여:상기 하나 이상의 제안된 액션으로 하여금 상기 사용자를 대신하여, 상기 어시스턴트 디바이스 또는 상기 사용자의 추가적인 어시스턴트 디바이스에 의해 수행되게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 청구항 1에 있어서, 상기 하나 이상의 제안된 액션 각각은 예측된 측정치와 연관되는, 방법.</claim></claimInfo><claimInfo><claim>3. 청구항 2에 있어서, 상기 하나 이상의 제안된 액션의 표현이 사용자에게 제시하기 위해 제공되게 하는 단계는 상기 하나 이상의 제안된 액션 각각과 연관된 예측된 측정치가 제1 임계 측정치를 만족한다는 결정 및 상기 하나 이상의 제안된 액션 각각과 연관된 예측된 측정치가 제2 임계 측정치를 만족하지 못한다는 결정에 대한 응답인, 방법.</claim></claimInfo><claimInfo><claim>4. 선행하는 청구항에 있어서, 상기 하나 이상의 제안된 액션의 대응하는 표현이 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스를 통해 상기 사용자에게 제시하기 위해 제공되게 하는 단계는:상기 하나 이상의 제안된 액션 각각에 대해, 대응하는 선택 가능한 엘리먼트가 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스의 디스플레이에서 시각적으로 렌더링되게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 청구항 4에 있어서, 상기 하나 이상의 제안된 액션의 대응하는 표현의 사용자 선택을 수신하는 것은:상기 대응하는 선택 가능한 엘리먼트 중 주어진 대응하는 선택 가능한 엘리먼트의 사용자 선택을 수신하는 것을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>6. 청구항 1 내지 3 중 어느 한 항에 있어서, 상기 하나 이상의 제안된 액션의 대응하는 표현이 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스를 통해 상기 사용자에게 제시하기 위해 제공되게 하는 단계는:상기 하나 이상의 제안된 액션의 표시가 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스의 하나 이상의 스피커에서 청각적으로 렌더링되게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 청구항 6에 있어서, 상기 하나 이상의 제안된 액션의 대응하는 표현의 사용자 선택을 수신하는 것은:상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스의 하나 이상의 마이크로폰을 통해 검출된 상기 사용자의 음성 발화를 통해 사용자 선택을 수신하는 것을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 임의의 선행하는 청구항에 있어서,상기 주변 상태의 표시가 상기 하나 이상의 액션의 표현과 함께 상기 사용자에게 제시하기 위해 제공되게 하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>9. 임의의 선행하는 청구항에 있어서, 상기 센서 데이터의 인스턴스에 기초하여 상기 주변 상태를 결정하는 단계는:상기 주변 상태를 결정하기 위해 상기 센서 데이터의 인스턴스를 프로세싱하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 임의의 선행하는 청구항에 있어서, 상기 센서 데이터의 인스턴스는 오디오 데이터, 모션 데이터 또는 페어링 데이터 중 하나 이상을 캡처하는, 방법.</claim></claimInfo><claimInfo><claim>11. 하나 이상의 프로세서에 의해 수행되는 방법으로서,센서 데이터의 인스턴스에 기초하여 주변 상태를 결정하는 단계, 상기 센서 데이터의 인스턴스는 사용자의 어시스턴트 디바이스의 하나 이상의 센서를 통해 검출되며, 상기 주변 상태는 상기 사용자의 상태 또는 상기 사용자의 환경을 반영하며;트레이닝된 주변 감지 기계 학습(ML) 모델을 사용하여, 상기 사용자를 대신하여, 상기 어시스턴트 디바이스 또는 상기 사용자의 추가적인 어시스턴트 디바이스에 의해 수행되도록 제안된 하나 이상의 제안된 액션을 생성하기 위해 상기 주변 상태를 프로세싱하는 단계; 및상기 하나 이상의 제안된 액션으로 하여금 상기 사용자를 대신하여, 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스에 의해 자동적으로 수행되게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 청구항 11에 있어서, 상기 하나 이상의 제안된 액션 각각은 예측된 측정치와 연관되는, 방법.</claim></claimInfo><claimInfo><claim>13. 청구항 12에 있어서, 상기 하나 이상의 제안된 액션으로 하여금 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스에 의해 자동적으로 수행되게 하는 단계는 상기 하나 이상의 제안된 액션 각각과 연관된 예측된 측정치가 제1 임계 측정치를 만족한다는 결정 및 상기 하나 이상의 제안된 액션 각각과 연관된 예측된 측정치가 제2 임계 측정치를 만족한다는 결정에 대한 응답인, 방법.</claim></claimInfo><claimInfo><claim>14. 하나 이상의 프로세서에 의해 수행되는 방법으로서,센서 데이터의 인스턴스가 주변 감지 이벤트에 대응한다고 결정하는 단계, 상기 센서 데이터의 인스턴스는 사용자의 어시스턴트 디바이스의 하나 이상의 센서를 통해 획득되며;상기 주변 감지 이벤트의 임계 지속시간 내에 수행된 시간적으로 대응하는 액션을 식별하는 단계, 상기 시간적으로 대응하는 액션은 상기 어시스턴트 디바이스 또는 상기 사용자의 추가적인 어시스턴트 디바이스를 통해 상기 사용자에 의해 수행되는 사용자-개시 액션이며; 및상기 시간적으로 대응하는 액션을 식별하는 것에 응답하여:상기 어시스턴트 디바이스에서, 상기 센서 데이터의 인스턴스와 상기 시간적으로 대응하는 액션에 기초하여, 주변 감지 기계 학습(ML) 모델 트레이닝에 활용될 트레이닝 인스턴스를 생성하는 단계; 및상기 주변 감지 ML 모델이 상기 트레이닝 인스턴스에 기초하여 트레이닝되게 하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>15. 청구항 14에 있어서, 상기 트레이닝 인스턴스는 트레이닝 인스턴스 입력 및 트레이닝 인스턴스 출력을 포함하고, 상기 트레이닝 인스턴스 입력은 센서 데이터의 인스턴스를 포함하고, 상기 트레이닝 인스턴스 출력은 시간적으로 대응하는 액션의 표시를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>16. 청구항 15에 있어서, 상기 센서 데이터의 인스턴스는 상기 어시스턴트 디바이스의 하나 이상의 마이크로폰에 의해 캡처된 오디오 데이터의 인스턴스를 포함하고, 상기 트레이닝 인스턴스 입력은 오디오 데이터의 인스턴스 프로세싱에 기초하여 추론된 사용자의 추론된 활동과 연관된 주변 상태를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>17. 청구항 15 또는 16에 있어서, 상기 센서 데이터의 인스턴스는 상기 어시스턴트 디바이스의 하나 이상의 마이크로폰에 의해 캡처된 오디오 데이터의 인스턴스를 포함하고, 상기 트레이닝 인스턴스 입력은 오디오 데이터의 인스턴스 프로세싱에 기초하여 추론된 사용자의 추론된 위치와 연관된 주변 상태를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>18. 청구항 15 내지 17 중 어느 한 항에 있어서, 상기 센서 데이터의 인스턴스는 어시스턴트 디바이스의 가속도계 또는 GPS 센서에 의해 캡처된 모션 데이터의 인스턴스를 포함하고, 상기 트레이닝 인스턴스 입력은 모션 데이터의 인스턴스 프로세싱에 기초하여 추론된 사용자의 추론된 활동과 연관된 주변 상태를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>19. 청구항 15 내지 18 중 어느 한 항에 있어서, 상기 센서 데이터의 인스턴스는 상기 어시스턴트 디바이스의 가속도계 또는 GPS 센서에 의해 캡처된 모션 데이터의 인스턴스를 포함하고, 상기 트레이닝 인스턴스 입력은 모션 데이터의 인스턴스 프로세싱에 기초하여 추론된 사용자의 추론된 위치를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>20. 청구항 15 내지 19 중 어느 한 항에 있어서, 상기 센서 데이터의 인스턴스는 상기 사용자의 클라이언트 디바이스와 페어링된 어시스턴트 디바이스에 기초하여 식별된 페어링 데이터의 인스턴스를 포함하고, 상기 트레이닝 인스턴스 입력은 상기 페어링 데이터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>21. 청구항 14 내지 20 중 어느 한 항에 있어서,상기 추가적인 어시스턴트 디바이스의 하나 이상의 추가 센서를 통해 추가 센서 데이터의 추가 인스턴스가 주변 감지 이벤트에 대응함을 감지하는 단계를 더 포함하고, 상기 트레이닝 인스턴스를 생성하는 것은 상기 추가 센서 데이터의 추가 인스턴스에 더 기초하는, 방법.</claim></claimInfo><claimInfo><claim>22. 청구항 14 내지 21 중 어느 한 항에 있어서, 상기 시간적으로 대응하는 액션을 식별하는 단계는: 상기 사용자로부터, 하나 이상의 어시스턴트 디바이스 또는 추가적인 어시스턴트 디바이스로 하여금 주변 감지 이벤트의 임계 지속시간 내에 사용자-개시 액션을 수행하게 하는 사용자 입력을 수신하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>23. 청구항 14 내지 22 중 어느 한 항에 있어서, 상기 임계 지속시간은 상기 주변 감지 이벤트 이전의 시간의 제1 부분을 포함하고, 상기 임계 지속시간은 상기 주변 감지 이벤트 이후의 시간의 제2 부분을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 청구항 14 내지 22 중 어느 한 항에 있어서, 상기 임계 지속시간은 상기 주변 감지 이벤트 이후인, 방법.</claim></claimInfo><claimInfo><claim>25. 제14항 내지 제22항 중 어느 한 항에 있어서, 상기 임계 지속시간은 상기 주변 감지 이벤트의 유형에 기초하여 결정되는, 방법.</claim></claimInfo><claimInfo><claim>26. 청구항 14 내지 25 중 어느 한 항에 있어서,상기 주변 감지 ML 모델이 상기 트레이닝 인스턴스 및 복수의 추가적인 트레이닝 인스턴스에 기초하여 트레이닝되게 한 후에:상기 어시스턴트 디바이스로 하여금 상기 센서 데이터의 추가 인스턴스 프로세싱에 기초하여, 상기 사용자를 대신하여, 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스에 의해 수행되도록 제안되는 하나 이상의 제안된 액션을 생성할 때 상기 주변 감지 ML 모델을 활용하게 하는 단계; 및상기 하나 이상의 제안된 액션의 표현이 상기 어시스턴트 디바이스 또는 상기 추가적인 어시스턴트 디바이스를 통해 상기 사용자에게 제시하기 위해 제공되게 하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>27. 청구항 26에 있어서,상기 하나 이상의 액션에 대한 사용자 선택 수신에 대한 응답으로:상기 트레이닝된 주변 감지 ML 모델을 업데이트하기 위한 긍정적인 피드백 신호로서 상기 사용자 선택을 활용하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>28. 청구항 26에 있어서,상기 하나 이상의 액션의 사용자 선택을 수신하지 않은 것에 응답하여:상기 트레이닝된 주변 감지 ML 모델을 업데이트하기 위한 부정적인 피드백 신호로서 상기 사용자 선택의 없음을 활용하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>29. 청구항 14 내지 28 중 어느 한 항에 있어서, 상기 트레이닝 인스턴스를 생성하는 단계는 상기 주변 감지 이벤트에 대응하는 센서 데이터의 인스턴스가 검출되는 시간 또는 요일 중 하나 이상에 추가로 기초하는, 방법.</claim></claimInfo><claimInfo><claim>30. 어시스턴트 디바이스로서,적어도 하나의 프로세서; 및실행될 때, 상기 적어도 하나의 프로세서로 하여금 청구항 1 내지 29 중 어느 한 항의 방법을 수행하게 하는 명령어들을 저장하는 메모리를 포함하는, 어시스턴트 디바이스.</claim></claimInfo><claimInfo><claim>31. 시스템으로서,적어도 하나의 프로세서; 및실행될 때, 상기 적어도 하나의 프로세서로 하여금 청구항 1 내지 29 중 어느 한 항의 방법을 수행하게 하는 명령어들을 저장하는 메모리를 포함하는, 적어도 하나의 컴퓨팅 디바이스.</claim></claimInfo><claimInfo><claim>32. 명령어들을 포함하는 비일시적 컴퓨터 판독가능 저장 매체로서, 상기 명령어들은 실행될 때, 상기 적어도 하나의 프로세서로 하여금 청구항 1 내지 29 중 어느 한 항의 방법을 수행하게 하는, 비일시적 컴퓨터 판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 캘리포니아 마운틴 뷰 엠피시어터 파크웨이 **** (우:*****)</address><code>520050013456</code><country>미국</country><engName>Google LLC</engName><name>구글 엘엘씨</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>CARBUNE, Victor</engName><name>카분 빅터</name></inventorInfo><inventorInfo><address>미국 캘리포니아 마운틴 뷰 엠...</address><code> </code><country> </country><engName>SHARIFI, Matthew</engName><name>샤리피 매튜</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 강남대로 *** (논현동) *-*F(박장원특허법률사무소)</address><code>919980002023</code><country>대한민국</country><engName>PARK, Jang Won</engName><name>박장원</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2021.02.25</priorityApplicationDate><priorityApplicationNumber>17/185,611</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2023.02.28</receiptDate><receiptNumber>1-1-2023-0232682-24</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2023.03.07</receiptDate><receiptNumber>1-5-2023-0037643-73</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020237007208.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93b047fb12ca1707d871c1929ea8ad031918dc7f155635fa9d528dea98a339e3ab654218b4e5d84c49c3cd1a6faaf6e98fbf65685514237e10</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf76896196bcf4e5b6ea04470364ca657e73b08ccb8f59ae3f90ec667a3e812a3bcec20d96c4c4abf76ecccc805ce039eca481d5985cbd9742</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>