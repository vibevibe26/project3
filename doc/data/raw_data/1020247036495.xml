<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:01:19.119</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.04.05</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2024-7036495</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>객체에 대한 픽셀 심도 결정</inventionTitle><inventionTitleEng>PIXEL DEPTH DETERMINATION FOR OBJECT</inventionTitleEng><openDate>2024.12.06</openDate><openNumber>10-2024-0171132</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2024.10.31</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2024.10.31</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 17/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/50</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/70</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/26</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/20</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/11</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2019.01.01)</ipcDate><ipcNumber>G06N 20/20</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 이미지에서 묘사되는 사람에 증강 현실 엘리먼트들을 적용하기 위한 동작들을 수행하기 위한 방법들 및 시스템들이 개시된다. 이러한 동작들은, 사람의 묘사를 표현하는 데이터를 포함하는 이미지를 수신하는 동작; 이미지의 부분을 추출하는 동작; 사람의 묘사를 표현하는 데이터에 대한 관심의 포인트의 심도를 예측하기 위해 부분에 제1 머신 학습 모델 단계를 적용하는 단계; 관심의 포인트의 예측된 심도에 대한 이미지의 부분에서의 각각의 픽셀의 상대적 심도를 예측하기 위해 이미지의 부분에 제2 머신 학습 모델 단계를 적용하는 동작; 머신 학습 모델의 제1 및 제2 단계들의 출력들에 기초하여 사람의 묘사를 표현하는 데이터의 조밀한 심도 재구성을 생성하는 단계; 및 조밀한 심도 재구성에 기초하여 이미지에 하나 이상의 AR 엘리먼트를 적용하는 동작을 포함한다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2023.10.12</internationOpenDate><internationOpenNumber>WO2023196387</internationOpenNumber><internationalApplicationDate>2023.04.05</internationalApplicationDate><internationalApplicationNumber>PCT/US2023/017550</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 방법으로서,클라이언트 디바이스의 하나 이상의 프로세서에 의해, 사람의 묘사를 표현하는 데이터를 포함하는 이미지를 수신하는 단계;상기 사람의 묘사를 표현하는 데이터에 대응하는 상기 이미지의 부분을 추출하는 단계;상기 사람의 묘사를 표현하는 데이터에 대한 관심의 포인트의 심도를 예측하기 위해 상기 이미지의 부분에 제1 머신 학습 모델 단계를 적용하는 단계;상기 관심의 포인트의 상기 예측된 심도에 대한 상기 이미지의 부분에서의 각각의 픽셀의 상대적 심도를 예측하기 위해 상기 이미지의 부분에 제2 머신 학습 모델 단계를 적용하는 단계;상기 머신 학습 모델의 상기 제1 및 제2 단계들의 출력들에 기초하여 상기 사람의 묘사를 표현하는 데이터의 조밀한 심도 재구성을 생성하는 단계; 및상기 조밀한 심도 재구성에 기초하여 상기 이미지에 하나 이상의 AR(augmented reality) 엘리먼트를 적용하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 관심의 포인트는 상기 사람의 흉골을 포함하고, 상기 머신 학습 모델의 상기 제1 및 제2 단계들은 상기 관심의 포인트의 심도 및 상기 관심의 포인트의 예측된 심도에 대한 상기 이미지의 부분에서의 각각의 픽셀의 상기 상대적 심도를 동시에 출력하는 상기 머신 학습 모델의 공통 부분의 일부인 방법.</claim></claimInfo><claimInfo><claim>3. 제1항 또는 제2항에 있어서, 상기 제1 머신 학습 모델 단계는 제1 머신 학습 모델을 포함하고, 상기 제2 머신 학습 모델 단계는 제2 머신 학습 모델을 포함하는 방법.</claim></claimInfo><claimInfo><claim>4. 제1항 내지 제3항 중 어느 한 항에 있어서, 상기 머신 학습 모델의 상기 제1 단계의 상기 출력은 상기 이미지를 캡처하기 위해 사용되는 카메라와 상기 이미지에 대해 글로벌인 제1 좌표계에서의 상기 관심의 포인트 사이의 거리를 포함하는 방법.</claim></claimInfo><claimInfo><claim>5. 제4항에 있어서, 상기 머신 학습 모델의 상기 제2 단계의 상기 출력은 각각의 전경 픽셀에 대한 상기 관심의 포인트로부터의 심도 및 상기 이미지의 부분에서의 각각의 픽셀에 대한 세그먼트화 마스크를 포함하는- 상기 이미지의 부분에서의 각각의 전경 픽셀에 대한 상기 관심의 포인트로부터의 상기 심도는 상기 이미지의 부분에 로컬인 제2 좌표계에서 표현됨 - 방법.</claim></claimInfo><claimInfo><claim>6. 제1항 내지 제5항 중 어느 한 항에 있어서, 상기 조밀한 심도 재구성에 기초하여 조밀한 포인트 클라우드를 생성하는 단계를 추가로 포함하는 방법.</claim></claimInfo><claimInfo><claim>7. 제1항 내지 제6항 중 어느 한 항에 있어서, 상기 하나 이상의 AR 엘리먼트를 적용하는 단계는 상기 사람의 신체의 지오메트리, 상기 사람의 머리카락, 상기 사람의 복장, 또는 상기 사람에 의해 착용되는 하나 이상의 액세서리 중 적어도 하나에 기초하여 상기 하나 이상의 AR 엘리먼트를 적용하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>8. 제1항 내지 제7항 중 어느 한 항에 있어서, 추가로,비디오의 제1 프레임에서 묘사되는 상기 사람을 표현하는 데이터의 제1 부분 상에 상기 하나 이상의 AR 엘리먼트를 디스플레이하는 단계- 상기 사람은 상기 제1 프레임에서의 제1 위치에 위치결정됨 -;상기 사람이 상기 제1 위치로부터 상기 비디오의 제2 프레임에서의 제2 위치로 이동하였다고 결정하는 단계; 및상기 이미지에서 묘사되는 상기 사람을 표현하는 데이터 상에 상기 하나 이상의 AR 엘리먼트의 디스플레이를 유지하기 위해 상기 제2 프레임에서의 상기 하나 이상의 AR 엘리먼트의 디스플레이 위치를 업데이트하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>9. 제1항 내지 제8항 중 어느 한 항에 있어서, 상기 AR 엘리먼트들은 상기 이미지를 포함하는 실시간 비디오 피드에 적용되는 방법.</claim></claimInfo><claimInfo><claim>10. 제1항 내지 제9항 중 어느 한 항에 있어서, 상기 하나 이상의 AR 엘리먼트를 적용하는 단계는 상기 사람의 묘사를 표현하는 데이터를 하나 이상의 시각적 효과로 대체하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>11. 제1항 내지 제10항 중 어느 한 항에 있어서, 상기 하나 이상의 AR 엘리먼트를 적용하는 단계는,상기 하나 이상의 AR 엘리먼트 중의 AR 엘리먼트를 상기 이미지를 캡처하기 위해 사용되는 카메라와 상기 AR 엘리먼트 사이의 제1 거리와 연관시키는 단계;상기 조밀한 심도 재구성에 기초하여, 상기 이미지의 부분에서의 주어진 픽셀과 상기 카메라 사이의 제2 거리를 결정하는 단계; 및상기 제2 거리를 상기 제1 거리와 비교하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>12. 제11항에 있어서, 추가로,상기 제2 거리가 상기 제1 거리보다 더 크다고 결정하는 것에 응답하여, 상기 이미지의 부분에 제1 시각적 효과를 적용하는 단계; 및상기 제2 거리가 상기 제1 거리보다 더 작다고 결정하는 것에 응답하여, 상기 이미지의 부분에 제2 시각적 효과를 적용하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>13. 제1항 내지 제12항 중 어느 한 항에 있어서, 상기 이미지는 상기 사람을 묘사하는 비디오의 제1 프레임이고, 추가로,상기 비디오에서 묘사되는 사람의 이동이 검출됨에 따라 상기 조밀한 심도 재구성을 계속 업데이트하는 단계;상기 제2 거리가 상기 제1 거리보다 더 크다고 결정하는 것에 응답하여 상기 이미지의 부분에 제1 시각적 효과를 적용하는 단계; 및상기 조밀한 심도 재구성이 상기 사람의 명시된 부분에 대응하는 상기 주어진 픽셀이 제1 위치로부터 제2 위치로 이동하였다는 점을 표시함에 따라:상기 명시된 부분이 상기 제1 거리보다 더 작은 제3 거리에 있다고 결정하는 단계; 및상기 이미지의 부분에 제2 시각적 효과를 적용하는 것과 동시에 제1 방식으로 상기 AR 엘리먼트를 수정하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>14. 제13항에 있어서, 상기 제1 방식으로 상기 AR 엘리먼트를 수정하는 단계는 상기 제1 시각적 효과를 폐색하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>15. 제1항 내지 제14항 중 어느 한 항에 있어서, 상기 제1 방식으로 상기 AR 엘리먼트를 수정하는 단계는 상기 AR 엘리먼트에 제1 애니메이션을 적용하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>16. 제1항 내지 제15항 중 어느 한 항에 있어서, 상기 머신 학습 모델은 신경망- 상기 신경망은 인체들의 상이한 배향들을 묘사하는 이미지 부분들과 상기 인체들의 관심의 포인트들의 심도들 사이의 관계를 수립하도록 트레이닝됨 -을 포함하는 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서, 동작들을 수행하는 것에 의해 상기 머신 학습 모델을 트레이닝하는 단계를 추가로 포함하고, 상기 동작들은,복수의 트레이닝 데이터 세트들을 수신하는 동작- 상기 복수의 트레이닝 데이터 세트들 각각은 이미지에서 묘사되는 트레이닝 사람을 표현하는 트레이닝 부분 및 대응하는 실측 심도 데이터를 포함함 -;상기 트레이닝 사람의 주어진 관심의 포인트에 대한 추정된 심도 데이터를 예측하기 위해 제1 트레이닝 데이터 세트의 제1 트레이닝 부분에 상기 머신 학습 모델을 적용하는 동작;상기 제1 트레이닝 부분과 연관된 상기 실측 심도 데이터와 상기 추정된 심도 데이터 사이의 편차를 컴퓨팅하는 단계; 및상기 컴퓨팅된 편차에 기초하여 상기 머신 학습 모델의 하나 이상의 파라미터를 업데이트하는 동작을 포함하는 방법.</claim></claimInfo><claimInfo><claim>18. 제1항 내지 제17항 중 어느 한 항에 있어서, 상기 머신 학습 모델은 상기 이미지에서의 각각의 픽셀을, 상기 픽셀이 배경에 대응하는지 또는 상기 사람의 묘사를 표현하는 데이터에 대응하는지의 표시와 연관시키는 세그먼트화 벡터를 생성하는- 상기 하나 이상의 AR 엘리먼트는 상기 세그먼트화 벡터에 추가로 기초하여 적용됨 - 방법.</claim></claimInfo><claimInfo><claim>19. 시스템으로서,클라이언트 디바이스의 프로세서; 및명령어들을 저장한 메모리 컴포넌트를 포함하고, 상기 명령어들은, 상기 프로세서에 의해 실행될 때, 상기 프로세서로 하여금 동작들을 수행하게 하고, 상기 동작들은, 사람의 묘사를 표현하는 데이터를 포함하는 이미지를 수신하는 동작; 상기 이미지에서의 상기 사람의 묘사를 표현하는 데이터에 대응하는 상기 이미지의 부분을 추출하는 동작; 상기 사람의 묘사를 표현하는 데이터에 대한 관심의 포인트의 심도를 예측하기 위해 상기 이미지의 부분에 제1 머신 학습 모델 단계를 적용하는 동작; 상기 관심의 포인트의 상기 예측된 심도에 대한 상기 이미지의 부분에서의 각각의 픽셀의 상대적 심도를 예측하기 위해 상기 이미지의 부분에 제2 머신 학습 모델 단계를 적용하는 동작; 상기 머신 학습 모델의 상기 제1 및 제2 단계들의 출력들에 기초하여 상기 사람의 묘사를 표현하는 데이터의 조밀한 심도 재구성을 생성하는 동작; 및 상기 조밀한 심도 재구성에 기초하여 상기 이미지에 하나 이상의 AR(augmented reality) 엘리먼트를 적용하는 동작을 포함하는 시스템.</claim></claimInfo><claimInfo><claim>20. 명령어들을 저장한 비-일시적 컴퓨터-판독가능 저장 매체로서, 상기 명령어들은, 클라이언트 디바이스의 프로세서에 의해 실행될 때, 상기 프로세서로 하여금 동작들을 수행하게 하고, 상기 동작들은,사람의 묘사를 표현하는 데이터를 포함하는 이미지를 수신하는 동작;상기 이미지에서의 상기 사람의 묘사를 표현하는 데이터에 대응하는 상기 이미지의 부분을 추출하는 동작;상기 사람의 묘사를 표현하는 데이터에 대한 관심의 포인트의 심도를 예측하기 위해 상기 이미지의 부분에 제1 머신 학습 모델 단계를 적용하는 동작;상기 관심의 포인트의 상기 예측된 심도에 대한 상기 이미지의 부분에서의 각각의 픽셀의 상대적 심도를 예측하기 위해 상기 이미지의 부분에 제2 머신 학습 모델 단계를 적용하는 동작;상기 머신 학습 모델의 상기 제1 및 제2 단계들의 출력들에 기초하여 상기 사람의 묘사를 표현하는 데이터의 조밀한 심도 재구성을 생성하는 동작; 및상기 조밀한 심도 재구성에 기초하여 상기 이미지에 하나 이상의 AR(augmented reality) 엘리먼트를 적용하는 동작을 포함하는 비-일시적 컴퓨터-판독가능 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>미국 ***** 캘리포니아주 산타 모니카 써티퍼스트 스트리트 ****</address><code>520140253774</code><country>미국</country><engName>SNAP INC.</engName><name>스냅 인코포레이티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 ***** 캘리포니아 ...</address><code> </code><country> </country><engName>AITBAYEV, Madiyar</engName><name>아이트바예브, 마디야르</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아 ...</address><code> </code><country> </country><engName>FULKERSON, Brian</engName><name>풀커슨, 브라이언</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아 ...</address><code> </code><country> </country><engName>GULER, Riza, Alp</engName><name>굴러, 리자, 알프</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아 ...</address><code> </code><country> </country><engName>PAPANDREOU, Georgios</engName><name>파판드레우, 게오르기오스</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아 ...</address><code> </code><country> </country><engName>TAM, Himmy</engName><name>탐, 히미</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920010003368</code><country>대한민국</country><engName>Kim Yeon Song</engName><name>김연송</name></agentInfo><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.06.16</priorityApplicationDate><priorityApplicationNumber>17/842,006</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>그리스</priorityApplicationCountry><priorityApplicationDate>2022.04.05</priorityApplicationDate><priorityApplicationNumber>20220100299</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2024.10.31</receiptDate><receiptNumber>1-1-2024-1193539-12</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2024.10.31</receiptDate><receiptNumber>1-1-2024-1195182-63</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2024.11.04</receiptDate><receiptNumber>1-5-2024-0177037-09</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020247036495.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c93ec3f199d18332468f987d029c982afc287506e6c85be59d5c501b062fcf3470ddfb9241c93792fb21ed1e57d2d385396bec344bdd6fb6a11</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf47343113159ccbecba568cdbf0dc2b7035c14e056530c509233b6dfa437fbc9b22edb85be1288a8719d631af657c29104259df34b8262c7a</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>