<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:40:05.405</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2019.05.21</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2020-7037010</applicationNumber><claimCount>24</claimCount><examinerName> </examinerName><finalDisposal>등록결정(일반)</finalDisposal><inventionTitle>딥 러닝 시스템</inventionTitle><inventionTitleEng>DEEP LEARNING SYSTEM</inventionTitleEng><openDate>2021.01.12</openDate><openNumber>10-2021-0003937</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국제출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate>2022.05.20</originalExaminationRequestDate><originalExaminationRequestFlag>Y</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate>2020.12.22</translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2023.01.01)</ipcDate><ipcNumber>G06N 3/08</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2006.01.01)</ipcDate><ipcNumber>G06T 17/10</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2011.01.01)</ipcDate><ipcNumber>G06T 19/00</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2017.01.01)</ipcDate><ipcNumber>G06T 7/70</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 머신 학습 모델의 다양한 양태를 강화하기 위해 머신 학습 시스템이 제공된다. 일부 양태에서, 객체의 상당히 사실적인 사진 3차원(3D) 그래픽 모델이 액세스되고, 3D 그래픽 모드의 훈련 이미지 세트가 생성되고, 생성된 훈련 이미지 세트는 불완전성을 추가하고 훈련 이미지의 사실적인 사진 품질을 저하시킨다. 훈련 이미지 세트는 인공 신경망을 훈련시키기 위한 훈련 데이터로서 제공된다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate>2019.11.28</internationOpenDate><internationOpenNumber>WO2019226686</internationOpenNumber><internationalApplicationDate>2019.05.21</internationalApplicationDate><internationalApplicationNumber>PCT/US2019/033373</internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 방법으로서,메모리로부터, 객체의 합성 3차원(3D) 그래픽 모델에 액세스하는 단계 - 상기 3D 그래픽 모델은 사실적인 사진 해상도(photo-realistic resolution)를 가짐 -;상기 3D 그래픽 모델의 뷰들로부터 복수의 상이한 훈련 샘플을 생성하는 단계 - 상기 복수의 훈련 샘플은 실제 센서 디바이스에 의해 생성된 실제 샘플들의 특성들을 시뮬레이션하기 위해 상기 복수의 훈련 샘플에 불완전성(imperfection)들을 추가하도록 생성됨 -; 및상기 복수의 훈련 샘플을 포함하는 훈련 세트를 생성하는 단계 - 상기 훈련 데이터는 인공 신경망을 훈련시키기 위한 것임 -를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>2. 제1항에 있어서, 상기 복수의 훈련 샘플은 디지털 이미지를 포함하고 상기 센서 디바이스는 카메라 센서를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>3. 제1항 또는 제2항에 있어서, 상기 복수의 훈련 샘플은 상기 객체의 포인트 클라우드 표현들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>4. 제3항에 있어서, 상기 센서 디바이스는 LIDAR 센서를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>5. 제1항 내지 제4항 중 어느 한 항에 있어서,상기 센서 디바이스의 파라미터들을 나타내는 데이터에 액세스하는 단계; 및상기 파라미터들에 기초하여 상기 복수의 훈련 샘플에 추가할 불완전성들을 결정하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>6. 제5항에 있어서, 상기 데이터는 상기 센서 디바이스의 모델을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>7. 제1항 내지 제6항 중 어느 한 항에 있어서,상기 3D 그래픽 모델에 의해 모델링된 상기 객체의 하나 이상의 표면의 특성들을 나타내는 데이터에 액세스하는 단계; 및상기 특성들에 기초하여 상기 복수의 훈련 샘플에 추가할 불완전성들을 결정하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>8. 제7항에 있어서, 상기 3D 그래픽 모델은 상기 데이터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>9. 제1항 내지 제8항 중 어느 한 항에 있어서, 상기 불완전성들은 노이즈 또는 눈부심 중 하나 이상을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>10. 제1항 내지 제9항 중 어느 한 항에 있어서, 상기 복수의 상이한 훈련 샘플을 생성하는 단계는,환경 내의 조명을 시뮬레이션하기 위해 상기 3D 그래픽 모델에 상이한 조명 설정들을 적용하는 단계;상기 상이한 조명 설정들 중 특정한 하나의 적용 동안에 생성된 상기 복수의 훈련 샘플 중의 한 서브세트에 대한 불완전성들을 결정하는 단계 - 상기 복수의 훈련 샘플 중의 서브세트에 대한 불완전성들은 상기 특정한 조명 설정에 기초함 -를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>11. 제1항 내지 제10항 중 어느 한 항에 있어서, 상기 복수의 상이한 훈련 샘플을 생성하는 단계는,상기 3D 그래픽 모델을 상이한 그래픽 환경들에 배치하는 단계 - 상기 그래픽 환경들은 각각의 실제 환경들을 모델링함 -; 및상기 3D 그래픽 모델이 상이한 그래픽 환경들 내에 배치되어 있는 동안 상기 복수의 훈련 샘플의 서브세트를 생성하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>12. 제1항 내지 제11항 중 어느 한 항의 방법을 수행하는 수단을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>13. 제12항에 있어서, 상기 시스템은 장치를 포함하고, 상기 장치는 제1항 내지 제11항 중 어느 한 항의 방법의 적어도 일부를 수행하는 하드웨어 회로를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>14. 제1항 내지 제11항 중 어느 한 항의 방법을 수행하기 위해 프로세서에 의해 실행가능한 명령어들을 저장하는, 컴퓨터 판독가능한 저장 매체.</claim></claimInfo><claimInfo><claim>15. 방법으로서,Siamese 신경망에서 대상 입력 및 기준 입력을 수신하는 단계 - 상기 Siamese 신경망은 제1 복수의 계층을 포함하는 제1 네트워크 부분 및 제2 복수의 계층을 포함하는 제2 네트워크 부분을 포함하고, 상기 제1 네트워크 부분의 가중치들은 상기 제2 네트워크 부분의 가중치들과 동일하며, 상기 대상 입력은 상기 제1 네트워크 부분에 대한 입력으로서 제공되고 상기 기준 입력은 상기 제2 네트워크 부분에 대한 입력으로서 제공됨 -; 및상기 대상 입력 및 기준 입력에 기초하여 상기 Siamese 신경망의 출력을 생성하는 단계 - 상기 Siamese 신경망의 출력은 상기 기준 입력과 상기 대상 입력 사이의 유사성을 나타냄 -를 포함하는 방법.</claim></claimInfo><claimInfo><claim>16. 제15항에 있어서, 상기 출력을 생성하는 단계는,상기 기준 입력과 상기 대상 입력 사이의 차이량을 결정하는 단계; 및상기 차이량이 임계값을 충족하는지를 결정하는 단계 - 상기 출력은 상기 차이량이 상기 임계값을 충족하는지를 식별시킴 -를 포함하는 방법.</claim></claimInfo><claimInfo><claim>17. 제16항에 있어서, 상기 기준 입력과 상기 대상 입력 사이의 차이량을 결정하는 단계는,상기 제1 네트워크 부분에 의해 출력된 제1 피처 벡터 및 상기 제2 네트워크 부분에 의해 출력된 제2 피처 벡터를 수신하는 단계; 및상기 제1 피처 벡터 및 상기 제2 피처 벡터에 기초하여 차이 벡터를 결정하는 단계를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>18. 제15항 내지 제17항 중 어느 한 항에 있어서, 상기 출력을 생성하는 단계는 원샷 분류(one-shot classification)를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>19. 제15항 내지 제18항 중 어느 한 항에 있어서, 하나 이상의 합성 훈련 샘플을 이용하여 상기 Siamese 신경망을 훈련시키는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>20. 제19항에 있어서, 상기 하나 이상의 합성 훈련 샘플은 제1항 내지 제11항 중 어느 한 항의 방법에 따라 생성되는, 방법.</claim></claimInfo><claimInfo><claim>21. 제15항 내지 제20항 중 어느 한 항에 있어서, 상기 기준 입력은 합성적으로 생성된 샘플을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>22. 제21항에 있어서, 상기 합성적으로 생성된 샘플은 제1항 내지 제11항 중 어느 한 항의 방법에 따라 생성되는, 방법.</claim></claimInfo><claimInfo><claim>23. 제15항 내지 제22항 중 어느 한 항에 있어서, 상기 대상 입력은 제1 디지털 이미지를 포함하고 상기 기준 입력은 제2 디지털 이미지를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>24. 제15항 내지 제22항 중 어느 한 항에 있어서, 상기 대상 입력은 제1 포인트 클라우드 표현을 포함하고, 상기 기준 입력은 제2 포인트 클라우드 표현을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>25. 제15항 내지 제24항 중 어느 한 항의 방법을 수행하는 수단을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>26. 제25항에 있어서, 상기 시스템은 장치를 포함하고, 상기 장치는 제15항 내지 제24항 중 어느 한 항의 방법의 적어도 일부를 수행하는 하드웨어 회로를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>27. 제25항에 있어서, 상기 시스템은 로봇, 드론 또는 자율 차량 중 하나를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>28. 제15항 내지 제24항 중 어느 한 항의 방법을 수행하기 위해 프로세서에 의해 실행가능한 명령어들을 저장하는, 컴퓨터 판독가능한 저장 매체.</claim></claimInfo><claimInfo><claim>29. 방법으로서,제1 입력 데이터를 Siamese 신경망에 제공하는 단계 - 상기 제1 입력 데이터는 제1 자세(pose)로부터의 3D 공간의 제1 표현을 포함함 -;제2 입력 데이터를 상기 Siamese 신경망에 제공하는 단계 - 상기 제2 입력 데이터는 제2 자세로부터의 3D 공간의 제2 표현을 포함하고, 상기 Siamese 신경망은 제1 복수의 계층을 포함하는 제1 네트워크 부분 및 제2 복수의 계층을 포함하는 제2 네트워크 부분을 포함하고, 상기 제1 네트워크 부분의 가중치들은 상기 제2 네트워크 부분의 가중치들과 동일하며, 상기 제1 입력 데이터는 상기 제1 네트워크 부분에 대한 입력으로서 제공되고 상기 제2 입력 데이터는 상기 제2 네트워크 부분에 대한 입력으로서 제공됨 -; 및상기 Siamese 신경망의 출력을 생성하는 단계 - 상기 출력은 상기 제1 자세와 제2 자세 사이의 상대적 자세를 포함함 -를 포함하는 방법.</claim></claimInfo><claimInfo><claim>30. 제29항에 있어서, 상기 3D 공간의 제1 표현은 제1 3D 포인트 클라우드를 포함하고, 상기 3D 공간의 제2 표현은 제2 3D 포인트 클라우드를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>31. 제29항 또는 제30항에 있어서, 상기 3D 공간의 제1 표현은 제1 포인트 클라우드를 포함하고, 상기 3D 공간의 제2 표현은 제2 포인트 클라우드를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>32. 제31항에 있어서, 상기 제1 포인트 클라우드 및 상기 제2 포인트 클라우드 각각은 각각의 복셀화된 포인트 클라우드 표현들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>33. 제29항 내지 제32항 중 어느 한 항에 있어서,상기 상대적 자세에 기초하여 적어도 상기 제1 및 제2 입력 데이터로부터 상기 3D 공간의 3D 매핑을 생성하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>34. 제29항 내지 제32항 중 어느 한 항에 있어서,상기 상대적 자세에 기초하여 상기 3D 공간 내에서 상기 제1 자세의 관찰자의 위치를 결정하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>35. 제34항에 있어서, 상기 관찰자는 자율 머신을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>36. 제35항에 있어서, 상기 자율 머신은, 로봇, 드론 또는 자율 차량 중 하나를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>37. 제29항 내지 제36항 중 어느 한 항의 방법을 수행하는 수단을 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>38. 제37항에 있어서, 상기 시스템은 장치를 포함하고, 상기 장치는 제29항 내지 제36항 중 어느 한 항의 방법의 적어도 일부를 수행하는 하드웨어 회로를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>39. 제29항 내지 제36항 중 어느 한 항의 방법을 수행하기 위해 프로세서에 의해 실행가능한 명령어들을 저장하는, 컴퓨터 판독가능한 저장 매체.</claim></claimInfo><claimInfo><claim>40. 방법으로서,머신 학습 모델의 제1 부분에 대한 입력으로서 제1 센서 데이터를 제공하는 단계;상기 머신 학습 모델의 제2 부분에 대한 입력으로서 제2 센서 데이터를 제공하는 단계 - 상기 머신 학습 모델은 연결기(concatenator) 및 완전-접속된 계층들의 세트를 포함하고, 상기 제1 센서 데이터는 디바이스에 의해 생성된 제1 유형이고, 상기 제2 센서 데이터는 상기 디바이스에 의해 생성된 상이한 제2 유형이며, 상기 연결기는 상기 머신 학습 모델의 제1 부분의 출력을 제1 입력으로서 취하고 상기 머신 학습 모델의 제2 부분의 출력을 제2 입력으로서 취하며, 상기 연결기의 출력은 상기 완전-접속된 계층들의 세트에 제공됨 -; 및상기 제1 데이터 및 제2 데이터로부터, 환경 내에서의 상기 디바이스의 자세를 포함하는 상기 머신 학습 모델의 출력을 생성하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>41. 제40항에 있어서, 상기 제1 센서 데이터는 이미지 데이터를 포함하고, 상기 제2 센서 데이터는 상기 디바이스의 움직임을 식별하는, 방법.</claim></claimInfo><claimInfo><claim>42. 제41항에 있어서, 상기 이미지 데이터는 적색-녹색-청색(red-green-blue)(RGB) 데이터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>43. 제41항에 있어서, 상기 이미지 데이터는 3D 포인트 클라우드 데이터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>44. 제41항에 있어서, 상기 제2 센서 데이터는 관성 측정 유닛(inertial measurement unit)(IMU) 데이터를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>45. 제41항에 있어서, 상기 제2 센서 데이터는 전역적 위치결정 데이터(global positioning data)를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>46. 제40항 내지 제45항 중 어느 한 항에 있어서, 상기 머신 학습 모델의 제1 부분은 상기 제1 유형의 센서 데이터에 대해 튜닝되고, 상기 머신 학습 모델의 제2 부분은 상기 제2 유형의 센서 데이터에 대해 튜닝되는, 방법.</claim></claimInfo><claimInfo><claim>47. 제40항 내지 제46항 중 어느 한 항에 있어서, 상기 머신 학습 모델의 제3 부분에 대한 입력으로서 제3 유형의 제3 센서 데이터를 제공하는 단계를 더 포함하고, 상기 출력은 상기 제3 데이터에 기초하여 추가로 발생되는, 방법.</claim></claimInfo><claimInfo><claim>48. 제40항 내지 제47항 중 어느 한 항에 있어서, 상기 자세의 출력은 회전 성분 및 병진 성분(translational component)을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>49. 제48항에 있어서, 상기 완전-접속된 계층들의 세트 중 하나는 상기 회전 성분을 결정하는 완전-접속된 계층을 포함하고, 상기 완전-접속된 계층들의 세트 중 다른 하나는 상기 병진 성분을 결정하는 완전-접속된 계층을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>50. 제40항 내지 제49항 중 어느 한 항에 있어서, 상기 머신 학습 모델의 제1 및 제2 부분들 중 하나 또는 양쪽 모두는 각각의 콘볼루션 계층들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>51. 제40항 내지 제50항 중 어느 한 항에 있어서, 상기 머신 학습 모델의 제1 및 제2 부분들 중 하나 또는 양쪽 모두는 하나 이상의 각각의 장단기 메모리(long short-term memory)(LSTM) 블록을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>52. 제40항 내지 제51항 중 어느 한 항에 있어서, 상기 디바이스는 자율 머신을 포함하고, 상기 자율 머신은 상기 자세에 기초하여 상기 환경 내에서 네비게이션하는, 방법.</claim></claimInfo><claimInfo><claim>53. 제52항에 있어서, 상기 자율 머신은 로봇, 드론 또는 자율 차량 중 하나를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>54. 제40항 내지 제52항 중 어느 한 항의 방법을 수행하는 수단을 포함하는 시스템.</claim></claimInfo><claimInfo><claim>55. 제54항에 있어서, 상기 시스템은 장치를 포함하고, 상기 장치는 제40항 내지 제52항 중 어느 한 항의 방법의 적어도 일부를 수행하는 하드웨어 회로를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>56. 제40항 내지 제52항 중 어느 한 항의 방법을 수행하기 위해 프로세서에 의해 실행가능한 명령어들을 저장하는, 컴퓨터 판독가능한 저장 매체.</claim></claimInfo><claimInfo><claim>57. 방법으로서,신경망들의 세트의 무작위 생성을 요청하는 단계;상기 신경망들의 세트 중의 각각의 것을 이용하여 머신 학습 작업을 수행하는 단계 - 상기 머신 학습 작업은 특정한 처리 하드웨어를 이용하여 수행됨 -;상기 신경망들의 세트 중의 각각에 대한 상기 머신 학습 작업의 수행의 속성들을 모니터링하는 단계 - 상기 속성들은 상기 머신 학습 작업의 결과들의 정확성을 포함함 -; 및상기 특정한 처리 하드웨어를 이용하여 상기 머신 학습 작업을 수행하는데 이용될 때 최고 성능의 신경망의 속성들에 기초하여 상기 신경망들의 세트 중 최고 성능의 것을 식별하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>58. 제57항에 있어서,머신 학습 애플리케이션을 수행할 때 머신에 의해 이용될 상기 최고 성능의 신경망을 제공하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>59. 제57항 또는 제58항에 있어서,상기 최고 성능의 신경망의 특성들을 결정하는 단계; 및상기 특성들에 따라 신경망들의 제2 세트의 생성을 요청하는 단계 - 상기 신경망들의 제2 세트는, 각각이 상기 특성들 중 하나 이상을 포함하는 복수의 상이한 신경망을 포함함 -;상기 신경망들의 제2 세트 중의 각각의 것을 이용하여 상기 머신 학습 작업을 수행하는 단계 - 상기 머신 학습 작업은 상기 특정한 처리 하드웨어를 이용하여 수행됨 -;상기 신경망들의 제2 세트 중의 각각에 대한 상기 머신 학습 작업의 수행의 속성들을 모니터링하는 단계; 및상기 속성들에 기초하여 상기 신경망들의 제2 세트 중 최고 성능의 것을 식별하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>60. 제57항 내지 제59항 중 어느 한 항에 있어서, 파라미터들에 기초하여 기준을 수신하는 단계를 더 포함하고, 상기 최고 성능의 신경망은 상기 기준에 기초하는, 방법.</claim></claimInfo><claimInfo><claim>61. 제57항 내지 제60항 중 어느 한 항에 있어서, 상기 속성들은 상기 특정한 처리 하드웨어의 속성들을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>62. 제61항에 있어서, 상기 특정한 처리 하드웨어의 속성들은, 상기 머신 학습 작업의 수행 동안 상기 특정한 처리 하드웨어에 의해 소비되는 전력, 상기 머신 학습 작업의 수행 동안 상기 특정한 처리 하드웨어의 온도, 및 상기 특정한 처리 하드웨어 상에 상기 신경망을 저장하는데 이용되는 메모리 중 하나 이상을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>63. 제57항 내지 제62항 중 어느 한 항에 있어서, 상기 속성들은 상기 신경망들의 세트 중 대응하는 것을 이용해 상기 머신 학습 작업을 완료하는 시간을 포함하는, 방법.</claim></claimInfo><claimInfo><claim>64. 제57항 내지 제63항 중 어느 한 항의 방법을 수행하는 수단을 포함하는 시스템.</claim></claimInfo><claimInfo><claim>65. 제64항에 있어서, 상기 시스템은 장치를 포함하고, 상기 장치는 제57항 내지 제63항 중 어느 한 항의 방법의 적어도 일부를 수행하는 하드웨어 회로를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>66. 제57항 내지 제63항 중 어느 한 항의 방법을 수행하기 위해 프로세서에 의해 실행가능한 명령어들을 저장하는, 컴퓨터 판독가능한 저장 매체.</claim></claimInfo><claimInfo><claim>67. 방법으로서,복수의 커널을 포함하는 신경망을 식별하는 단계 - 상기 커널들 중의 각각의 것은 각각의 세트의 가중치들을 포함함 -;상기 복수의 커널을 특정한 세트의 커널들로 감소시키기 위해 하나 이상의 파라미터에 따라 상기 복수의 커널의 서브세트를 프루닝(pruning)하는 단계;상기 특정한 세트의 커널들에서 가중치들의 서브세트를 프루닝하여 상기 신경망의 프루닝된 버전을 형성하는 단계를 포함하고, 상기 가중치들의 서브세트를 프루닝하는 것은 상기 가중치들의 서브세트에서 하나 이상의 비제로 가중치를 0으로 할당하며, 상기 가중치들의 서브세트는 상기 가중치들의 원래의 값들에 기초하여 선택되는, 방법.</claim></claimInfo><claimInfo><claim>68. 제67항에 있어서, 상기 가중치들의 서브세트는 임계값 아래로 떨어지는 상기 가중치들의 서브세트의 값들에 기초하여 프루닝되는, 방법.</claim></claimInfo><claimInfo><claim>69. 제67항 또는 제68항에 있어서, 상기 커널들 및 가중치들의 프루닝을 통한 정확도 손실의 적어도 일부를 복구하기 위해 상기 신경망의 프루닝된 버전을 이용하여 머신 학습 작업의 하나 이상의 반복을 수행하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>70. 제67항 내지 제69항 중 어느 한 항에 있어서, 상기 신경망의 컴팩트 버전을 생성하기 위해 상기 신경망의 프루닝된 버전에서 프루닝되지 않은 가중치들의 값들을 양자화하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>71. 제70항에 있어서, 상기 양자화는 로그 베이스 양자화(log base quantization)를 포함하는, 방법.</claim></claimInfo><claimInfo><claim>72. 제71항에 있어서, 상기 가중치들은 부동 소수점 값들로부터 베이스 2인 값들로 양자화되는, 방법.</claim></claimInfo><claimInfo><claim>73. 제67항 내지 제72항 중 어느 한 항에 있어서, 저밀도 행렬 산술(sparse matrix arithmetic)에 적응된 하드웨어를 이용한 머신 학습 작업들의 실행을 위해 상기 신경망의 프루닝된 버전을 제공하는 단계를 더 포함하는, 방법.</claim></claimInfo><claimInfo><claim>74. 제67항 내지 제73항 중 어느 한 항의 방법을 수행하는 수단을 포함하는 시스템.</claim></claimInfo><claimInfo><claim>75. 제64항에 있어서, 상기 시스템은 장치를 포함하고, 상기 장치는 제67항 내지 제73항 중 어느 한 항의 방법의 적어도 일부를 수행하는 하드웨어 회로를 포함하는, 시스템.</claim></claimInfo><claimInfo><claim>76. 제67항 내지 제73항 중 어느 한 항의 방법을 수행하기 위해 프로세서에 의해 실행가능한 명령어들을 저장하는, 컴퓨터 판독가능한 저장 매체.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>네덜란드 엔지 스히폴-리크 **** 카프로닐란 **</address><code>520170104249</code><country>아일랜드</country><engName>Movidius Limited</engName><name>모비디어스 리미티드</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>아일랜드 디** 엔*씨* ...</address><code> </code><country> </country><engName>MOLONEY, David Macdara</engName><name>몰로니, 데이비드 막다라</name></inventorInfo><inventorInfo><address>아일랜드 에이** 이*씨* 더...</address><code> </code><country> </country><engName>BUCKLEY, Leonie Raideen</engName><name>버클리, 레오니 라이딘</name></inventorInfo><inventorInfo><address>스페인 ***** 다이미엘 (...</address><code> </code><country> </country><engName>RODRIGUEZ MARTIN DE LA SIERRA, Luis M.</engName><name>로드리게즈 마틴 데 라 시에라, 루이스 엠.</name></inventorInfo><inventorInfo><address>스페인 ***** 카스티야 라 만...</address><code> </code><country> </country><engName>MARQUEZ RODRIGUEZ-PERAL, Carlos</engName><name>마르케즈 로드리게즈-페랄, 카를로스</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아...</address><code> </code><country> </country><engName>BRICK, Cormac M.</engName><name>브릭, 코르막 엠.</name></inventorInfo><inventorInfo><address>아일랜드 에이** ...</address><code> </code><country> </country><engName>BYRNE, Jonathan David</engName><name>번, 조나단 데이비드</name></inventorInfo><inventorInfo><address>아일랜드 디** 에이엑스*피...</address><code> </code><country> </country><engName>XU, Xiaofan</engName><name>쉬, 샤오판</name></inventorInfo><inventorInfo><address>아일랜드 디 ** 더블린 ** 캐...</address><code> </code><country> </country><engName>PENA CARRILLO, Dexmont Alejandro</engName><name>페나 카릴로, 덱스몬트 알레한드로</name></inventorInfo><inventorInfo><address>미국 ***** 캘리포니아주 산...</address><code> </code><country> </country><engName>PARK, Mi Sun</engName><name>박, 미 선</name></inventorInfo><inventorInfo><address>미국 ***** 워싱턴주...</address><code> </code><country> </country><engName>PALLA, Alessandro</engName><name>팔라, 알레산드로</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>920020002981</code><country>대한민국</country><engName>Lee Min Ho</engName><name>이민호</name></agentInfo><agentInfo><address>서울특별시 종로구 사직로*길 **, 세양빌딩 (내자동) *층(김.장법률사무소)</address><code>919980003619</code><country>대한민국</country><engName>YANG, Young June</engName><name>양영준</name></agentInfo><agentInfo><address>서울 중구 정동길 **-** (정동, 정동빌딩) **층(김.장법률사무소)</address><code>919990005000</code><country>대한민국</country><engName>PAIK MAN GI</engName><name>백만기</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2018.05.23</priorityApplicationDate><priorityApplicationNumber>62/675,601</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Document according to the Article 203 of Patent Act</documentEngName><documentName>[특허출원]특허법 제203조에 따른 서면</documentName><receiptDate>2020.12.22</receiptDate><receiptNumber>1-1-2020-1397353-18</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notice of Acceptance</documentEngName><documentName>수리안내서</documentName><receiptDate>2021.01.05</receiptDate><receiptNumber>1-5-2021-0002102-17</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[심사청구]심사청구서·우선심사신청서</documentName><receiptDate>2022.05.20</receiptDate><receiptNumber>1-1-2022-0535477-10</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2022.05.20</receiptDate><receiptNumber>1-1-2022-0535468-09</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Notification of reason for refusal</documentEngName><documentName>의견제출통지서</documentName><receiptDate>2025.05.09</receiptDate><receiptNumber>9-5-2025-0444040-10</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName> </documentEngName><documentName>[거절이유 등 통지에 따른 의견]의견서·답변서·소명서</documentName><receiptDate>2025.07.09</receiptDate><receiptNumber>1-1-2025-0775319-81</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>보정승인간주 (Regarded as an acceptance of amendment) </commonCodeName><documentEngName>[Amendment to Description, etc.] Amendment</documentEngName><documentName>[명세서등 보정]보정서</documentName><receiptDate>2025.07.09</receiptDate><receiptNumber>1-1-2025-0775301-60</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>발송처리완료 (Completion of Transmission) </commonCodeName><documentEngName>Decision to grant</documentEngName><documentName>등록결정서</documentName><receiptDate>2025.09.23</receiptDate><receiptNumber>9-5-2025-0925169-05</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020207037010.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c932e4bc43522830d523f95c70132f7bccf0319eecedaa54b1fdd413bb6c3002dcd075bf21087a2081254227e46411f1598b541f72e3be26348</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cf4a8e85342c1d22ad4406c4152d0758eb7506501dd7825f0df2cd5f7f369e22a81a1b015ffb23d280c7d844076e32308b5b406f6a507a68fd</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>