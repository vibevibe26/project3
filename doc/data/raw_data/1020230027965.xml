<?xml version="1.0" encoding="UTF-8" standalone="yes"?><response><header><requestMsgID></requestMsgID><responseTime>2025-11-17 18:11:13.1113</responseTime><responseMsgID></responseMsgID><successYN>Y</successYN><resultCode>00</resultCode><resultMsg>NORMAL SERVICE.</resultMsg></header><body><item><biblioSummaryInfoArray><biblioSummaryInfo><applicationDate>2023.03.02</applicationDate><applicationFlag> </applicationFlag><applicationNumber>10-2023-0027965</applicationNumber><claimCount>20</claimCount><examinerName> </examinerName><finalDisposal> </finalDisposal><inventionTitle>비디오 시맨틱 분할 네트워크 방법 및 이를 포함하는 시스템</inventionTitle><inventionTitleEng>VIDEO SEMANTIC SEGMENTATION NETWORK METHOD AND  INCLUDING SYSTEM THEREOF</inventionTitleEng><openDate>2024.06.05</openDate><openNumber>10-2024-0080073</openNumber><originalApplicationDate> </originalApplicationDate><originalApplicationKind>국내출원/신규</originalApplicationKind><originalApplicationNumber> </originalApplicationNumber><originalExaminationRequestDate> </originalExaminationRequestDate><originalExaminationRequestFlag>N</originalExaminationRequestFlag><publicationDate> </publicationDate><publicationNumber> </publicationNumber><registerDate> </registerDate><registerNumber> </registerNumber><registerStatus>공개</registerStatus><translationSubmitDate> </translationSubmitDate></biblioSummaryInfo></biblioSummaryInfoArray><ipcInfoArray><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 20/40</ipcNumber></ipcInfo><ipcInfo><ipcDate>(2022.01.01)</ipcDate><ipcNumber>G06V 10/75</ipcNumber></ipcInfo></ipcInfoArray><familyInfoArray><familyInfo/></familyInfoArray><abstractInfoArray><abstractInfo><astrtCont> 본 개시는 비디오 시맨틱 분할 네트워크 방법 및 이를 포함하는 시스템이 개시된다. 본 개시의 기술적 사상에 따른 방법은, 시맨틱 분할 네트워크에서 복수의 프레임들로부터 입력 데이터를 수신하는 단계, 복수의 프레임들에 대한 실측 라벨을 계산하는 단계, 복수의 프레임들에 대한 실측 라벨에서 실측 시간적 시맨틱 경계 맵을 생성하는 단계, 입력 데이터의 출력에 기반으로 예측된 시간적 시맨틱 경계 맵을 생성하는 단계 및, 실측 시간적 시맨틱 경계 맵 및 예측된 시간적 시맨틱 경계 맵을 기반으로 하는 손실을 결정하는 단계를 포함할 수 있다. </astrtCont></abstractInfo></abstractInfoArray><internationalInfoArray><internationalInfo><internationOpenDate> </internationOpenDate><internationOpenNumber> </internationOpenNumber><internationalApplicationDate> </internationalApplicationDate><internationalApplicationNumber> </internationalApplicationNumber></internationalInfo></internationalInfoArray><claimInfoArray><claimInfo><claim>1. 시맨틱 분할 네트워크에서 복수의 프레임들로부터 입력 데이터를 수신하는 단계;상기 복수의 프레임들에 대한 실측 라벨을 계산하는 단계;상기 복수의 프레임들에 대한 상기 실측 라벨에서 실측 시간적 시맨틱 경계 맵 생성하는 단계;상기 입력 데이터의 출력을 기반으로 예측된 시간적 시맨틱 경계 맵을 생성하는 단계; alc상기 실측 시간적 시맨틱 경계 맵 및 상기 예측된 시간적 시맨틱 경계 맵을 기반으로 하는 손실을 결정하는 단계를 포함하는 방법.</claim></claimInfo><claimInfo><claim>2. 제1 항에 있어서,상기 실측 시간적 시맨틱 경계 맵 및 상기 예측된 시간적 시맨틱 경계 맵이 동일해야 상기 결정된 손실을 최소화하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>3. 제2 항에 있어서,상기 결정된 손실을 기반으로 시맨틱 분할 모델을 트레이닝 하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>4. 제1 항에 있어서,상기 입력 데이터는 상기 복수의 프레임들의 비디오 데이터로부터 얻은 이미지 프레임인 것을 특징으로 하는 방법. </claim></claimInfo><claimInfo><claim>5. 제4 항에 있어서,상기 입력 데이터의 출력은 상기 복수의 프레임들에서 검벨(gumbel) 소프트맥스 출력을 감산하여 생성되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>6. 제1 항에 있어서,상기 입력 데이터는 광학 흐름 또는 모션 추정 기술에 의해 처리되지 않는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>7. 제1 항에 있어서,상기 실측 시간적 시맨틱 경계 맵을 생성하는 것과 상기 예측된 시간적 경계 맵을 생성하는 것 중 적어도 하나는 광학 흐름 또는 모션 추정을 수행하는 것을 포함하는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>8. 제1 항에 있어서,상기 예측된 시맨틱 분할 맵에서 상기 복수의 프레임들 각각에 대해 동일한 예측 프로세서가 수행되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>9. 제1 항에 있어서,상기 결정된 손실에 기초하여 적어도 하나의 시간적 융합 모듈로 비디오 시맨틱 분할 네트워크 트레이닝시키는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>10. 제1 항에 있어서,상기 입력 데이터의 출력에 대해 소프트맥스 교차 엔트로피를 수행하는 단계를 더 포함하는 방법.</claim></claimInfo><claimInfo><claim>11. 제1 항에 있어서,상기 실측 시간적 시맨틱 경계 맵은 상기 복수의 프레임들 중 두 개의 연속 프레임의 실측 라벨 사이의 감산에 의해 생성되는 것을 특징으로 하는 방법.</claim></claimInfo><claimInfo><claim>12. 프로세서를 포함하는 시스템에 있어서,상기 프로세서는,시맨틱 분할 네트워크에서 복수의 프레임들로부터 입력 데이터를 수신하고,상기 복수의 프레임들의 실측 라벨을 계산하고,상기 복수의 프레임들의 상기 실측 라벨로부터 실측 시간적 시맨틱 경계 맵을 생성하고,상기 입력 데이터의 출력에 기초하여 예측된 시간적 시맨틱 경계 맵을 생성하고 및상기 실측 시간적 시맨틱 경계 맵 및 상기 예측된 시간적 시맨틱 경계 맵을 기반으로 하는 손실을 결정하는 것을 포함하는 시스템.</claim></claimInfo><claimInfo><claim>13. 제12 항에 있어서,상기 프로세서는 상기 실측 시간적 시맨틱 경계 맵 및 상기 예측된 시간적 시맨틱 경계 맵이 동일해야 상기 결정된 손실을 최소화하는 것을 더 포함하는 시스템.</claim></claimInfo><claimInfo><claim>14. 제13 항에 있어서,상기 프로세서는 상기 결정된 손실을 기반으로 시맨틱 분할 모델을 트레이닝하는 것을 더 포함하는 시스템.</claim></claimInfo><claimInfo><claim>15. 제12 항에 있어서,상기 입력 데이터는 상기 복수의 프레임들의 비디오 데이터로부터 얻은 이미지 프레임들인 것을 특징으로 하는 시스템.</claim></claimInfo><claimInfo><claim>16. 제15 항에 있어서,상기 입력 데이터의 출력은 상기 복수의 프레임들로부터 검벨(gumble) 소프트 맥스 출력을 감산하여 생성되는 것을 특징으로 하는 시스템.</claim></claimInfo><claimInfo><claim>17. 제12 항에 있어서,상기 입력 데이터는 광학 흐름 또는 모션 추정 기술에 의해 처리되지 않는 것을 특징으로 하는 시스템.</claim></claimInfo><claimInfo><claim>18. 제12 항에 있어서,상기 실측 시간적 시맨틱 경계 맵을 생성하는 것과 상기 예측된 시간적 시맨틱 경계 맵을 생성하는 것 중 적어도 하나는 과학 흐름 또는 모션 추정을 수행하는 것을 포함하는 시스템.</claim></claimInfo><claimInfo><claim>19. 제12 항에 있어서,상기 예측된 시맨틱 분할 맵에서 상기 복수의 프레임들 각각에 대해 동일한 예측 프로세스가 수행되는 것을 특징으로 하는 시스템.</claim></claimInfo><claimInfo><claim>20. 제12 항에 있어서,상기 프로세서는 상기 결정된 손실에 기초하여 적어도 하나의 시간적 융합 모듈로 비디오 시맨틱 분할 네트워크를 트레이닝시키는 것을 더 포함하는 시스템.</claim></claimInfo></claimInfoArray><applicantInfoArray><applicantInfo><address>경기도 수원시 영통구...</address><code>119981042713</code><country>대한민국</country><engName>SAMSUNG ELECTRONICS CO., LTD.</engName><name>삼성전자주식회사</name></applicantInfo></applicantInfoArray><inventorInfoArray><inventorInfo><address>미국 캘리포니아 ***** ...</address><code> </code><country> </country><engName>SU, Hai</engName><name>수 하이</name></inventorInfo><inventorInfo><address>미국 캘리포니아 ***** ...</address><code> </code><country> </country><engName>ELKHAMY, Mostafa</engName><name>엘카미 모스타파</name></inventorInfo><inventorInfo><address>미국 캘리포니아 ***** ...</address><code> </code><country> </country><engName>LIU, Qingfeng</engName><name>류 칭펑</name></inventorInfo></inventorInfoArray><agentInfoArray><agentInfo><address>서울 강남구 언주로 **길 **, *층, **층, **층, **층(도곡동, 대림아크로텔)</address><code>920051000028</code><country>대한민국</country><engName>Y.P.LEE,MOCK&amp;PARTNERS</engName><name>리앤목특허법인</name></agentInfo></agentInfoArray><priorityInfoArray><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2022.11.29</priorityApplicationDate><priorityApplicationNumber>63/428,618</priorityApplicationNumber></priorityInfo><priorityInfo><priorityApplicationCountry>미국</priorityApplicationCountry><priorityApplicationDate>2023.02.08</priorityApplicationDate><priorityApplicationNumber>18/107,173</priorityApplicationNumber></priorityInfo></priorityInfoArray><designatedStateInfoArray/><priorArtDocumentsInfoArray/><legalStatusInfoArray><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>[Patent Application] Patent Application</documentEngName><documentName>[특허출원]특허출원서</documentName><receiptDate>2023.03.02</receiptDate><receiptNumber>1-1-2023-0242591-57</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Submission of Priority Certificate(USPTO)</documentEngName><documentName>우선권주장증명서류제출서(USPTO)</documentName><receiptDate>2023.03.07</receiptDate><receiptNumber>9-1-2023-9002717-58</receiptNumber></legalStatusInfo><legalStatusInfo><commonCodeName>수리 (Accepted) </commonCodeName><documentEngName>Submission of Priority Certificate(USPTO)</documentEngName><documentName>우선권주장증명서류제출서(USPTO)</documentName><receiptDate>2023.03.22</receiptDate><receiptNumber>9-1-2023-9003542-33</receiptNumber></legalStatusInfo></legalStatusInfoArray><imagePathInfo><docName>1020230027965.jpg</docName><largePath>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=6c650beb4cee9ce4122b704b88878c9355aca172e784f58d74867cd73c6c63498a2718a276f67b246f7f3a99f1995b40f6ff0eff61071f92d8b0c02388cb467353c69a0f2109866f</largePath><path>http://plus.kipris.or.kr/kiprisplusws/fileToss.jsp?arg=ed43a0609e94d6e22d01c5c32ba711cff11f4c227d1dbbe2a0eb48dd5d879d1d352cd6fc3ac16d03feeb5300592450cf9d24be302fcf075d78f4a4b9340df6b073b4067646cb3215</path></imagePathInfo><rndInfoArray/></item></body><count><numOfRows>1</numOfRows><pageNo>1</pageNo><totalCount>1</totalCount></count></response>