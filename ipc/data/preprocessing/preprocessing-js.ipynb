{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397e9178",
   "metadata": {},
   "source": [
    "## ì „ì²˜ë¦¬ ì½”ë“œ ì „ë¬¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a2bde",
   "metadata": {},
   "source": [
    "### XML íŒŒì¼ Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../EN_ipc_scheme_20250101.xml'ì„ íŒŒì‹±í•˜ì—¬ 'output_pretty.xml'ì— ì˜ˆì˜ê²Œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def pretty_print_xml(input_file, output_file):\n",
    "    \"\"\"\n",
    "    XML íŒŒì¼ì„ ì½ì–´ ë“¤ì—¬ì“°ê¸°ë¥¼ ì ìš©í•œ ë’¤,\n",
    "    íƒœê·¸ì™€ êµ¬ì¡°ê°€ ë³´ì´ë„ë¡ ìƒˆ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    (Python 3.9 ì´ìƒ ê¶Œì¥)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. XML íŒŒì¼ íŒŒì‹±\n",
    "        tree = ET.parse(input_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # 2. XML ë“¤ì—¬ì“°ê¸° ì ìš© (í•µì‹¬!)\n",
    "        # Python 3.9ë¶€í„° ì‚¬ìš© ê°€ëŠ¥\n",
    "        ET.indent(root, space=\"  \")  # ìŠ¤í˜ì´ìŠ¤ 2ì¹¸ìœ¼ë¡œ ë“¤ì—¬ì“°ê¸°\n",
    "\n",
    "        # 3. ì˜ˆì˜ê²Œ ì •ë ¬ëœ XMLì„ ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "        # xml_declaration=TrueëŠ” '<?xml version=\"1.0\" ...?>' ì„ ì–¸ë¶€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "        print(f\"'{input_file}'ì„ íŒŒì‹±í•˜ì—¬ '{output_file}'ì— ì˜ˆì˜ê²Œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: '{input_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"ì˜¤ë¥˜: {e}\")\n",
    "        print(\"ì°¸ê³ : ET.indent() í•¨ìˆ˜ëŠ” Python 3.9 ì´ìƒì—ì„œë§Œ ë™ì‘í•©ë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "\n",
    "# --- ì‚¬ìš© ì˜ˆì‹œ ---\n",
    "# 'input.xml' íŒŒì¼ì„ ì½ì–´ì„œ 'output_pretty.xml' íŒŒì¼ë¡œ ì €ì¥\n",
    "input_xml_file = \"../raw/EN_ipc_scheme_20250101.xml\"  # ì›ë³¸ XML íŒŒì¼ ê²½ë¡œ\n",
    "output_xml_file = \"../raw/output_pretty.xml\"  # ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "pretty_print_xml(input_xml_file, output_xml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81d6c8",
   "metadata": {},
   "source": [
    "### IPCì½”ë“œ Jsoní™” (WIPO í‘œì‹œ í˜•ì‹)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaaa581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../EN_ipc_scheme_20250101.xml' íŒŒì¼ íŒŒì‹± ì‹œì‘...\n",
      "XML êµ¬ì¡° ë¶„ì„ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "íŒŒì‹± ì¤‘ (ìµœìƒìœ„): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "íŒŒì‹± ì™„ë£Œ. (í•„í„°ë§ ì „ ì´ 81818ê°œ í•­ëª©)\n",
      "\n",
      "ì¤‘ë³µ í•„í„°ë§ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í•„í„°ë§ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81818/81818 [00:00<00:00, 2086517.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í•„í„°ë§ ì™„ë£Œ. (ìµœì¢… 69337ê°œ í•­ëª©)\n",
      "'ipc_filtered_data_formatted.json' íŒŒì¼ ì €ì¥ ì¤‘...\n",
      "\n",
      "--- ëª¨ë“  ì‘ì—… ì™„ë£Œ ---\n",
      "ì…ë ¥: ../EN_ipc_scheme_20250101.xml\n",
      "ì¶œë ¥: ipc_filtered_data_formatted.json\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re  # ì •ê·œ í‘œí˜„ì‹ì„ ìœ„í•´ import\n",
    "\n",
    "# --- 1. IPC ì‹¬ë³¼ í¬ë§·íŒ… í•¨ìˆ˜ (â­ ìƒˆë¡œ ì¶”ê°€ë¨) ---\n",
    "\n",
    "\n",
    "def format_ipc_symbol(symbol):\n",
    "    \"\"\"\n",
    "    'A01B0001040000' ê°™ì€ ê¸´ ì‹¬ë³¼ì„ 'A01B1/04' í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    ì§§ì€ ì‹¬ë³¼('A', 'A01B')ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ Subclass ë¶€ë¶„ê³¼ ìˆ«ì ë¶€ë¶„ì„ ë¶„ë¦¬\n",
    "    #    (ì˜ˆ: \"A01B\"ì™€ \"0001040000\"ìœ¼ë¡œ ë¶„ë¦¬)\n",
    "    #    IPC SubclassëŠ” \"H05K\"ì²˜ëŸ¼ [A-H][0-9][0-9][A-Z] í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤.\n",
    "    match = re.match(r\"([A-H][0-9]{2}[A-Z])([0-9]+)$\", symbol)\n",
    "\n",
    "    # 2. ë§¤ì¹˜ë˜ì§€ ì•Šìœ¼ë©´ (e.g., \"A\", \"A01\", \"A01B\") ì›ë³¸ ì‹¬ë³¼ ë°˜í™˜\n",
    "    if not match:\n",
    "        return symbol\n",
    "\n",
    "    prefix = match.group(1)  # \"A01B\"\n",
    "    numeric_part = match.group(2)  # \"0001040000\"\n",
    "\n",
    "    # 3. ìˆ«ì ë¶€ë¶„ì´ 6ìë¦¬(ë©”ì¸+ì„œë¸Œ) ì´ìƒì¸ì§€ í™•ì¸\n",
    "    if len(numeric_part) < 6:\n",
    "        return symbol  # í¬ë§·íŒ… ë¶ˆê°€, ì›ë³¸ ë°˜í™˜\n",
    "\n",
    "    # 4. ë©”ì¸ ê·¸ë£¹ (ì• 4ìë¦¬)\n",
    "    # \"0001\" -> 1 -> \"1\"\n",
    "    main_group = str(int(numeric_part[0:4]))\n",
    "\n",
    "    # 5. ì„œë¸Œ ê·¸ë£¹ (ë‹¤ìŒ 2ìë¦¬)\n",
    "    # \"04\"\n",
    "    sub_group = numeric_part[4:6]\n",
    "\n",
    "    # 6. ì¡°í•©\n",
    "    # ì„œë¸Œ ê·¸ë£¹ì´ \"00\"ì´ë©´ \"A01B1/00\"\n",
    "    if sub_group == \"00\":\n",
    "        return f\"{prefix}{main_group}/00\"\n",
    "    # ì„œë¸Œ ê·¸ë£¹ì´ \"00\"ì´ ì•„ë‹ˆë©´ \"A01B1/04\"\n",
    "    else:\n",
    "        return f\"{prefix}{main_group}/{sub_group}\"\n",
    "\n",
    "\n",
    "# --- 2. XML íŒŒì‹±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ (ìˆ˜ì •ë¨) ---\n",
    "\n",
    "\n",
    "def extract_title(element, namespaces):\n",
    "    title_parts = []\n",
    "    text_body = element.find(\"ns0:textBody\", namespaces)\n",
    "    if text_body is None:\n",
    "        return \"\"\n",
    "    title = text_body.find(\"ns0:title\", namespaces)\n",
    "    if title is None:\n",
    "        return \"\"\n",
    "    for title_part in title.findall(\"ns0:titlePart\", namespaces):\n",
    "        text_element = title_part.find(\"ns0:text\", namespaces)\n",
    "        if text_element is not None and text_element.text:\n",
    "            title_parts.append(text_element.text.strip())\n",
    "    return \" \".join(title_parts)\n",
    "\n",
    "\n",
    "def parse_recursive(element, namespaces, parent_symbol, ancestor_path):\n",
    "    \"\"\"\n",
    "    <ns0:ipcEntry> ìš”ì†Œë¥¼ ì¬ê·€ì ìœ¼ë¡œ íŒŒì‹±\n",
    "    (â­ ì‹¬ë³¼ í¬ë§·íŒ… ê¸°ëŠ¥ ì ìš©ë¨)\n",
    "    \"\"\"\n",
    "    processed_list = []\n",
    "\n",
    "    # 1. ì›ë³¸ ì‹¬ë³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    raw_symbol = element.get(\"symbol\")\n",
    "\n",
    "    # 2. (â­ ìˆ˜ì •) ì‹¬ë³¼ í¬ë§·íŒ…\n",
    "    formatted_symbol = format_ipc_symbol(raw_symbol)\n",
    "\n",
    "    kind = element.get(\"kind\")\n",
    "\n",
    "    # 3. (â­ ìˆ˜ì •) í¬ë§·íŒ…ëœ ì‹¬ë³¼ì„ pathì™€ parentì— ì‚¬ìš©\n",
    "    current_path = ancestor_path + [formatted_symbol]\n",
    "    title = extract_title(element, namespaces)\n",
    "\n",
    "    current_item = {\n",
    "        \"ipc_code\": formatted_symbol,  # í¬ë§·íŒ…ëœ ì½”ë“œë¡œ ì €ì¥\n",
    "        \"title_en\": title,\n",
    "        \"kind\": kind,\n",
    "        \"parent\": parent_symbol,\n",
    "        \"path\": current_path,\n",
    "    }\n",
    "    processed_list.append(current_item)\n",
    "\n",
    "    for child_element in element.findall(\"ns0:ipcEntry\", namespaces):\n",
    "        processed_list.extend(\n",
    "            parse_recursive(\n",
    "                child_element,\n",
    "                namespaces,\n",
    "                parent_symbol=formatted_symbol,  # ìì‹ì—ê²Œ í¬ë§·íŒ…ëœ ì½”ë“œë¥¼ ë¬¼ë ¤ì¤Œ\n",
    "                ancestor_path=current_path,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return processed_list\n",
    "\n",
    "\n",
    "# --- 3. ì¤‘ë³µ í•„í„°ë§ í—¬í¼ í•¨ìˆ˜ (ë™ì¼) ---\n",
    "\n",
    "\n",
    "def filter_ipc_data(data):\n",
    "    filtered_items = {}\n",
    "    print(\"\\nì¤‘ë³µ í•„í„°ë§ ì‹œì‘...\")\n",
    "    for item in tqdm(data, desc=\"í•„í„°ë§ ì¤‘\"):\n",
    "        code = item[\"ipc_code\"]\n",
    "        kind = item[\"kind\"]\n",
    "        if code not in filtered_items:\n",
    "            filtered_items[code] = item\n",
    "            continue\n",
    "        existing_item = filtered_items[code]\n",
    "        existing_kind = existing_item[\"kind\"]\n",
    "        if existing_kind == \"t\" and kind == \"c\":\n",
    "            filtered_items[code] = item\n",
    "    return list(filtered_items.values())\n",
    "\n",
    "\n",
    "# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë™ì¼) ---\n",
    "\n",
    "\n",
    "def process_xml_to_filtered_json(input_xml_file, output_json_file):\n",
    "    try:\n",
    "        namespaces = {\"ns0\": \"http://www.wipo.int/classifications/ipc/masterfiles\"}\n",
    "        print(f\"'{input_xml_file}' íŒŒì¼ íŒŒì‹± ì‹œì‘...\")\n",
    "        tree = ET.parse(input_xml_file)\n",
    "        root = tree.getroot()\n",
    "        all_ipc_data = []\n",
    "        top_level_entries = root.findall(\"ns0:ipcEntry\", namespaces)\n",
    "        if not top_level_entries:\n",
    "            print(\"ì˜¤ë¥˜: ìµœìƒìœ„ <ns0:ipcEntry> íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        print(\"XML êµ¬ì¡° ë¶„ì„ ì¤‘...\")\n",
    "        for entry in tqdm(top_level_entries, desc=\"íŒŒì‹± ì¤‘ (ìµœìƒìœ„)\"):\n",
    "            all_ipc_data.extend(\n",
    "                parse_recursive(entry, namespaces, parent_symbol=None, ancestor_path=[])\n",
    "            )\n",
    "        print(f\"\\níŒŒì‹± ì™„ë£Œ. (í•„í„°ë§ ì „ ì´ {len(all_ipc_data)}ê°œ í•­ëª©)\")\n",
    "\n",
    "        filtered_data = filter_ipc_data(all_ipc_data)\n",
    "\n",
    "        print(f\"\\ní•„í„°ë§ ì™„ë£Œ. (ìµœì¢… {len(filtered_data)}ê°œ í•­ëª©)\")\n",
    "\n",
    "        print(f\"'{output_json_file}' íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "        with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"\\n--- ëª¨ë“  ì‘ì—… ì™„ë£Œ ---\")\n",
    "        print(f\"ì…ë ¥: {input_xml_file}\")\n",
    "        print(f\"ì¶œë ¥: {output_json_file}\")\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: '{input_xml_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "\n",
    "# --- ì‚¬ìš© ì˜ˆì‹œ ---\n",
    "\n",
    "# (ì¤‘ìš”) ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "input_file = \"../raw/EN_ipc_scheme_20250101.xml\"  # ğŸ‘ˆ ì—¬ê¸°ì— ì‹¤ì œ ì›ë³¸ XML íŒŒì¼ ê²½ë¡œ\n",
    "output_file = (\n",
    "    \"../raw/json/ipc_filtered_data_formatted.json\"  # ğŸ‘ˆ ì—¬ê¸°ì— ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ\n",
    ")\n",
    "\n",
    "process_xml_to_filtered_json(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf3c61",
   "metadata": {},
   "source": [
    "### JSONê³„ì¸µ êµ¬ì¡°ì—ì„œ ì¢…ì†í˜• ì½”ë“œ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../raw/json/ipc_filtered_data_formatted.json' íŒŒì¼ ì½ëŠ” ì¤‘...\n",
      "íŒŒì¼ ì½ê¸° ì™„ë£Œ (ì´ 69337ê°œ í•­ëª©).\n",
      "ì¡°íšŒìš© ë§µ ìƒì„± ì¤‘...\n",
      "ë§µ ìƒì„± ì™„ë£Œ.\n",
      "ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69337/69337 [00:00<00:00, 306462.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'processed_data.json' íŒŒì¼ ì €ì¥ ì¤‘...\n",
      "íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì €ì¥ ìœ„ì¹˜: processed_data.json)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm  # tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "\n",
    "\n",
    "def process_ipc_json(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    JSON íŒŒì¼ì„ ì½ì–´, path ê¸°ë°˜ìœ¼ë¡œ title_enì„ ì¬ì¡°í•©í•œ ë’¤\n",
    "    ìƒˆ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. (tqdm ì§„í–‰ë¥  í‘œì‹œ)\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): ì›ë³¸ JSON íŒŒì¼ ê²½ë¡œ\n",
    "        output_file_path (str): ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. ì›ë³¸ JSON íŒŒì¼ ì½ê¸° (tqdm ì ìš©ì´ í¬ê²Œ ì˜ë¯¸ ì—†ìŒ)\n",
    "        print(f\"'{input_file_path}' íŒŒì¼ ì½ëŠ” ì¤‘...\")\n",
    "        with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"íŒŒì¼ ì½ê¸° ì™„ë£Œ (ì´ {len(data)}ê°œ í•­ëª©).\")\n",
    "\n",
    "        # 2. 'ipc_code'ì™€ ì›ë³¸ 'title_en' ì¡°íšŒìš© ë§µ ìƒì„± (tqdm ì ìš©ì´ í¬ê²Œ ì˜ë¯¸ ì—†ìŒ)\n",
    "        print(\"ì¡°íšŒìš© ë§µ ìƒì„± ì¤‘...\")\n",
    "        title_lookup = {item[\"ipc_code\"]: item[\"title_en\"] for item in data}\n",
    "        print(\"ë§µ ìƒì„± ì™„ë£Œ.\")\n",
    "\n",
    "        processed_data = []\n",
    "\n",
    "        # 3. ê° í•­ëª©ì„ ìˆœíšŒí•˜ë©° 'title_en'ì„ ìƒˆë¡œ ì¡°í•© (tqdm ì ìš©)\n",
    "        print(\"ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        # â­ tqdmìœ¼ë¡œ data ë¦¬ìŠ¤íŠ¸ë¥¼ ê°ì‹¸ì„œ ì§„í–‰ë¥  í‘œì‹œ\n",
    "        for item in tqdm(data, desc=\"ë°ì´í„° ì²˜ë¦¬ ì¤‘\"):\n",
    "            # pathì˜ ê° ì½”ë“œì— í•´ë‹¹í•˜ëŠ” titleì„ ì¡°íšŒ\n",
    "            path_titles = [title_lookup[code] for code in item[\"path\"]]\n",
    "\n",
    "            # titleë“¤ì„ ê³µë°±ìœ¼ë¡œ í•©ì¹˜ê¸°\n",
    "            new_title = \" \".join(path_titles)\n",
    "\n",
    "            # ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ê³  title_enë§Œ êµì²´\n",
    "            new_item = item.copy()\n",
    "            new_item[\"title_en\"] = new_title\n",
    "\n",
    "            processed_data.append(new_item)\n",
    "\n",
    "        # 4. ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ìƒˆ JSON íŒŒì¼ë¡œ ì €ì¥ (tqdm ì ìš©ì´ í¬ê²Œ ì˜ë¯¸ ì—†ìŒ)\n",
    "        print(f\"\\n'{output_file_path}' íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì €ì¥ ìœ„ì¹˜: {output_file_path})\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: '{input_file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"ì˜¤ë¥˜: 'path'ì— ì˜ëª»ëœ ipc_codeê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ({e})\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "\n",
    "# --- ì‚¬ìš© ì˜ˆì‹œ ---\n",
    "# \"original_data.json\" íŒŒì¼ì„ ì½ì–´ì„œ \"processed_data.json\" íŒŒì¼ë¡œ ì €ì¥\n",
    "input_file = \"../raw/json/ipc_filtered_data_formatted.json\"\n",
    "output_file = \"processed_data.json\"\n",
    "process_ipc_json(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34a952",
   "metadata": {},
   "source": [
    "## ì™„ì„±ëœ JSONíŒŒì¼ì—ì„œ title_en ê¸¸ì´ ê²€ì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d07207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ë¶„ì„ ê²°ê³¼ (../processed_datav2.json) ---\n",
      "ìµœëŒ€ ê¸¸ì´: 1558\n",
      "í‰ê·  ê¸¸ì´: 370.41\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def analyze_title_lengths(input_file_path):\n",
    "    \"\"\"\n",
    "    JSON íŒŒì¼ì„ ì½ì–´ 'title_en' í•„ë“œì˜\n",
    "    ìµœëŒ€ ê¸¸ì´ì™€ í‰ê·  ê¸¸ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): ë¶„ì„í•  JSON íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "    Returns:\n",
    "        tuple: (max_length, avg_length)\n",
    "               (ì˜¤ë¥˜ ë°œìƒ ì‹œ None, None)\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "\n",
    "    try:\n",
    "        # 1. íŒŒì¼ ì½ê¸°\n",
    "        with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data:\n",
    "            print(\"ì˜¤ë¥˜: íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "            return 0, 0.0\n",
    "\n",
    "        # 2. 'title_en' í‚¤ì˜ ê¸¸ì´ ê³„ì‚°\n",
    "        for item in data:\n",
    "            # .get()ì„ ì‚¬ìš©í•˜ë©´ 'title_en' í‚¤ê°€ ì—†ì–´ë„ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•ŠìŒ\n",
    "            title = item.get(\"title_en\")\n",
    "            if title is not None:\n",
    "                lengths.append(len(title))\n",
    "            else:\n",
    "                # title_en í‚¤ê°€ ì—†ëŠ” í•­ëª©ì´ ìˆë‹¤ë©´ ê²½ê³ \n",
    "                print(\n",
    "                    f\"Warning: 'title_en' í‚¤ê°€ ì—†ëŠ” í•­ëª© ë°œê²¬ (ipc_code: {item.get('ipc_code')})\"\n",
    "                )\n",
    "\n",
    "        # 3. í†µê³„ ê³„ì‚°\n",
    "        if not lengths:\n",
    "            print(\"ì˜¤ë¥˜: 'title_en' í•„ë“œë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return 0, 0.0\n",
    "\n",
    "        max_length = max(lengths)\n",
    "        avg_length = sum(lengths) / len(lengths)\n",
    "\n",
    "        return max_length, avg_length\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: '{input_file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ì˜¤ë¥˜: '{input_file_path}' íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# 2. í•¨ìˆ˜ ì‹¤í–‰\n",
    "input_file = \"../processed_datav2.json\"\n",
    "max_len, avg_len = analyze_title_lengths(input_file)\n",
    "\n",
    "# 3. ê²°ê³¼ ì¶œë ¥\n",
    "if max_len is not None:\n",
    "    print(f\"\\n--- ë¶„ì„ ê²°ê³¼ ({input_file}) ---\")\n",
    "    print(f\"ìµœëŒ€ ê¸¸ì´: {max_len}\")\n",
    "    print(f\"í‰ê·  ê¸¸ì´: {avg_len:.2f}\")  # ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ í‘œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c04e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ ì½ëŠ” ì¤‘... ../raw/json/ipc_filtered_data_formatted.json\n",
      "ì²˜ë¦¬ ì™„ë£Œ: 69337ê°œ í•­ëª©\n",
      "ì €ì¥ ì™„ë£Œ -> ./ipc_embedded.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def process_ipc_json_overwrite(input_path, output_path):\n",
    "    # 1. íŒŒì¼ ì½ê¸°\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"ì˜¤ë¥˜: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ -> {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"íŒŒì¼ ì½ëŠ” ì¤‘... {input_path}\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2. ì›ë³¸ íƒ€ì´í‹€ ë°±ì—… (ì¤‘ìš”!)\n",
    "    # ì²˜ë¦¬ ë„ì¤‘ ë¶€ëª¨ì˜ title_enì´ ë°”ë€Œë”ë¼ë„, ìì‹ì€ 'ì›ë˜ ì§§ì€ íƒ€ì´í‹€'ì„ ì°¸ì¡°í•´ì•¼ í•˜ë¯€ë¡œ ë³„ë„ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    original_titles = {item[\"ipc_code\"]: item[\"title_en\"] for item in data}\n",
    "\n",
    "    processed_count = 0\n",
    "\n",
    "    for item in data:\n",
    "        # í˜„ì¬ íƒ€ì´í‹€ì€ ì›ë³¸ ë§µì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì•ˆì „í•¨ (í˜¹ì€ ì•„ì§ ìˆ˜ì • ì „ì¸ item['title_en'])\n",
    "        current_title = original_titles.get(item[\"ipc_code\"], \"\")\n",
    "        path = item.get(\"path\", [])\n",
    "\n",
    "        # ìƒìœ„ ì½”ë“œ ì¶”ì¶œ (í˜„ì¬ ë…¸ë“œ ì œì™¸, ê°€ê¹Œìš´ ë¶€ëª¨ë¶€í„° ì—­ìˆœ)\n",
    "        parent_codes = reversed(path[:-1])\n",
    "\n",
    "        parent_titles = []\n",
    "        for code in parent_codes:\n",
    "            # ë°˜ë“œì‹œ 'original_titles'ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨ (ìˆ˜ì •ë˜ì§€ ì•Šì€ ìˆœìˆ˜ íƒ€ì´í‹€)\n",
    "            if code in original_titles:\n",
    "                parent_titles.append(original_titles[code])\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì¡°í•©: \"Hand tools (Classification context: Soil working... > Agriculture...)\"\n",
    "        if parent_titles:\n",
    "            context_str = \" > \".join(parent_titles)\n",
    "            combined_text = f\"{current_title} (Classification context: {context_str})\"\n",
    "        else:\n",
    "            combined_text = current_title  # ìµœìƒìœ„ ë…¸ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€\n",
    "\n",
    "        # 3. title_en ë®ì–´ì“°ê¸°\n",
    "        item[\"title_en\"] = combined_text\n",
    "        processed_count += 1\n",
    "\n",
    "    # 4. ê²°ê³¼ ì €ì¥\n",
    "    print(f\"ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ í•­ëª©\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ -> {output_path}\")\n",
    "\n",
    "\n",
    "# --- ì‹¤í–‰ ì„¤ì • ---\n",
    "# ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”\n",
    "input_json_path = \"../raw/json/ipc_filtered_data_formatted.json\"  # ì½ì–´ì˜¬ íŒŒì¼\n",
    "output_json_path = \"./ipc_embedded.json\"  # ì €ì¥í•  íŒŒì¼\n",
    "\n",
    "process_ipc_json_overwrite(input_json_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ ì½ëŠ” ì¤‘... ../raw/json/ipc_filtered_data_formatted.json\n",
      "ì²˜ë¦¬ ì™„ë£Œ: 69337ê°œ\n",
      "ì €ì¥ ì™„ë£Œ -> ./processed_datav3.json\n"
     ]
    }
   ],
   "source": [
    "# ìµœìƒìœ„ ì½”ë“œ í”„ë£¨ë‹ ì ìš©\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def process_ipc_with_pruning(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"ì˜¤ë¥˜: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ -> {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"íŒŒì¼ ì½ëŠ” ì¤‘... {input_path}\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # =========================================================\n",
    "    # [í•µì‹¬ í•´ê²°ì±…]\n",
    "    # ìˆ˜ì • ì‘ì—…ì„ ì‹œì‘í•˜ê¸° ì „ì—, ëª¨ë“  ë…¸ë“œì˜ 'ìˆœìˆ˜í•œ ì›ë³¸ ì •ë³´'ë¥¼\n",
    "    # ë³„ë„ì˜ ë”•ì…”ë„ˆë¦¬(Lookup Table)ì— ìŠ¤ëƒ…ìƒ·ì²˜ëŸ¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    # ì´ë ‡ê²Œ í•˜ë©´ ë¦¬ìŠ¤íŠ¸ì˜ title_enì´ ë°”ë€Œì–´ë„, ì°¸ì¡°í•  ë•ŒëŠ” ì˜í–¥ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "    # =========================================================\n",
    "    clean_lookup = {}\n",
    "    for item in data:\n",
    "        # í˜¹ì‹œ ì…ë ¥ ë°ì´í„°ê°€ ì´ë¯¸ í•œ ë²ˆ ì˜ëª» ì²˜ë¦¬ë˜ì–´ (Context...)ê°€ ë¶™ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ\n",
    "        # ì•ˆì „í•˜ê²Œ ë¶„ë¦¬í•˜ì—¬ ì•ë¶€ë¶„ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "        raw_title = item.get(\"title_en\", \"\").split(\" (Classification context:\")[0]\n",
    "\n",
    "        clean_lookup[item[\"ipc_code\"]] = {\n",
    "            \"title_en\": raw_title,  # ìˆœìˆ˜ ì œëª©\n",
    "            \"kind\": item.get(\"kind\"),\n",
    "            \"path\": item.get(\"path\", []),\n",
    "        }\n",
    "\n",
    "    processed_count = 0\n",
    "\n",
    "    # ì´ì œ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©° ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
    "    for item in data:\n",
    "        # í˜„ì¬ ë…¸ë“œì˜ ìˆœìˆ˜ ì œëª© ê°€ì ¸ì˜¤ê¸° (lookupì—ì„œ)\n",
    "        my_code = item[\"ipc_code\"]\n",
    "        current_clean_title = clean_lookup[my_code][\"title_en\"]\n",
    "        path = item.get(\"path\", [])\n",
    "\n",
    "        # ìƒìœ„ ì½”ë“œ ì¶”ì¶œ (ìì‹  ì œì™¸, ë¶€ëª¨ -> ì¡°ë¶€ëª¨ ì—­ìˆœ)\n",
    "        parent_codes = reversed(path[:-1])\n",
    "\n",
    "        valid_parent_titles = []\n",
    "\n",
    "        for p_code in parent_codes:\n",
    "            # ë°˜ë“œì‹œ 'clean_lookup'ì—ì„œ ë¶€ëª¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "            # (item ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì°¸ì¡°í•˜ë©´ ìˆ˜ì •ëœ ë¶€ëª¨ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë¨ -> ì˜¤ë¥˜ ì›ì¸)\n",
    "            if p_code in clean_lookup:\n",
    "                p_node = clean_lookup[p_code]\n",
    "\n",
    "                # ì„¹ì…˜(s) ê°€ì§€ì¹˜ê¸° (ì›í•˜ì‹œëŠ” ê²½ìš°)\n",
    "                if p_node[\"kind\"] == \"s\":\n",
    "                    continue\n",
    "\n",
    "                valid_parent_titles.append(p_node[\"title_en\"])\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì¡°í•©\n",
    "        if valid_parent_titles:\n",
    "            context_str = \" > \".join(valid_parent_titles)\n",
    "            final_text = (\n",
    "                f\"{current_clean_title} (Classification context: {context_str})\"\n",
    "            )\n",
    "        else:\n",
    "            final_text = current_clean_title\n",
    "\n",
    "        # ê²°ê³¼ ë®ì–´ì“°ê¸°\n",
    "        item[\"title_en\"] = final_text\n",
    "        processed_count += 1\n",
    "\n",
    "    # ì €ì¥\n",
    "    print(f\"ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ -> {output_path}\")\n",
    "\n",
    "\n",
    "# --- ì‹¤í–‰ ì„¤ì • ---\n",
    "input_json_path = \"../raw/json/ipc_filtered_data_formatted.json\"  # ì½ì–´ì˜¬ íŒŒì¼\n",
    "output_json_path = \"./processed_datav3.json\"  # ì €ì¥í•  íŒŒì¼\n",
    "\n",
    "process_ipc_with_pruning(input_json_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99408175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ ì½ëŠ” ì¤‘... ../raw/json/ipc_filtered_data_formatted.json\n",
      "ì²˜ë¦¬ ì™„ë£Œ: 69337ê°œ\n",
      "ì €ì¥ ì™„ë£Œ -> ../processed_datav4.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def process_ipc_top_down(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"ì˜¤ë¥˜: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ -> {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"íŒŒì¼ ì½ëŠ” ì¤‘... {input_path}\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 1. ì›ë³¸ ë°ì´í„° ë°±ì—… (ëˆˆë©ì´ íš¨ê³¼ ë°©ì§€ìš© í•„ìˆ˜ ë‹¨ê³„)\n",
    "    clean_lookup = {}\n",
    "    for item in data:\n",
    "        # í˜¹ì‹œ ì´ë¯¸ ë³€í™˜ëœ ë°ì´í„°ë¼ë©´ ê´„í˜¸ ì•ë¶€ë¶„ë§Œ ì˜ë¼ì„œ ì›ë³¸ í™•ë³´\n",
    "        raw_title = item.get(\"title_en\", \"\").split(\" (Classification context:\")[0]\n",
    "        clean_lookup[item[\"ipc_code\"]] = {\n",
    "            \"title_en\": raw_title,\n",
    "            \"kind\": item.get(\"kind\"),\n",
    "            \"path\": item.get(\"path\", []),\n",
    "        }\n",
    "\n",
    "    processed_count = 0\n",
    "\n",
    "    for item in data:\n",
    "        current_code = item[\"ipc_code\"]\n",
    "        # ì›ë³¸ ë§µì—ì„œ í˜„ì¬ ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "        current_clean_title = clean_lookup[current_code][\"title_en\"]\n",
    "        path = item.get(\"path\", [])\n",
    "\n",
    "        # 2. ìƒìœ„ ì½”ë“œ ì¶”ì¶œ ë°©ì‹ ë³€ê²½ (Top-down)\n",
    "        # reversed()ë¥¼ ì œê±°í•˜ì—¬ path ìˆœì„œ ê·¸ëŒ€ë¡œ(ë£¨íŠ¸ -> ë¶€ëª¨) ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "        # path[:-1] : ìê¸° ìì‹ ì„ ì œì™¸í•œ ëª¨ë“  ì¡°ìƒë“¤\n",
    "        ancestor_codes = path[:-1]\n",
    "\n",
    "        valid_ancestor_titles = []\n",
    "\n",
    "        for p_code in ancestor_codes:\n",
    "            if p_code in clean_lookup:\n",
    "                p_node = clean_lookup[p_code]\n",
    "\n",
    "                # (ì„ íƒì‚¬í•­) ì„¹ì…˜(Section, kind='s')ì´ ë„ˆë¬´ í¬ê´„ì ì´ë¼ ì œì™¸í•˜ê³  ì‹¶ìœ¼ë©´ ìœ ì§€\n",
    "                # ìµœìƒìœ„(A)ë¶€í„° ëª¨ë‘ í¬í•¨í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ifë¬¸ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.\n",
    "                if p_node[\"kind\"] == \"s\":\n",
    "                    continue\n",
    "\n",
    "                valid_ancestor_titles.append(p_node[\"title_en\"])\n",
    "\n",
    "        # 3. í…ìŠ¤íŠ¸ ì¡°í•©\n",
    "        if valid_ancestor_titles:\n",
    "            # ë¦¬ìŠ¤íŠ¸ ìˆœì„œê°€ [ìƒìœ„ -> í•˜ìœ„]ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ join\n",
    "            context_str = \" > \".join(valid_ancestor_titles)\n",
    "            final_text = (\n",
    "                f\"{current_clean_title} (Classification context: {context_str})\"\n",
    "            )\n",
    "        else:\n",
    "            final_text = current_clean_title\n",
    "\n",
    "        # ê²°ê³¼ ì ìš©\n",
    "        item[\"title_en\"] = final_text\n",
    "        processed_count += 1\n",
    "\n",
    "    # ì €ì¥\n",
    "    print(f\"ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ -> {output_path}\")\n",
    "\n",
    "\n",
    "# --- ì‹¤í–‰ ---\n",
    "input_json_path = \"../raw/json/ipc_filtered_data_formatted.json\"  # ì½ì–´ì˜¬ íŒŒì¼\n",
    "output_json_path = \"../processed_datav4.json\"  # ì €ì¥í•  íŒŒì¼\n",
    "\n",
    "process_ipc_top_down(input_json_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e9def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ ì½ëŠ” ì¤‘... ../raw/json/ipc_filtered_data_formatted.json\n",
      "ì²˜ë¦¬ ì™„ë£Œ: 69337ê°œ\n",
      "ì €ì¥ ì™„ë£Œ -> ../processed_datav5.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def process_ipc_hybrid_mix(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"ì˜¤ë¥˜: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ -> {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"íŒŒì¼ ì½ëŠ” ì¤‘... {input_path}\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 1. ì›ë³¸ ë°ì´í„° ë°±ì—… (í•„ìˆ˜: ëˆˆë©ì´ íš¨ê³¼ ë°©ì§€)\n",
    "    clean_lookup = {}\n",
    "    for item in data:\n",
    "        # í˜¹ì‹œ ì´ë¯¸ ë³€í™˜ëœ ë°ì´í„°ë¼ë©´ ì •ì œ\n",
    "        raw_title = item.get(\"title_en\", \"\").split(\" (Classification context:\")[0]\n",
    "        clean_lookup[item[\"ipc_code\"]] = {\n",
    "            \"title_en\": raw_title,\n",
    "            \"kind\": item.get(\"kind\"),\n",
    "            \"path\": item.get(\"path\", []),\n",
    "        }\n",
    "\n",
    "    processed_count = 0\n",
    "\n",
    "    for item in data:\n",
    "        current_code = item[\"ipc_code\"]\n",
    "        current_node = clean_lookup[current_code]\n",
    "        current_title = current_node[\"title_en\"]\n",
    "        path = item.get(\"path\", [])\n",
    "\n",
    "        # ë¬¸ë§¥ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ (ìˆœì„œ: Class -> Main Group -> Parent)\n",
    "        context_parts = []\n",
    "\n",
    "        # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ ì§‘í•© (ì´ë¯¸ ì¶”ê°€ëœ ipc_code ê¸°ë¡)\n",
    "        added_codes = set()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # (1) Class ì°¾ê¸° (kind='c')\n",
    "        # -------------------------------------------------------\n",
    "        for code in path:\n",
    "            # ìê¸° ìì‹ ì€ Contextì— ë„£ì§€ ì•ŠìŒ (ë‚˜ì¤‘ì— ë¶™ì„)\n",
    "            if code == current_code:\n",
    "                continue\n",
    "\n",
    "            node = clean_lookup.get(code)\n",
    "            if node and node[\"kind\"] == \"c\":\n",
    "                context_parts.append(node[\"title_en\"])\n",
    "                added_codes.add(code)\n",
    "                break  # ClassëŠ” í•˜ë‚˜ë¿ì´ë¯€ë¡œ ì°¾ìœ¼ë©´ ì¢…ë£Œ\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # (2) Main Group ì°¾ê¸° (kind='m')\n",
    "        # -------------------------------------------------------\n",
    "        for code in path:\n",
    "            if code == current_code:\n",
    "                continue\n",
    "\n",
    "            if code in added_codes:  # ì´ë¯¸ Classë¡œ ì¶”ê°€ë˜ì—ˆë‹¤ë©´ íŒ¨ìŠ¤\n",
    "                continue\n",
    "\n",
    "            node = clean_lookup.get(code)\n",
    "            if node and node[\"kind\"] == \"m\":\n",
    "                context_parts.append(node[\"title_en\"])\n",
    "                added_codes.add(code)\n",
    "                # Main Groupì€ ê²½ë¡œìƒì— í•˜ë‚˜ì¼ ìˆ˜ë„ ìˆê³  ì—¬ëŸ¬ ê°œì¼ ìˆ˜ë„ ìˆìœ¼ë‚˜,\n",
    "                # ë³´í†µ ê³„ì¸µìƒ ê°€ì¥ ìƒìœ„ì˜ Main Group í•˜ë‚˜ë¥¼ ì˜ë¯¸í•˜ê±°ë‚˜\n",
    "                # ì§ê³„ ì¡°ìƒ ì¤‘ Main Groupì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì—¬ê¸°ì„  'ê²½ë¡œìƒ ë°œê²¬ë˜ëŠ” ì²« MG'ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "                # (ë§Œì•½ ëª¨ë“  MGë¥¼ ì›í•˜ì‹œë©´ breakë¥¼ ì œê±°í•˜ì„¸ìš”)\n",
    "                break\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # (3) ì§ì „ ë¶€ëª¨ ì°¾ê¸° (Immediate Parent)\n",
    "        # -------------------------------------------------------\n",
    "        if len(path) >= 2:\n",
    "            parent_code = path[-2]  # ë°”ë¡œ ìœ— ë‹¨ê³„\n",
    "\n",
    "            # ë¶€ëª¨ ì½”ë“œê°€ ì´ë¯¸ Classë‚˜ Main Groupìœ¼ë¡œ ì¶”ê°€ë˜ì§€ ì•Šì•˜ì„ ë•Œë§Œ ì¶”ê°€\n",
    "            if parent_code not in added_codes:\n",
    "                node = clean_lookup.get(parent_code)\n",
    "                if node:\n",
    "                    context_parts.append(node[\"title_en\"])\n",
    "                    added_codes.add(parent_code)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # (4) ìµœì¢… ì¡°í•©: \"í˜„ì¬ ì„¤ëª… (Context: Class > MG > Parent)\"\n",
    "        # -------------------------------------------------------\n",
    "        if context_parts:\n",
    "            context_str = \" > \".join(context_parts)\n",
    "            final_text = f\"{current_title} (Classification context: {context_str})\"\n",
    "        else:\n",
    "            final_text = current_title\n",
    "\n",
    "        # ê²°ê³¼ ì ìš©\n",
    "        item[\"title_en\"] = final_text\n",
    "        processed_count += 1\n",
    "\n",
    "    # ì €ì¥\n",
    "    print(f\"ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ -> {output_path}\")\n",
    "\n",
    "\n",
    "# --- ì‹¤í–‰ ---\n",
    "input_json_path = \"../raw/json/ipc_filtered_data_formatted.json\"  # ì½ì–´ì˜¬ íŒŒì¼\n",
    "output_json_path = \"../processed_datav5.json\"  # ì €ì¥í•  íŒŒì¼\n",
    "\n",
    "process_ipc_hybrid_mix(input_json_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566c6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
